import asyncio
import json
import sys
from pathlib import Path
from typing import AsyncGenerator, TypedDict, Annotated, List, Union, Optional, Dict, Any

from pydantic import BaseModel, Field
from langgraph.graph import StateGraph, END
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.runnables import RunnableConfig
from langchain_core.messages.utils import convert_to_openai_messages # å¼•å…¥æ­¤è¡Œ

dir_name = Path(__file__).resolve().parent.parent.parent
sys.path.append(str(dir_name))

from applications.summarization.summarization_prompt import (
    SUMMARIZATION_SYSTEM_MESSAGE,
    SUMMARIZATION_PROMPT,
    SUMMARY_TYPE_GUIDELINES
)
from base_agent import BaseAgent
from preprocess.long_text_preprocessor import LongTextPreprocessor
from llm_api.llm_client_chat_model import LLMClientChatModel
from utils.time_count import timer
from utils.stream_chunk import StreamChunk # ç¡®ä¿ StreamChunk åœ¨è¿™é‡Œè¢«æ­£ç¡®å¯¼å…¥


# ===== è¾“å‡ºç»“æ„å®šä¹‰ (BaseModel ç”¨äºå†…éƒ¨éªŒè¯ï¼Œæœ€ç»ˆè¾“å‡ºç»“æ„ä¼šè°ƒæ•´) =====
class SummarizationOutputContent(BaseModel):
    success: bool = Field(description="æ‘˜è¦æ˜¯å¦æˆåŠŸ")
    original_text: str = Field(description="åŸæ–‡")
    summarized_text: str = Field(description="æ‘˜è¦æ–‡æœ¬")
    summary_type: str = Field(description="æ‘˜è¦ç±»å‹")
    target_words: Optional[int] = Field(default=None, description="ç›®æ ‡å­—æ•°ï¼ŒNoneè¡¨ç¤ºä¸é™åˆ¶")
    actual_words: int = Field(description="å®é™…å­—æ•°")
    chunk_count: int = Field(description="åˆ†å—æ•°é‡")
    quality_score: float = Field(description="è´¨é‡è¯„åˆ†", ge=0, le=10)
    word_limit_mode: bool = Field(description="æ˜¯å¦é™åˆ¶å­—æ•°æ¨¡å¼")


# ===== çŠ¶æ€å®šä¹‰ =====
class SummarizationState(TypedDict):
    messages: Annotated[List[Union[HumanMessage, AIMessage]], "æ¶ˆæ¯å†å²"]
    original_text: str
    target_words: Optional[int]  # Noneè¡¨ç¤ºä¸é™åˆ¶å­—æ•°
    original_target_words: Optional[int]  # åŸå§‹ç›®æ ‡å­—æ•°ï¼ŒNoneè¡¨ç¤ºä¸é™åˆ¶å­—æ•°
    summary_type: str
    processed_chunks: List[Dict[str, Any]]
    summarized_text: str
    actual_words: int
    quality_score: float
    final_output: Optional[dict] # ä¿®æ”¹ä¸ºOptional[dict]
    word_limit_mode: bool  # æ˜¯å¦é™åˆ¶å­—æ•°æ¨¡å¼


# ===== æ–‡æœ¬æ‘˜è¦Agentä¸»ç±» =====
class TextSummarizationAgent(BaseAgent):
    def __init__(
            self,
            name: str = "text-summarization-agent",
            # openai client init config
            base_url: str = "https://api.deepseek.com/v1",
            api_key: Optional[str] = None,
            timeout: float = 60.0,
            max_retries: int = 3,
            # openai client run config
            model: str = "deepseek-chat",
            max_tokens: Optional[int] = None,
            temperature: float = 0.3,
            top_p: float = 1.0,
            stream: bool = False,
            enable_thinking: bool = False,
            # chunk config
            max_chunk_length: int = 1000  # æ‘˜è¦åˆ†å—å¯ä»¥å¤§ä¸€äº›
    ):
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        super().__init__(
            name=name,
            model=model,
            base_url=base_url,
            api_key=api_key,
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
            timeout=timeout,
            max_retries=max_retries,
            stream=stream,
            enable_thinking=enable_thinking,
        )

        # ä¿å­˜è‡ªå®šä¹‰é…ç½®
        self.init_config.update({
            "max_chunk_length": max_chunk_length,
        })

        # åˆå§‹åŒ–ç»„ä»¶
        self.preprocessor = LongTextPreprocessor()
        
        # éªŒè¯æ”¯æŒçš„æ‘˜è¦ç±»å‹
        self.supported_types = list(SUMMARY_TYPE_GUIDELINES.keys())

        # æ„å»ºå·¥ä½œæµå›¾
        self.graph = self._build_graph()

    def _build_summarization_prompt(self, text: str, target_words: Optional[int], summary_type: str, word_limit_mode: bool) -> str:
        """æ„å»ºæ‘˜è¦æç¤ºè¯"""
        # ä½¿ç”¨é¢„å¤„ç†å™¨åˆ†å—
        chunks = self.preprocessor.prepare_correction_chunks(
            text, 
            max_chunk_length=self.init_config["max_chunk_length"]
        )
        
        # æ„å»ºåˆ†å—ä¿¡æ¯æç¤º
        chunk_info = ""
        total_original_chars = len(text)
        
        if word_limit_mode and target_words is not None:
            # é™åˆ¶å­—æ•°æ¨¡å¼ï¼šæ˜¾ç¤ºå­—æ•°åˆ†é…
            for i, chunk in enumerate(chunks):
                chunk_length = len(chunk["text"])
                # ç¡®ä¿åˆ†é…çš„å­—æ•°è‡³å°‘ä¸º1ï¼Œé¿å…é™¤é›¶é”™è¯¯æˆ–è¿‡å°
                suggested_summary_length = max(1, int(chunk_length * target_words / total_original_chars))
                
                chunk_info += f"\n<chunk{i+1}: åŸæ–‡{chunk_length}å­— | å»ºè®®åˆ†é…{suggested_summary_length}å­—>\n"
                chunk_info += f"{chunk['text']}\n"
                chunk_info += f"</chunk{i+1}>\n"
        else:
            # ä¸é™åˆ¶å­—æ•°æ¨¡å¼ï¼šåªæ˜¾ç¤ºåŸæ–‡å—
            for i, chunk in enumerate(chunks):
                chunk_info += f"{chunk['text']}"
        
        # è·å–æ‘˜è¦ç±»å‹æŒ‡å—
        type_guidelines = SUMMARY_TYPE_GUIDELINES.get(summary_type, SUMMARY_TYPE_GUIDELINES["è¦ç‚¹æ‘˜è¦"])

        chunk_count=len(chunks) if word_limit_mode else 1 # è¿™é‡Œå¯ä»¥ç®€åŒ–ï¼Œå§‹ç»ˆæ˜¯len(chunks)
        
        prompt = SUMMARIZATION_PROMPT.format(
            chunk_info=chunk_info,
            target_words=f"çº¦{target_words}å­—ï¼ˆÂ±20%èŒƒå›´å†…ï¼‰" if word_limit_mode else "æ— å­—æ•°é™åˆ¶",
            summary_type=summary_type,
            type_guidelines=type_guidelines,
            total_original_chars=total_original_chars,
            chunk_count=f"{chunk_count}å—",
        )
        
        return prompt

    def _calculate_quality_score(self, original_text: str, summarized_text: str, target_words: Optional[int], word_limit_mode: bool) -> float:
        """è®¡ç®—æ‘˜è¦è´¨é‡è¯„åˆ†"""
        actual_words = len(summarized_text)
        original_words = len(original_text)
        
        # åŸºç¡€åˆ†
        score = 7.0
        
        if word_limit_mode and target_words is not None:
            # é™åˆ¶å­—æ•°æ¨¡å¼ï¼šæ£€æŸ¥å­—æ•°ç¬¦åˆåº¦
            word_ratio = actual_words / target_words
            if 0.6 <= word_ratio <= 1.1: # å…è®¸ç•¥å¾®è¶…å‡º
                score += 3.0
            elif 1.1 < word_ratio <= 1.3: # ç¨å¾®è¶…å‡º
                score += 1.0
            elif word_ratio < 0.6: # å­—æ•°è¿‡å°‘
                score -= 1.0
        else:
            # ä¸é™åˆ¶å­—æ•°æ¨¡å¼ï¼šæ£€æŸ¥å‹ç¼©æ¯”åˆç†æ€§
            if original_words == 0: # é¿å…é™¤é›¶
                return 0.0
            compression_ratio = actual_words / original_words
            if 0.1 <= compression_ratio <= 0.5:  # åˆç†çš„å‹ç¼©æ¯”èŒƒå›´
                score += 2.0
            elif 0.05 <= compression_ratio <= 0.7:
                score += 1.0
            else: # å‹ç¼©æ¯”ä¸åˆç†
                score -= 1.0
        
        return max(0.0, min(10.0, score)) # ç¡®ä¿åˆ†æ•°åœ¨0-10ä¹‹é—´

    def _build_graph(self) -> StateGraph:
        """æ„å»º LangGraph å·¥ä½œæµ"""
        graph = StateGraph(SummarizationState)

        async def preprocess_node(state: SummarizationState, config: RunnableConfig) -> SummarizationState:
            """é¢„å¤„ç†èŠ‚ç‚¹ï¼šå‡†å¤‡åˆ†å—ä¿¡æ¯"""
            run_config = config.get("configurable", {})
            request_id = run_config.get("request_id")

            with timer(self.logger, f"request_id: {request_id}, æ–‡æœ¬é¢„å¤„ç†åˆ†å—"):
                # éªŒè¯æ‘˜è¦ç±»å‹
                if state["summary_type"] not in self.supported_types:
                    raise ValueError(f"ä¸æ”¯æŒçš„æ‘˜è¦ç±»å‹: {state['summary_type']}ï¼Œæ”¯æŒçš„ç±»å‹: {', '.join(self.supported_types)}")
                
                # å‡†å¤‡åˆ†å—
                max_chunk_length = run_config.get("max_chunk_length", self.init_config["max_chunk_length"])
                chunks = self.preprocessor.prepare_correction_chunks(
                    state["original_text"],
                    max_chunk_length=max_chunk_length
                )

                mode_desc = "é™åˆ¶å­—æ•°æ¨¡å¼" if state["word_limit_mode"] else "è‡ªç”±é•¿åº¦æ¨¡å¼"
                target_desc = f"ç›®æ ‡å­—æ•°{state['target_words']}" if state["word_limit_mode"] else "æ— å­—æ•°é™åˆ¶"
                
                self.logger.info(f"request_id: {request_id}, æ–‡æœ¬åˆ†å—å®Œæˆ, å…±{len(chunks)}ä¸ªå—, {mode_desc}, {target_desc}")
                
                return {
                    **state,
                    "processed_chunks": chunks
                }

        async def summarize_node(state: SummarizationState, config: RunnableConfig) -> SummarizationState:
            """æ‘˜è¦èŠ‚ç‚¹ï¼šä¸€æ¬¡æ€§ç”Ÿæˆæ‘˜è¦"""
            run_config = config.get("configurable", {})
            request_id = run_config.get("request_id")

            with timer(self.logger, f"request_id: {request_id}, ç”Ÿæˆæ‘˜è¦"):
                # æ„å»ºæç¤ºè¯
                prompt_text = self._build_summarization_prompt(
                    state["original_text"],
                    state["target_words"],
                    state["summary_type"],
                    state["word_limit_mode"]
                )

                mode_desc = "é™åˆ¶å­—æ•°" if state["word_limit_mode"] else "è‡ªç”±é•¿åº¦"
                self.logger.info(f"request_id: {request_id}, è°ƒç”¨LLMç”Ÿæˆæ‘˜è¦, æ¨¡å¼: {mode_desc}")

                messages = [
                    SystemMessage(content=SUMMARIZATION_SYSTEM_MESSAGE), 
                    HumanMessage(content=prompt_text)
                ]

                # è°ƒç”¨LLM
                llm_client = self.get_llm_client(run_config)
                chat_model = LLMClientChatModel(llm_client=llm_client)
                try:
                    response = await chat_model.ainvoke(messages, config=config)

                    chat_completion = response.chat_completion.to_dict()

                    print(chat_completion)
                    choices = chat_completion.get("choices", [])
                    
                    # åˆå§‹åŒ–æœ€ç»ˆè¾“å‡ºç»“æ„ï¼ŒåŒ…å«metadata
                    output_metadata = {
                        "usage": chat_completion.get("usage", {}),
                        # "messages": []
                    }

                    if len(choices) > 0:
                        content = choices[0].get("message", {}).get("content", "")
                        reasoning_content = choices[0].get("message", {}).get("reasoning_content", "")

                        self.logger.debug(f"request_id: {request_id}, LLMå“åº”é•¿åº¦: {len(content)}")
                        summarized_text = content.strip()
                        
                        # è®¡ç®—è´¨é‡è¯„åˆ†
                        quality_score = self._calculate_quality_score(
                            state["original_text"],
                            summarized_text,
                            state["original_target_words"],
                            state["word_limit_mode"]
                        )
                        
                        # æ›´æ–°æ¶ˆæ¯å†å²
                        new_messages = state["messages"] + messages + [AIMessage(content=content)]
                        # output_metadata["messages"] = convert_to_openai_messages(new_messages)
                        
                        # æ„é€  output å­—å…¸
                        output_data = {
                            "summarized_text": summarized_text,
                            "summary_type": state["summary_type"],
                            "target_words": state["original_target_words"] if state["word_limit_mode"] else None,
                            "actual_words": len(summarized_text),
                            "quality_score": quality_score,
                            "word_limit_mode": state["word_limit_mode"]
                        }

                        # æ„å»ºåŒ…å«å››ä¸ªå›ºå®šå…ƒç´ çš„ final_output
                        final_output_structure = {
                            "output": output_data,
                            "content": content,
                            "reasoning_content": reasoning_content,
                            "metadata": output_metadata,
                            "confidence": 0.0 # æš‚æ—¶è®¾ä¸º0ï¼Œåœ¨finalize_nodeä¸­æ›´æ–°
                        }
                        
                        self.logger.debug(f"request_id: {request_id}, æ‘˜è¦ç”Ÿæˆå®Œæˆ, å®é™…å­—æ•°: {len(summarized_text)}, è´¨é‡è¯„åˆ†: {quality_score:.2f}")
                        
                        return {
                            **state,
                            "messages": new_messages,
                            "summarized_text": summarized_text,
                            "actual_words": len(summarized_text),
                            "quality_score": quality_score,
                            "final_output": final_output_structure # æ›´æ–°final_output
                        }
                    else:
                        raise ValueError("LLM apiè¾“å‡ºé”™è¯¯, choicesä¸ºç©º")
                except asyncio.CancelledError:
                    self.logger.warning(f"â›” request_id: {request_id}, ä»»åŠ¡è¢«ä¸­æ–­ï¼Œå·²åœæ­¢ LLM è¯·æ±‚")
                    raise

        def finalize_node(state: SummarizationState, config: RunnableConfig) -> SummarizationState:
            """æœ€ç»ˆå¤„ç†èŠ‚ç‚¹ï¼šæ±‡æ€»ç»“æœ"""
            run_config = config.get("configurable", {})
            request_id = run_config.get("request_id")

            with timer(self.logger, f"request_id: {request_id}, ç»“æœæ±‡æ€»"):
                success = bool(state.get("summarized_text"))
                
                # è®¡ç®—ç½®ä¿¡åº¦ (ç®€åŒ–ç¤ºä¾‹ï¼Œå¯æ ¹æ®å®é™…éœ€æ±‚æ›´å¤æ‚è®¡ç®—)
                # ä¾‹å¦‚ï¼ŒåŸºäºè´¨é‡è¯„åˆ†å’Œæ˜¯å¦æœ‰æ‘˜è¦å†…å®¹
                confidence = state.get("quality_score", 0.0) / 10.0 if success else 0.0

                final_output = state.get("final_output", {})

                # æ›´æ–° confidence
                final_output["confidence"] = confidence

                mode_desc = "é™åˆ¶å­—æ•°æ¨¡å¼" if state["word_limit_mode"] else "è‡ªç”±é•¿åº¦æ¨¡å¼"
                status_msg = "æˆåŠŸ" if success else "å¤±è´¥"
                quality_msg = f"ï¼Œè´¨é‡è¯„åˆ†: {state.get('quality_score', 0):.2f}" if success else ""
                self.logger.success(f"request_id: {request_id}, æ‘˜è¦ç”Ÿæˆ{status_msg} [{mode_desc}]{quality_msg}")

                return {
                    **state,
                    "final_output": final_output
                }

        # æ·»åŠ èŠ‚ç‚¹
        graph.add_node("preprocess", preprocess_node)
        graph.add_node("summarize", summarize_node)
        graph.add_node("finalize", finalize_node)

        # è®¾ç½®å·¥ä½œæµ
        graph.set_entry_point("preprocess")
        graph.add_edge("preprocess", "summarize")
        graph.add_edge("summarize", "finalize")
        graph.add_edge("finalize", END)

        return graph.compile()

    async def run(self, text: str, target_words: Optional[int] = None, summary_type: str = "è¦ç‚¹æ‘˜è¦", ratio: float = 1.5, **kwargs) -> Dict[str, Any]:
        """
        æ‰§è¡Œæ–‡æœ¬æ‘˜è¦æµç¨‹

        :param text: è¾“å…¥æ–‡æœ¬
        :param target_words: ç›®æ ‡å­—æ•°ï¼ŒNoneè¡¨ç¤ºä¸é™åˆ¶å­—æ•°
        :param summary_type: æ‘˜è¦ç±»å‹ ("è¦ç‚¹æ‘˜è¦", "æ®µè½æ‘˜è¦", "æ–°é—»æ‘˜è¦", "æŠ€æœ¯æ‘˜è¦", "ä¼šè®®æ‘˜è¦")
        :param ratio: å­—æ•°è°ƒæ•´æ¯”ä¾‹
        :return: ç»“æ„åŒ–è¾“å‡ºå­—å…¸
        """
        if not text.strip():
            raise ValueError("æ–‡æœ¬ä¸èƒ½ä¸ºç©º")
        
        # ç¡®å®šæ¨¡å¼
        word_limit_mode = target_words is not None
        
        if word_limit_mode:
            if target_words <= 0:
                raise ValueError("ç›®æ ‡å­—æ•°å¿…é¡»å¤§äº0")
            if target_words > len(text):
                raise ValueError("ç›®æ ‡å­—æ•°ä¸èƒ½è¶…è¿‡åŸæ–‡é•¿åº¦")
            # åº”ç”¨ratioè°ƒæ•´
            adjusted_target_words = int(target_words / ratio)
        else:
            adjusted_target_words = None

        request_id = kwargs.get("request_id")

        mode_desc = "é™åˆ¶å­—æ•°æ¨¡å¼" if word_limit_mode else "è‡ªç”±é•¿åº¦æ¨¡å¼"
        target_desc = f"target_words: {adjusted_target_words}" if word_limit_mode else "æ— å­—æ•°é™åˆ¶"
        
        self.logger.info(f"ğŸ“ request_id: {request_id}, å¼€å§‹å¤„ç†æ‘˜è¦è¯·æ±‚, text_length: {len(text)}, {target_desc}, type: {summary_type}, mode: {mode_desc}")

        # æ„å»ºè¿è¡Œæ—¶é…ç½®
        run_config = {k: v for k, v in kwargs.items() if k in self.init_config or k == "request_id"}

        inputs = {
            "messages": [],
            "original_text": text,
            "target_words": adjusted_target_words,
            "original_target_words": target_words,  # ä¿å­˜åŸå§‹ç›®æ ‡å­—æ•°
            "summary_type": summary_type,
            "processed_chunks": [],
            "summarized_text": "",
            "actual_words": 0,
            "quality_score": 0.0,
            "final_output": None,
            "word_limit_mode": word_limit_mode
        }

        with timer(self.logger, f"request_id: {request_id}, å®Œæ•´æ‘˜è¦æµç¨‹"):
            # ä¼ é€’è¿è¡Œæ—¶é…ç½®
            config = {"configurable": run_config} if run_config else {}
            final_state = await self.graph.ainvoke(inputs, config=config)
            output = final_state.get("final_output", {})
            self.logger.success(f"ğŸ‰ request_id: {request_id}, æ‘˜è¦å®Œæˆ")

        return output
    
    async def run_stream(self, text: str, target_words: Optional[int] = None, summary_type: str = "è¦ç‚¹æ‘˜è¦", ratio: float = 1.5, **kwargs) -> AsyncGenerator[StreamChunk, None]: # StreamChunkåº”æ˜¯utils.stream_chunk.StreamChunk
        """
        æµå¼æ‰§è¡Œæ–‡æœ¬æ‘˜è¦æµç¨‹

        :param text: è¾“å…¥æ–‡æœ¬
        :param target_words: ç›®æ ‡å­—æ•°ï¼ŒNoneè¡¨ç¤ºä¸é™åˆ¶å­—æ•°
        :param summary_type: æ‘˜è¦ç±»å‹
        :param ratio: å­—æ•°è°ƒæ•´æ¯”ä¾‹
        :return: æµå¼è¾“å‡ºç”Ÿæˆå™¨
        """
        if not text.strip():
            raise ValueError("æ–‡æœ¬ä¸èƒ½ä¸ºç©º")
        
        # ç¡®å®šæ¨¡å¼
        word_limit_mode = target_words is not None
        
        if word_limit_mode:
            if target_words <= 0:
                raise ValueError("ç›®æ ‡å­—æ•°å¿…é¡»å¤§äº0")
            if target_words > len(text):
                raise ValueError("ç›®æ ‡å­—æ•°ä¸èƒ½è¶…è¿‡åŸæ–‡é•¿åº¦")
            # åº”ç”¨ratioè°ƒæ•´
            adjusted_target_words = int(target_words / ratio)
        else:
            adjusted_target_words = None

        request_id = kwargs.get("request_id")

        mode_desc = "é™åˆ¶å­—æ•°æ¨¡å¼" if word_limit_mode else "è‡ªç”±é•¿åº¦æ¨¡å¼"
        self.logger.info(f"ğŸ“ request_id: {request_id}, å¼€å§‹æµå¼å¤„ç†æ‘˜è¦è¯·æ±‚, text_length: {len(text)}, mode: {mode_desc}, type: {summary_type}")

        # æ„å»ºè¿è¡Œæ—¶é…ç½®
        run_config = {k: v for k, v in kwargs.items() if k in self.init_config or k == "request_id"}

        inputs = {
            "messages": [],
            "original_text": text,
            "target_words": adjusted_target_words,
            "original_target_words": target_words,  # ä¿å­˜åŸå§‹ç›®æ ‡å­—æ•°
            "summary_type": summary_type,
            "processed_chunks": [],
            "summarized_text": "",
            "actual_words": 0,
            "quality_score": 0.0,
            "final_output": None,
            "word_limit_mode": word_limit_mode
        }

        with timer(self.logger, f"request_id: {request_id}, å®Œæ•´æµå¼æ‘˜è¦æµç¨‹"):
            # ä¼ é€’è¿è¡Œæ—¶é…ç½®
            config = {"configurable": run_config} if run_config else {}
            
            async for event in self.graph.astream_events(inputs, config=config):
                event_type = event.get("event", "")
                
                # å¤„ç†LLMæµå¼è¾“å‡º
                if event_type == "on_chat_model_stream":
                    chunk = event.get("data", {}).get("chunk", None)
                    if chunk and hasattr(chunk, "chat_completion_chunk") and chunk.chat_completion_chunk:
                        chat_completion_chunk = chunk.chat_completion_chunk.to_dict()
                        choices = chat_completion_chunk.get("choices", [])
                        if choices:
                            delta = choices[0].get("delta", {})
                            content = delta.get("content", "")
                            reasoning_content = delta.get("reasoning_content", "")
                            
                            # è¾“å‡ºæ€è€ƒå†…å®¹
                            if reasoning_content:
                                yield StreamChunk(
                                    type="thinking",
                                    content=reasoning_content
                                )
                            # è¾“å‡ºæ‘˜è¦å†…å®¹
                            elif content:
                                yield StreamChunk(
                                    type="content",
                                    content=content
                                )
                
                # å¤„ç†èŠ‚ç‚¹å¼€å§‹äº‹ä»¶
                elif event_type == "on_chain_start":
                    name = event.get("name", "")
                    if name == "preprocess":
                        yield StreamChunk(
                            type="processing", # ä½¿ç”¨ "processing" ç±»å‹
                            content="å¼€å§‹æ–‡æœ¬é¢„å¤„ç†å’Œåˆ†å—åˆ†æ..."
                        )
                    elif name == "summarize":
                        if word_limit_mode:
                            yield StreamChunk(
                                type="processing", # ä½¿ç”¨ "processing" ç±»å‹
                                content=f"æ­£åœ¨ç”Ÿæˆ{summary_type}ï¼Œç›®æ ‡å­—æ•°: {adjusted_target_words}..."
                            )
                        else:
                            yield StreamChunk(
                                type="processing", # ä½¿ç”¨ "processing" ç±»å‹
                                content=f"æ­£åœ¨ç”Ÿæˆ{summary_type}ï¼Œè‡ªç”±é•¿åº¦æ¨¡å¼..."
                            )
                    elif name == "finalize":
                        yield StreamChunk(
                            type="processing", # ä½¿ç”¨ "processing" ç±»å‹
                            content="æ­£åœ¨æ±‡æ€»æœ€ç»ˆç»“æœ..."
                        )
                
                # å¤„ç†èŠ‚ç‚¹ç»“æŸäº‹ä»¶ (å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ ä¸€äº›ä¸­é—´çŠ¶æ€çš„Thinking)
                elif event_type == "on_chain_end":
                    name = event.get("name", "")
                    # å¤„ç†å›¾ç»“æŸäº‹ä»¶ï¼Œè¾“å‡ºæœ€ç»ˆç»“æœ
                    if name == "LangGraph":
                        output = event.get("data", {}).get("output", {})
                        final_output_data = output.get("final_output", {}) # ç¡®ä¿è·å–çš„æ˜¯æ•´ä¸ªfinal_output               
                        yield StreamChunk(
                            type="final",
                            content="",
                            metadata=final_output_data
                        )

            self.logger.success(f"ğŸ‰ request_id: {request_id}, æµå¼æ‘˜è¦å®Œæˆ")


# ===== ä½¿ç”¨ç¤ºä¾‹ =====
async def main():
    """ä½¿ç”¨ç¤ºä¾‹"""
    
    # åˆå§‹åŒ–Agent
    agent = TextSummarizationAgent(
        name="test-summarization-agent",
        base_url="https://api.deepseek.com/v1",
        api_key="YOUR_API_KEY",  # æ›¿æ¢ä¸ºå®é™…APIå¯†é’¥
        temperature=0.3,
        max_chunk_length=500
    )
    
    # æµ‹è¯•æ–‡æœ¬
    test_text = """
    æ–°åç¤¾ä¸‰äºš11æœˆ7æ—¥ç”µï¼ˆè®°è€…æ¢…å¸¸ä¼Ÿï¼‰æˆ‘å›½ç¬¬ä¸€è‰˜ç”µç£å¼¹å°„å‹èˆªç©ºæ¯èˆ°ç¦å»ºèˆ°å…¥åˆ—æˆæ——ä»ªå¼5æ—¥åœ¨æµ·å—ä¸‰äºšæŸå†›æ¸¯ä¸¾è¡Œã€‚ä¸­å…±ä¸­å¤®æ€»ä¹¦è®°ã€å›½å®¶ä¸»å¸­ã€ä¸­å¤®å†›å§”ä¸»å¸­ä¹ è¿‘å¹³å‡ºå¸­å…¥åˆ—æˆæ——ä»ªå¼å¹¶ç™»èˆ°è§†å¯Ÿã€‚

ã€€ã€€åä¸€æœˆçš„ä¸‰äºšï¼Œæµ·é˜”å¤©é«˜ï¼Œç¢§æ³¢æµ©ç€šã€‚å†›æ¸¯å†…ï¼Œç¦å»ºèˆ°è¸æµ·è€Œç«‹ã€æ»¡æ——é«˜æ‚¬ï¼Œå±±ä¸œèˆ°ä¼æ³¢ç›¸ä¼´ï¼Œæ¥è‡ªæµ·å†›éƒ¨é˜Ÿå’Œèˆªæ¯å»ºè®¾å•ä½çš„ä»£è¡¨2000ä½™äººåœ¨ç å¤´æ•´é½åˆ—é˜Ÿï¼Œæ°”æ°›éš†é‡çƒ­çƒˆã€‚

    ä¸‹åˆ4æ—¶30åˆ†è®¸ï¼Œå…¥åˆ—æˆæ——ä»ªå¼å¼€å§‹ï¼Œå…¨åœºé«˜å”±ä¸­åäººæ°‘å…±å’Œå›½å›½æ­Œï¼Œäº”æ˜Ÿçº¢æ——å†‰å†‰å‡èµ·ã€‚ä»ªä»—ç¤¼å…µæŠ¤å«ç€å…«ä¸€å†›æ——ï¼Œæ­£æ­¥è¡Œè¿›åˆ°ä¸»å¸­å°å‰ã€‚ä¹ è¿‘å¹³å°†å…«ä¸€å†›æ——æˆäºˆç¦å»ºèˆ°èˆ°é•¿ã€æ”¿æ²»å§”å‘˜ã€‚ç¦å»ºèˆ°èˆ°é•¿ã€æ”¿æ²»å§”å‘˜å‘ä¹ è¿‘å¹³æ•¬ç¤¼ï¼Œä»ä¹ è¿‘å¹³æ‰‹ä¸­æ¥è¿‡å…«ä¸€å†›æ——ã€‚ä¹ è¿‘å¹³åŒä»–ä»¬åˆå½±ç•™å¿µã€‚å…¥åˆ—æˆæ——ä»ªå¼åœ¨ä¸­å›½äººæ°‘è§£æ”¾å†›å†›æ­Œå£°ä¸­ç»“æŸã€‚

ã€€ã€€ä¹ è¿‘å¹³å¯¹æˆ‘å›½èˆªæ¯å»ºè®¾å‘å±•ä¸€ç›´å¾ˆå…³æ³¨ã€‚ä»ªå¼ç»“æŸåï¼Œä¹ è¿‘å¹³ç™»ä¸Šç¦å»ºèˆ°ï¼Œå¬å–æˆ‘å›½èˆªæ¯å»ºè®¾å‘å±•å·¥ä½œæ±‡æŠ¥ï¼Œäº†è§£èˆªæ¯ä½“ç³»ä½œæˆ˜èƒ½åŠ›ç”Ÿæˆã€ç”µç£å¼¹å°„ç³»ç»Ÿå»ºè®¾è¿ç”¨ç­‰æƒ…å†µã€‚

ã€€ã€€å®½é˜”çš„é£è¡Œç”²æ¿ä¸Šï¼Œ4é“é˜»æ‹¦ç´¢ã€3ä¸ªå¼¹å°„èµ·é£ä½æ ¼å¤–é†’ç›®ï¼Œæ­¼-35ã€æ­¼-15Tã€ç©ºè­¦-600ç­‰æ–°å‹èˆ°è½½æœºä¾æ¬¡åœæ”¾ã€‚ä¹ è¿‘å¹³å¬å–ç”²æ¿åŠŸèƒ½å¸ƒå±€ä»‹ç»ï¼Œä¸æ—¶é©»è¶³å¯Ÿçœ‹è£…å¤‡è®¾æ–½ã€‚ä¹ è¿‘å¹³åŒèˆ°è½½æœºé£è¡Œå‘˜äº²åˆ‡äº¤æµï¼Œè¯¦ç»†è¯¢é—®é£æœºæŠ€æˆ˜æœ¯æ€§èƒ½å’Œç”µç£å¼¹å°„ç‰¹ç‚¹ä¼˜åŠ¿ï¼Œè§‚çœ‹èˆ°è½½æœºå¼¹å°„æ”¾é£æµç¨‹æ¼”ç¤ºã€‚èº«ç€å¤šç§é¢œè‰²é©¬ç”²çš„èˆªç©ºä¿éšœäººå‘˜çœ‹åˆ°ä¹ ä¸»å¸­åˆ°æ¥ï¼Œçº·çº·å›´æ‹¢è¿‡æ¥ï¼Œå‘ä¹ ä¸»å¸­é—®å¥½ï¼ŒæŠ¥å‘Šå„è‡ªå²—ä½å’Œä¸»è¦èŒè´£ã€‚ä¹ è¿‘å¹³å‹‰åŠ±å¤§å®¶ä¸æ–­æå‡ä¸“ä¸šæŠ€èƒ½å’Œæ‰“ä»—æœ¬é¢†ï¼Œä¸ºç¦å»ºèˆ°æˆ˜æ–—åŠ›å»ºè®¾è´¡çŒ®åŠ›é‡ã€‚

ã€€ã€€éšåï¼Œä¹ è¿‘å¹³å‰å¾€ç¦å»ºèˆ°èˆ°å²›ï¼Œç™»ä¸Šå¡”å°ï¼Œäº†è§£é£è¡ŒæŒ‡æŒ¥å’Œèµ·é™è¿è¡Œæƒ…å†µã€‚ä¹ è¿‘å¹³è¿›å…¥é©¾é©¶å®¤ï¼Œå¯Ÿçœ‹å€¼å‹¤æˆ˜ä½ï¼Œåœ¨èˆªæ³Šæ—¥å¿—ä¸Šéƒ‘é‡ç­¾åã€‚ä¹ è¿‘å¹³äº²è‡ªå†³ç­–ç¦å»ºèˆ°é‡‡ç”¨ç”µç£å¼¹å°„æŠ€æœ¯ã€‚ä»–æ¥åˆ°å¼¹å°„ç»¼åˆæ§åˆ¶ç«™ï¼Œä»”ç»†è§‚æ‘©å·¥ä½œæµç¨‹ï¼ŒæŒ‰ä¸‹å¼¹å°„æŒ‰é’®ï¼Œç”²æ¿ä¸Šç©ºè½½çš„åŠ¨å­å¦‚ç¦»å¼¦ä¹‹ç®­å¼¹å‘èˆ°è‰ã€‚ä¹ è¿‘å¹³ååˆ†å…³å¿ƒèˆ°ä¸Šå®˜å…µç”Ÿæ´»ï¼Œä¸“é—¨æ¥åˆ°é¤å…å’Œå£«å…µèˆ±ï¼Œå¯Ÿçœ‹é¥®é£Ÿå’Œä½å®¿ä¿éšœæƒ…å†µï¼ŒåŒå£«å…µä»¬äº²åˆ‡äº¤æµï¼Œå®å˜±å„çº§æå¥½å„æ–¹é¢ä¿éšœï¼Œè®©å¹¿å¤§å®˜å…µæ›´å¥½æŠ•èº«éƒ¨é˜Ÿå»ºè®¾å’Œå¤‡æˆ˜æ‰“ä»—ã€‚

ã€€ã€€ç¦»åˆ«æ—¶ï¼Œå…¨èˆ°å®˜å…µä¾ä¾ä¸èˆï¼Œåœ¨é£è¡Œç”²æ¿å’Œç å¤´æ•´é½åˆ—é˜Ÿï¼Œå‘ä¹ ä¸»å¸­æ•¬ç¤¼ï¼Œé½å£°é«˜å‘¼â€œå¬å…šæŒ‡æŒ¥ã€èƒ½æ‰“èƒœä»—ã€ä½œé£ä¼˜è‰¯â€ã€‚

ã€€ã€€è”¡å¥‡ã€å¼ å›½æ¸…å‡ºå¸­ç¦å»ºèˆ°å…¥åˆ—æˆæ——ä»ªå¼ã€‚å¼ å‡æ°‘ä¸»æŒä»ªå¼ã€‚

ã€€ã€€ç¦å»ºèˆ°æ˜¯æˆ‘å›½ç¬¬ä¸€è‰˜ç”µç£å¼¹å°„å‹èˆªç©ºæ¯èˆ°ï¼Œä¹Ÿæ˜¯æˆ‘å›½ç¬¬ä¸‰è‰˜èˆªç©ºæ¯èˆ°ï¼Œèˆ·å·ä¸ºâ€œ18â€ï¼Œ2022å¹´6æœˆä¸‹æ°´å‘½åã€‚ç¦å»ºèˆ°ç”±æˆ‘å›½å®Œå…¨è‡ªä¸»è®¾è®¡å»ºé€ ï¼Œå…¶ç”µç£å¼¹å°„æŠ€æœ¯å¤„äºä¸–ç•Œå…ˆè¿›æ°´å¹³ã€‚

ã€€ã€€ä¸­å¤®å’Œå›½å®¶æœºå…³æœ‰å…³éƒ¨é—¨ã€å†›å§”æœºå…³æœ‰å…³éƒ¨é—¨ã€å—éƒ¨æˆ˜åŒºã€æµ·å†›ã€æµ·å—çœä»¥åŠèˆªæ¯å»ºè®¾å•ä½çš„è´Ÿè´£åŒå¿—å‚åŠ ä»ªå¼ã€‚

ä»Šå¹´9æœˆçš„ç›¸å…³æŠ¥é“æ˜¾ç¤ºï¼Œæ­¼-35ã€æ­¼-15Tã€ç©ºè­¦-600ä¸‰å‹èˆ°è½½æœºå·²å®Œæˆåœ¨ç¦å»ºèˆ°ä¸Šçš„é¦–æ¬¡å¼¹å°„èµ·é£å’Œç€èˆ°è®­ç»ƒï¼Œæ ‡å¿—ç€ç¦å»ºèˆ°å…·å¤‡äº†ç”µç£å¼¹å°„å’Œå›æ”¶èƒ½åŠ›ã€‚

ã€€ã€€ç”µç£å¼¹å°„å…·æœ‰æ¨åŠ›å¤§ã€æ•ˆç‡é«˜ã€ç²¾å‡†æ§åˆ¶åŠ›é“ç­‰ä¼˜åŠ¿ï¼Œè®©èˆ°è½½æœºå®ç°â€œæ»¡å¼¹æ»¡æ²¹â€èµ·é£ã€çŸ­è·èµ·é£ã€é«˜æ•ˆå‡ºåŠ¨ï¼Œè¿›ä¸€æ­¥æå‡èˆªæ¯çš„ç»¼åˆä½œæˆ˜æ•ˆèƒ½ã€‚ç›®å‰å…¨çƒåªæœ‰æå°‘æ•°å›½å®¶èƒ½å¤Ÿç†Ÿç»ƒæŒæ¡è¿™ä¸€æŠ€æœ¯ã€‚

ã€€ã€€å°šæœªå…¥åˆ—æ—¶ï¼Œç¦å»ºèˆ°å°±å·²å®ç°ä¸»è¦èˆ°è½½æœºå¼¹å°„èµ·é£ã€‚å¼ å†›ç¤¾æŒ‡å‡ºï¼Œè¿™è¯´æ˜ä¸­å›½å·²ç»èƒ½å¤Ÿå®Œå…¨æŒæ¡å’Œæˆç†Ÿè¿ç”¨ç”µç£å¼¹å°„è¿™ç§å¤æ‚çš„é£æœºèµ·é£ç³»ç»Ÿï¼Œä¹Ÿè¯´æ˜ä¸­å›½æµ·å†›å®˜å…µé©¾é©­é«˜ç§‘æŠ€è£…å¤‡çš„èƒ½åŠ›å’Œæ°´å¹³åœ¨ä¸æ–­æé«˜ã€‚

ã€€ã€€å®ç°ç”µç£å¼¹å°„èµ·é£æ˜¯ç¦å»ºèˆ°å…·å¤‡æˆ˜æ–—åŠ›çš„å…³é”®ç¯èŠ‚ã€‚ç”µç£å¼¹å°„æé«˜äº†èˆ°è½½æœºçš„å‡ºåŠ¨æ•ˆç‡ï¼Œè®©èˆªæ¯â€œå‡ºæ‹³â€æ›´å¿«ï¼›è€Œæ—¥å‰å®Œæˆå¼¹å°„èµ·é£çš„ä¸‰å‹èˆ°è½½æœºèƒ½å¤Ÿæ„æˆç©ºä¸­ä½œæˆ˜ä½“ç³»ï¼Œæ›´å¥½åœ°æ‰§è¡Œè¿›æ”»å’Œé˜²å¾¡ä»»åŠ¡ï¼Œè®©ç¦å»ºèˆ°çš„â€œæ‹³å¤´â€æ›´ç¡¬ã€‚

ã€€ã€€åˆ†æè®¤ä¸ºï¼Œæ­¼-35é£æœºèƒ½ä¸æ­¼-15Té£æœºå®ç°é«˜æ•ˆååŒå‡ºå‡»ï¼Œå¤§å¤§æå‡èˆªæ¯ç¼–é˜Ÿéšèº«çªé˜²ä¸é¥±å’Œæ‰“å‡»åŒé‡èƒ½åŠ›ï¼Œæœ€å¤§ç¨‹åº¦å‘æŒ¥èˆªæ¯èˆ°è½½æœºçš„ä½œæˆ˜èƒ½åŠ›ã€‚å‡­å€Ÿç”µç£å¼¹å°„æŠ€æœ¯ï¼Œèµ·é£é€Ÿåº¦è¾ƒæ…¢çš„ç©ºè­¦-600ä¹Ÿèƒ½ä½œä¸ºèˆ°è½½æœºå‡ºå¾è¿œæµ·ï¼Œæ“¦äº®èˆªæ¯çš„â€œåƒé‡Œçœ¼â€ã€‚

ã€€ã€€å¼ å†›ç¤¾æŒ‡å‡ºï¼Œéšç€è¿™ä¸‰å‹èˆ°è½½æœºå¼¹å°„èµ·é£å’Œç€èˆ°è®­ç»ƒæˆåŠŸï¼Œä¸­å›½æµ·å†›èˆªæ¯å…·å¤‡åˆ¶ç©ºã€åˆ¶æµ·ã€é¢„è­¦ã€ç”µå­å¯¹æŠ—ã€åæ½œèƒ½åŠ›çš„æ ¸å¿ƒèˆ°è½½æœºä½“ç³»ï¼Œå³â€œèˆªæ¯äº”ä»¶å¥—â€å·²ç»åŸºæœ¬æˆå‹ã€‚â€œæ‹³å¤´â€æ›´ç¡¬ã€â€œå‡ºæ‹³â€æ›´å¿«ï¼Œç¦å»ºèˆ°å…¥åˆ—å³å…·å¤‡æˆ˜æ–—åŠ›ï¼Œç»¼åˆä½œæˆ˜èƒ½åŠ›æœ‰äº†æ˜¾è‘—å¢å¼ºã€‚

ã€€ã€€æœ‰æŠ¥é“ç§°ï¼Œä»2024å¹´5æœˆå¯åŠ¨é¦–æ¬¡æµ·è¯•ï¼Œåˆ°2025å¹´9æœˆå®£å¸ƒå®Œæˆå…³é”®å¼¹å°„è¯•éªŒï¼Œç¦å»ºèˆ°åœ¨ä¸€å¹´å¤šæ—¶é—´å†…é¡ºåˆ©å¼€å±•å¤šæ¬¡æµ·è¯•ï¼Œè¿›åº¦è¿œè¶…é¢„æœŸã€‚

ã€€ã€€ä½œä¸ºæ–°è´¨ä½œæˆ˜åŠ›é‡çš„ä»£è¡¨ï¼Œç¦å»ºèˆ°å»ºè®¾å‘å±•ä¹‹è¿…é€Ÿå¾—ç›Šäºè¾½å®èˆ°ã€å±±ä¸œèˆ°â€œè¹šå‡ºæ¥â€çš„æˆåŠŸç»éªŒã€‚å¼ å†›ç¤¾è¯´ï¼Œè¾½å®èˆ°ã€å±±ä¸œèˆ°çš„ç»éªŒæ¢ç´¢ä¸ºåç»­èˆªæ¯çš„æ“ä½œã€è®­ç»ƒã€è¿ç”¨æä¾›äº†æå¤§çš„å€Ÿé‰´ä¸å¸®åŠ©ï¼Œâ€œæˆ‘å›½èˆªæ¯ä½“ç³»ä½œæˆ˜èƒ½åŠ›å› æ­¤æœ‰äº†å¾ˆå¤§æå‡â€ã€‚

ã€€ã€€ç¦å»ºèˆ°å…¥åˆ—æœå½¹ï¼Œä¸­å›½è¿›å…¥â€œä¸‰èˆªæ¯æ—¶ä»£â€ï¼Œè¿™å¯¹ä¸­å›½æµ·å†›çš„å‘å±•æ„å‘³ç€ä»€ä¹ˆï¼Ÿ

ã€€ã€€â€œä¸­å›½æµ·å†›çš„è¿œæµ·é˜²å¾¡ä½œæˆ˜èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨è¿œæµ·ç‹¬ç«‹ä½œæˆ˜å’Œç”Ÿå­˜çš„èƒ½åŠ›å°†è¿›ä¸€æ­¥å¢å¼ºã€‚â€å¼ å†›ç¤¾æŒ‡å‡ºï¼Œå¯ä»¥é¢„è§ï¼Œæœªæ¥ä¸­å›½ä¸‰èˆªæ¯ç¼–é˜Ÿä½œæˆ˜ï¼Œèˆ°è½½æœºå‡ºåŠ¨æ•°é‡å¤šã€é˜²ç©ºè¦†ç›–èŒƒå›´å¤§ã€åå‹¤ä¿éšœå’Œæ¥ç»­æ©æŠ¤èƒ½åŠ›å¼ºï¼Œéƒ½å°†è¿›ä¸€æ­¥æé«˜ä¸­å›½æµ·å†›åœ¨è¿œæµ·çš„æ”»é˜²å’Œç”Ÿå­˜èƒ½åŠ›ã€‚

ã€€ã€€è¿™ä½ä¸“å®¶ä¹Ÿè¡¨ç¤ºï¼Œèˆªæ¯æ˜¯å›½ä¹‹é‡å™¨ã€å¤§å›½æ ‡é…ã€‚ç¦å»ºèˆ°çš„å…¥åˆ—ä½“ç°äº†ä¸­å›½ç»¼åˆå›½åŠ›çš„å¢å¼ºå’Œç§‘æŠ€æ°´å¹³çš„æé«˜ã€‚â€œä¸‰èˆªæ¯æ—¶ä»£â€çš„åˆ°æ¥å°†è¿›ä¸€æ­¥æ‰©å¤§ä¸­å›½é˜²å¾¡ä½œæˆ˜çš„çºµæ·±ï¼Œå¢å¼ºä¸­å›½å†›é˜Ÿâ€œå¾¡æ•Œäºå¤–â€çš„èƒ½åŠ›ã€‚(å®Œ)
    """
    
    # ç¤ºä¾‹1ï¼šé™åˆ¶å­—æ•°æ¨¡å¼ (å¼‚æ­¥éæµå¼)
    print("=== é™åˆ¶å­—æ•°æ¨¡å¼ (éæµå¼) ===")
    result1 = await agent.run(
        text=test_text,
        target_words=200,  # æŒ‡å®šç›®æ ‡å­—æ•°
        summary_type="è¦ç‚¹æ‘˜è¦",
        ratio=1.33,
        request_id="test-summary-001"
    )
    
    # ä»è£å‰ªåçš„ç»“æœä¸­è·å–ä¿¡æ¯
    output_data = result1.get("output", {})
    confidence = result1.get("confidence", 0.0)

    print(f"æ¨¡å¼: {'é™åˆ¶å­—æ•°' if output_data['word_limit_mode'] else 'è‡ªç”±é•¿åº¦'}")
    print(f"åŸæ–‡é•¿åº¦: {len(test_text)} å­—")
    print(f"æ‘˜è¦é•¿åº¦: {output_data['actual_words']} å­— (ç›®æ ‡: {output_data['target_words']} å­—)")
    print(f"è´¨é‡è¯„åˆ†: {output_data['quality_score']:.2f}/10")
    print(f"åˆ†å—æ•°é‡: {output_data['chunk_count']} å—")
    print(f"ç½®ä¿¡åº¦: {confidence:.2f}")
    print(f"\næ‘˜è¦å†…å®¹:\n{output_data['summarized_text']}")
    
    print("\n" + "="*50 + "\n")
    
    # ç¤ºä¾‹2ï¼šä¸é™åˆ¶å­—æ•°æ¨¡å¼ (å¼‚æ­¥æµå¼)
    print("=== ä¸é™åˆ¶å­—æ•°æ¨¡å¼ (æµå¼) ===")
    full_stream_summary = ""
    async for chunk in agent.run_stream(
        text=test_text,
        target_words=None,  # Noneè¡¨ç¤ºä¸é™åˆ¶å­—æ•°
        summary_type="æ–°é—»æ‘˜è¦", 
        request_id="test-summary-002"
    ):
        if chunk.type == "thinking":
            print(f"ğŸ¤” {chunk.content}")
        elif chunk.type == "processing":
            print(f"ğŸ”„ {chunk.content}")
        elif chunk.type == "summary":
            print(f"{chunk.content}", end="", flush=True)
            full_stream_summary += chunk.content
        elif chunk.type == "final":
            result = json.loads(chunk.content)
            print("\n") # ç¡®ä¿æ¢è¡Œ
            status = "æˆåŠŸ" if result["success"] else "å¤±è´¥"
            print(f"âœ… æµå¼æ‘˜è¦å®Œæˆ: {status}")
            print(f"æ¨¡å¼: {'é™åˆ¶å­—æ•°' if result['word_limit_mode'] else 'è‡ªç”±é•¿åº¦'}")
            print(f"æ‘˜è¦ç±»å‹: {result['summary_type']}")
            print(f"åŸæ–‡é•¿åº¦: {len(test_text)} å­—")
            print(f"å®é™…æ‘˜è¦é•¿åº¦: {result['actual_words']} å­— (æµå¼æ¥æ”¶åˆ°: {len(full_stream_summary)} å­—)")
            print(f"è´¨é‡è¯„åˆ†: {result['quality_score']:.2f}/10")
            print(f"åˆ†å—æ•°é‡: {result['chunk_count']} å—")
            print(f"ç½®ä¿¡åº¦: {result['confidence']:.2f}")

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())