import asyncio
import json
import re
import sys
from pathlib import Path
from typing import AsyncGenerator, TypedDict, Annotated, List, Union, Optional, Dict, Any

from pydantic import BaseModel, Field
from langgraph.graph import StateGraph, END
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.runnables import RunnableConfig
from langchain_core.messages.utils import convert_to_openai_messages

dir_name = Path(__file__).resolve().parent.parent.parent
sys.path.append(str(dir_name))

from base_agent import BaseAgent
from applications.query_rewriting.query_rewriting_prompt import (
    QUERY_REWRITE_SYSTEM_MESSAGE,
    QUERY_REWRITE_PROMPT
)
from llm_api.llm_client_chat_model import LLMClientChatModel
from utils.time_count import timer
from utils.stream_chunk import StreamChunk
from utils.schema_parse import SchemaParser


# ===== è¾“å‡ºç»“æ„å®šä¹‰ =====
class QueryRewriteOutput(BaseModel):
    success: bool = Field(description="æŸ¥è¯¢æ”¹å†™æ˜¯å¦æˆåŠŸ")
    original_query: str = Field(description="åŸå§‹æŸ¥è¯¢")
    rewritten_queries: List[Dict[str, str]] = Field(description="æ”¹å†™åçš„æŸ¥è¯¢åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å«queryå’Œstrategy")
    optimization_notes: str = Field(description="ä¼˜åŒ–è¯´æ˜")
    validation_errors: List[str] = Field(default_factory=list, description="éªŒè¯é”™è¯¯ä¿¡æ¯")
    confidence: float = Field(description="æ•´ä½“ç½®ä¿¡åº¦", ge=0, le=1)
    statistics: Dict[str, Any] = Field(description="ç»Ÿè®¡ä¿¡æ¯")


# ===== æ”¹å†™æŸ¥è¯¢é¡¹å®šä¹‰ =====
class RewrittenQueryItem(BaseModel):
    rewritten_query: str = Field(description="æ”¹å†™åçš„æŸ¥è¯¢æ–‡æœ¬")
    rewritten_strategy: str = Field(description="ä½¿ç”¨çš„æ”¹å†™ç­–ç•¥")


# ===== LLMè¾“å‡ºç»“æ„å®šä¹‰ =====
class LLMRewriteOutput(BaseModel):
    """LLMè¾“å‡ºçš„æŸ¥è¯¢æ”¹å†™ç»“æœ"""
    rewritten_queries: List[RewrittenQueryItem] = Field(
        description="æ”¹å†™åçš„æŸ¥è¯¢åˆ—è¡¨ï¼ŒæŒ‰ä¼˜åŒ–æ•ˆæœä»å¥½åˆ°å·®æ’åºï¼Œæ¯ä¸ªæŸ¥è¯¢åŒ…å«æŸ¥è¯¢æ–‡æœ¬å’Œä½¿ç”¨çš„ç­–ç•¥"
    )
    optimization_notes: str = Field(
        description="ä¼˜åŒ–è¯´æ˜å’Œä¸»è¦é‡‡ç”¨çš„ç­–ç•¥æ€»ç»“"
    )


# ===== çŠ¶æ€å®šä¹‰ =====
class QueryRewriteState(TypedDict):
    messages: Annotated[List[Union[HumanMessage, AIMessage]], "æ¶ˆæ¯å†å²"]
    current_query: str
    conversation_history: List[Dict[str, Any]]
    max_rewrites: int
    preserve_system: bool
    domain_context: Optional[str]
    rewritten_queries: List[Dict[str, str]]  # æ”¹ä¸ºå­—å…¸åˆ—è¡¨ï¼ŒåŒ…å«queryå’Œstrategy
    optimization_notes: str
    validation_errors: List[str]
    statistics: Dict[str, Any]
    final_output: Optional[dict]


# ===== æŸ¥è¯¢æ”¹å†™Agentä¸»ç±» =====
class QueryRewriteAgent(BaseAgent):
    def __init__(
            self,
            name: str = "query-rewrite-agent",
            # openai client init config
            base_url: str = "https://api.deepseek.com/v1",
            api_key: Optional[str] = None,
            timeout: float = 60.0,
            max_retries: int = 3,
            # openai client run config
            model: str = "deepseek-chat",
            max_tokens: Optional[int] = None,
            temperature: float = 0.3,  # ç¨é«˜çš„æ¸©åº¦ä»¥äº§ç”Ÿå¤šæ ·æ€§
            top_p: float = 1.0,
            stream: bool = False,
            enable_thinking: bool = False,
            default_max_rewrites: int = 5
    ):
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        super().__init__(
            name=name,
            model=model,
            base_url=base_url,
            api_key=api_key,
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
            timeout=timeout,
            max_retries=max_retries,
            stream=stream,
            enable_thinking=enable_thinking,
        )

        self.default_max_rewrites = default_max_rewrites
        
        # åˆå§‹åŒ–SchemaParser
        self.schema_parser = SchemaParser(LLMRewriteOutput)
        
        # æ„å»ºå·¥ä½œæµå›¾
        self.graph = self._build_graph()

    def _build_graph(self) -> StateGraph:
        """æ„å»º LangGraph å·¥ä½œæµ"""
        graph = StateGraph(QueryRewriteState)

        def initialize_node(state: QueryRewriteState, config: RunnableConfig) -> QueryRewriteState:
            """åˆå§‹åŒ–èŠ‚ç‚¹ï¼šå‡†å¤‡æŸ¥è¯¢æ”¹å†™ä»»åŠ¡"""
            run_config = config.get("configurable", {})
            request_id = run_config.get("request_id")

            with timer(self.logger, f"request_id: {request_id}, åˆå§‹åŒ–æŸ¥è¯¢æ”¹å†™ä»»åŠ¡"):
                self.logger.info(f"request_id: {request_id}, å¼€å§‹æŸ¥è¯¢æ”¹å†™, å½“å‰æŸ¥è¯¢: '{state['current_query']}'")
                
                # è®¾ç½®é»˜è®¤å€¼
                if not state.get("max_rewrites") or state["max_rewrites"] <= 0:
                    state["max_rewrites"] = self.default_max_rewrites
                
                if not state.get("domain_context"):
                    state["domain_context"] = "é€šç”¨é¢†åŸŸ"
                
                if state.get("preserve_system") is None:
                    state["preserve_system"] = True
                
                # å¤„ç†å¯¹è¯å†å²
                processed_history = self._process_conversation_history(
                    state.get("conversation_history", []),
                    state["preserve_system"]
                )
                state["conversation_history"] = processed_history
                
                self.logger.debug(f"request_id: {request_id}, æœ€å¤§æ”¹å†™æ•°: {state['max_rewrites']}, å†å²æ¶ˆæ¯æ•°: {len(processed_history)}")
                
                return state

        async def rewrite_node(state: QueryRewriteState, config: RunnableConfig) -> QueryRewriteState:
            """æ”¹å†™èŠ‚ç‚¹ï¼šæ‰§è¡ŒæŸ¥è¯¢æ”¹å†™"""
            run_config = config.get("configurable", {})
            request_id = run_config.get("request_id")

            with timer(self.logger, f"request_id: {request_id}, æ‰§è¡ŒæŸ¥è¯¢æ”¹å†™"):
                # æ„å»ºæç¤ºè¯
                prompt_text = self._build_rewrite_prompt(
                    state["current_query"],
                    state["conversation_history"],
                    state["domain_context"],
                    state["max_rewrites"]
                )

                self.logger.info(f"request_id: {request_id}, è°ƒç”¨LLMè¿›è¡ŒæŸ¥è¯¢æ”¹å†™, æŸ¥è¯¢é•¿åº¦: {len(state['current_query'])}")

                system_message_content = QUERY_REWRITE_SYSTEM_MESSAGE
                messages = [SystemMessage(content=system_message_content), HumanMessage(content=prompt_text)]

                # è°ƒç”¨LLM
                llm_client = self.get_llm_client(run_config)
                chat_model = LLMClientChatModel(llm_client=llm_client)
                try:
                    response = await chat_model.ainvoke(messages, config=config)

                    chat_completion = response.chat_completion.to_dict()
                    choices = chat_completion.get("choices", [])
                    
                    # åˆå§‹åŒ–æœ€ç»ˆè¾“å‡ºç»“æ„
                    final_output = {
                        "metadata": {
                            "usage": chat_completion.get("usage", {}),
                            # "messages": []
                        }
                    }

                    if len(choices) > 0:
                        content = choices[0].get("message", {}).get("content", "")
                        reasoning_content = choices[0].get("message", {}).get("reasoning_content", "")

                        self.logger.debug(f"request_id: {request_id}, LLMå“åº”é•¿åº¦: {len(content)}")

                        # ä½¿ç”¨SchemaParserè§£æå“åº”
                        rewrite_result = self.schema_parser.parse_response_to_json(content)
                        
                        final_output["output"] = rewrite_result
                        
                        # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
                        statistics = self._generate_statistics(rewrite_result, state["current_query"])
                        
                        # æ›´æ–°æ¶ˆæ¯å†å²
                        new_messages = state["messages"] + messages + [AIMessage(content=content)]
                        # final_output["metadata"]["messages"] = convert_to_openai_messages(new_messages)
                        
                        final_output["content"] = content
                        final_output["reasoning_content"] = reasoning_content

                        self.logger.info(f"request_id: {request_id}, LLM Parse Output: {json.dumps(rewrite_result, ensure_ascii=False)}")
                        
                        return {
                            **state,
                            "messages": new_messages,
                            "rewritten_queries": rewrite_result["rewritten_queries"],
                            "optimization_notes": rewrite_result["optimization_notes"],
                            "statistics": statistics,
                            "final_output": final_output
                        }
                    else:
                        raise ValueError("LLM apiè¾“å‡ºé”™è¯¯, choicesä¸ºç©º")
                except asyncio.CancelledError:
                    self.logger.warning(f"â›” request_id: {request_id}, ä»»åŠ¡è¢«ä¸­æ–­ï¼Œå·²åœæ­¢ LLM è¯·æ±‚")
                    raise

        def validate_node(state: QueryRewriteState, config: RunnableConfig) -> QueryRewriteState:
            """éªŒè¯èŠ‚ç‚¹ï¼šéªŒè¯æ”¹å†™çš„æŸ¥è¯¢"""
            run_config = config.get("configurable", {})
            request_id = run_config.get("request_id")

            with timer(self.logger, f"request_id: {request_id}, éªŒè¯æ”¹å†™çš„æŸ¥è¯¢"):
                validation_errors = []
                rewritten_queries = state["rewritten_queries"]
                
                # éªŒè¯åŸºæœ¬æ ¼å¼
                if not isinstance(rewritten_queries, list):
                    validation_errors.append("æ”¹å†™æŸ¥è¯¢æ ¼å¼é”™è¯¯ï¼šåº”è¯¥æ˜¯ä¸€ä¸ªåˆ—è¡¨")
                
                if len(rewritten_queries) == 0:
                    validation_errors.append("æœªç”Ÿæˆä»»ä½•æ”¹å†™æŸ¥è¯¢")
                
                # éªŒè¯æ¯ä¸ªæ”¹å†™æŸ¥è¯¢é¡¹
                for i, query_item in enumerate(rewritten_queries):
                    if not isinstance(query_item, dict):
                        validation_errors.append(f"ç¬¬{i+1}ä¸ªæ”¹å†™æŸ¥è¯¢é¡¹ä¸æ˜¯å­—å…¸ç±»å‹")
                        continue
                    
                    # éªŒè¯å¿…è¦å­—æ®µ
                    if "rewritten_query" not in query_item:
                        validation_errors.append(f"ç¬¬{i+1}ä¸ªæ”¹å†™æŸ¥è¯¢é¡¹ç¼ºå°‘'rewritten_query'å­—æ®µ")
                    
                    if "rewritten_strategy" not in query_item:
                        validation_errors.append(f"ç¬¬{i+1}ä¸ªæ”¹å†™æŸ¥è¯¢é¡¹ç¼ºå°‘'rewritten_strategy'å­—æ®µ")
                    
                    # éªŒè¯æŸ¥è¯¢æ–‡æœ¬
                    query_text = query_item.get("rewritten_query", "")
                    if not isinstance(query_text, str):
                        validation_errors.append(f"ç¬¬{i+1}ä¸ªæ”¹å†™æŸ¥è¯¢æ–‡æœ¬ä¸æ˜¯å­—ç¬¦ä¸²ç±»å‹")
                    elif not query_text.strip():
                        validation_errors.append(f"ç¬¬{i+1}ä¸ªæ”¹å†™æŸ¥è¯¢æ–‡æœ¬ä¸ºç©º")
                    elif len(query_text) < 3:
                        validation_errors.append(f"ç¬¬{i+1}ä¸ªæ”¹å†™æŸ¥è¯¢'{query_text}'è¿‡çŸ­")
                    elif len(query_text) > 500:
                        validation_errors.append(f"ç¬¬{i+1}ä¸ªæ”¹å†™æŸ¥è¯¢è¿‡é•¿({len(query_text)}å­—ç¬¦)")
                    
                    # éªŒè¯ç­–ç•¥æè¿°
                    strategy = query_item.get("rewritten_strategy", "")
                    if not isinstance(strategy, str):
                        validation_errors.append(f"ç¬¬{i+1}ä¸ªç­–ç•¥æè¿°ä¸æ˜¯å­—ç¬¦ä¸²ç±»å‹")
                    elif not strategy.strip():
                        validation_errors.append(f"ç¬¬{i+1}ä¸ªç­–ç•¥æè¿°ä¸ºç©º")
                
                if not validation_errors:
                    self.logger.info(f"request_id: {request_id}, æŸ¥è¯¢æ”¹å†™éªŒè¯é€šè¿‡, ç”Ÿæˆ{len(rewritten_queries)}ä¸ªæ”¹å†™ç‰ˆæœ¬")
                else:
                    error_count = len(validation_errors)
                    self.logger.warning(f"request_id: {request_id}, å‘ç° {error_count} ä¸ªéªŒè¯é—®é¢˜")
                    for error in validation_errors[:3]:
                        self.logger.debug(f"æŸ¥è¯¢æ”¹å†™éªŒè¯é—®é¢˜: {error}")

                state["validation_errors"] = validation_errors
                return state

        def finalize_node(state: QueryRewriteState, config: RunnableConfig) -> QueryRewriteState:
            """æœ€ç»ˆå¤„ç†èŠ‚ç‚¹ï¼šæ±‡æ€»ç»“æœ"""
            run_config = config.get("configurable", {})
            request_id = run_config.get("request_id")

            with timer(self.logger, f"request_id: {request_id}, ç»“æœæ±‡æ€»"):
                success = len(state["validation_errors"]) == 0
                
                # è®¡ç®—ç½®ä¿¡åº¦
                base_confidence = max(0.0, 1.0 - len(state["validation_errors"]) * 0.15)
                
                # æ ¹æ®æ”¹å†™æ•°é‡å’Œè´¨é‡è°ƒæ•´ç½®ä¿¡åº¦
                rewrite_count = len(state["rewritten_queries"])
                expected_count = state["max_rewrites"]
                count_ratio = min(rewrite_count / expected_count, 1.0) if expected_count > 0 else 1.0
                
                # å¤šæ ·æ€§è¯„ä¼°ï¼ˆåŸºäºæŸ¥è¯¢ç›¸ä¼¼åº¦ï¼‰
                diversity_score = self._calculate_diversity_score([item["rewritten_query"] for item in state["rewritten_queries"]])
                confidence = base_confidence * 0.6 + count_ratio * 0.2 + diversity_score * 0.2
                
                # æ„å»ºæœ€ç»ˆè¾“å‡ºï¼Œä¿ç•™metadataä¿¡æ¯
                final_output = state.get("final_output", {})
                final_output.update({
                    # "success": success,
                    # "original_query": state["current_query"],
                    # "rewritten_queries": state["rewritten_queries"],
                    # "optimization_notes": state["optimization_notes"],
                    # "validation_errors": state["validation_errors"],
                    "confidence": confidence,
                    # "statistics": state["statistics"],
                    # "query_length": len(state["current_query"]),
                    # "rewrite_count": rewrite_count,
                    # "max_rewrites_set": state["max_rewrites"],
                    # "history_messages_count": len(state["conversation_history"])
                })

                status_msg = "æˆåŠŸ" if success else f"æœ‰{len(state['validation_errors'])}ä¸ªè­¦å‘Š"
                self.logger.success(f"request_id: {request_id}, æŸ¥è¯¢æ”¹å†™å®Œæˆ, çŠ¶æ€: {status_msg}, ç½®ä¿¡åº¦: {confidence:.2f}, ç”Ÿæˆ{rewrite_count}ä¸ªæ”¹å†™ç‰ˆæœ¬")

                return {
                    **state,
                    "final_output": final_output
                }

        # æ·»åŠ èŠ‚ç‚¹
        graph.add_node("initialize", initialize_node)
        graph.add_node("rewrite", rewrite_node)
        graph.add_node("validate", validate_node)
        graph.add_node("finalize", finalize_node)

        # è®¾ç½®å·¥ä½œæµ
        graph.set_entry_point("initialize")
        graph.add_edge("initialize", "rewrite")
        graph.add_edge("rewrite", "validate")
        graph.add_edge("validate", "finalize")
        graph.add_edge("finalize", END)

        return graph.compile()

    def _convert_llm_output(self, llm_output: LLMRewriteOutput, original_query: str) -> Dict[str, Any]:
        """è½¬æ¢LLMè¾“å‡ºæ ¼å¼"""
        try:
            # è½¬æ¢RewrittenQueryItemå¯¹è±¡ä¸ºå­—å…¸
            rewritten_queries = []
            for item in llm_output.rewritten_queries:
                rewritten_queries.append({
                    "rewritten_query": item.rewritten_query,
                    "rewritten_strategy": item.rewritten_strategy
                })
            
            return {
                "rewritten_queries": rewritten_queries,
                "optimization_notes": llm_output.optimization_notes
            }
        except Exception as e:
            self.logger.error(f"è½¬æ¢LLMè¾“å‡ºå¤±è´¥: {e}")
            # è¿”å›é»˜è®¤ç»“æœ
            return {
                "rewritten_queries": [{
                    "rewritten_query": original_query,
                    "rewritten_strategy": "åŸå§‹æŸ¥è¯¢"
                }],
                "optimization_notes": "è§£æå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢"
            }

    def _process_conversation_history(self, history: List[Dict[str, Any]], preserve_system: bool) -> List[Dict[str, Any]]:
        """å¤„ç†å¯¹è¯å†å²"""
        if not history:
            return []
        
        processed_history = []
        for msg in history:
            if isinstance(msg, dict):
                role = msg.get("role", "")
                content = msg.get("content", "")
                
                # æ ¹æ®preserve_systemå†³å®šæ˜¯å¦ä¿ç•™systemæ¶ˆæ¯
                if role == "system" and not preserve_system:
                    continue
                
                if content and isinstance(content, str):
                    processed_history.append({
                        "role": role,
                        "content": content[:1000]  # é™åˆ¶é•¿åº¦
                    })
        
        return processed_history[-10:]  # åªä¿ç•™æœ€è¿‘10æ¡æ¶ˆæ¯

    def _build_rewrite_prompt(self, current_query: str, conversation_history: List[Dict[str, Any]], 
                            domain_context: str, max_rewrites: int) -> str:
        """æ„å»ºæŸ¥è¯¢æ”¹å†™æç¤ºè¯"""
        
        # æ ¼å¼åŒ–å¯¹è¯å†å²
        history_text = "æ— "
        if conversation_history:
            history_lines = []
            for msg in conversation_history:
                role = msg.get("role", "")
                content = msg.get("content", "")
                if role and content:
                    history_lines.append(f"{role}: {content}")
            history_text = "\n".join(history_lines)
        
        return QUERY_REWRITE_PROMPT.format(
            current_query=current_query,
            conversation_history=history_text,
            domain_context=domain_context,
            max_rewrites=max_rewrites
        )

    def _generate_statistics(self, rewrite_result: Dict[str, Any], original_query: str) -> Dict[str, Any]:
        """ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
        queries = [item["rewritten_query"] for item in rewrite_result["rewritten_queries"]]
        strategies = [item["rewritten_strategy"] for item in rewrite_result["rewritten_queries"]]
        original_length = len(original_query)
        
        return {
            "total_rewrites": len(queries),
            "average_query_length": round(sum(len(q) for q in queries) / len(queries), 2) if queries else 0,
            "original_query_length": original_length,
            "length_change_ratio": round((sum(len(q) for q in queries) / len(queries) - original_length) / original_length, 4) if original_length > 0 else 0,
            "unique_strategies": len(set(strategies)),
            "diversity_score": self._calculate_diversity_score(queries)
        }

    def _calculate_diversity_score(self, queries: List[str]) -> float:
        """è®¡ç®—æŸ¥è¯¢å¤šæ ·æ€§å¾—åˆ†"""
        if len(queries) <= 1:
            return 0.0
        
        # ç®€å•çš„å¤šæ ·æ€§è¯„ä¼°ï¼šåŸºäºè¯æ±‡é‡å åº¦
        total_similarity = 0
        count = 0
        
        for i in range(len(queries)):
            for j in range(i + 1, len(queries)):
                words_i = set(queries[i].lower().split())
                words_j = set(queries[j].lower().split())
                
                if words_i and words_j:
                    overlap = len(words_i & words_j) / len(words_i | words_j)
                    total_similarity += overlap
                    count += 1
        
        if count == 0:
            return 1.0
        
        average_similarity = total_similarity / count
        return max(0.0, 1.0 - average_similarity)

    async def run(self, query: str, conversation_history: Optional[List[Dict[str, Any]]] = None, 
                 max_rewrites: Optional[int] = None, preserve_system: bool = True,
                 domain_context: Optional[str] = None, **kwargs) -> Dict[str, Any]:
        """
        æ‰§è¡ŒæŸ¥è¯¢æ”¹å†™æµç¨‹

        :param query: å½“å‰æŸ¥è¯¢
        :param conversation_history: å¯¹è¯å†å² (OpenAI messagesæ ¼å¼)
        :param max_rewrites: æœ€å¤§æ”¹å†™æ•°é‡
        :param preserve_system: æ˜¯å¦ä¿ç•™systemæ¶ˆæ¯
        :param domain_context: é¢†åŸŸä¸Šä¸‹æ–‡
        :return: ç»“æ„åŒ–è¾“å‡ºå­—å…¸
        """
        if not query.strip():
            raise ValueError("æŸ¥è¯¢ä¸èƒ½ä¸ºç©º")
        
        request_id = kwargs.get("request_id")

        self.logger.info(f"ğŸ”§ request_id: {request_id}, å¼€å§‹å¤„ç†æŸ¥è¯¢æ”¹å†™è¯·æ±‚, æŸ¥è¯¢: '{query}'")

        # æ„å»ºè¿è¡Œæ—¶é…ç½®
        run_config = {k: v for k, v in kwargs.items() if k in self.init_config or k == "request_id"}

        inputs = {
            "messages": [],
            "current_query": query,
            "conversation_history": conversation_history or [],
            "max_rewrites": max_rewrites or self.default_max_rewrites,
            "preserve_system": preserve_system,
            "domain_context": domain_context,
            "rewritten_queries": [],
            "optimization_notes": "",
            "validation_errors": [],
            "statistics": {},
            "final_output": None
        }

        with timer(self.logger, f"request_id: {request_id}, å®Œæ•´æŸ¥è¯¢æ”¹å†™æµç¨‹"):
            # ä¼ é€’è¿è¡Œæ—¶é…ç½®
            config = {"configurable": run_config} if run_config else {}
            final_state = await self.graph.ainvoke(inputs, config=config)
            output = final_state.get("final_output", {})
            self.logger.success(f"ğŸ‰ request_id: {request_id}, æŸ¥è¯¢æ”¹å†™å®Œæˆ")

        return output
    
    async def run_stream(self, query: str, conversation_history: Optional[List[Dict[str, Any]]] = None,
                        max_rewrites: Optional[int] = None, preserve_system: bool = True,
                        domain_context: Optional[str] = None, **kwargs) -> AsyncGenerator[StreamChunk, None]:
        """
        æµå¼æ‰§è¡ŒæŸ¥è¯¢æ”¹å†™æµç¨‹

        :param query: å½“å‰æŸ¥è¯¢
        :param conversation_history: å¯¹è¯å†å² (OpenAI messagesæ ¼å¼)
        :param max_rewrites: æœ€å¤§æ”¹å†™æ•°é‡
        :param preserve_system: æ˜¯å¦ä¿ç•™systemæ¶ˆæ¯
        :param domain_context: é¢†åŸŸä¸Šä¸‹æ–‡
        :return: æµå¼è¾“å‡ºç”Ÿæˆå™¨
        """
        if not query.strip():
            raise ValueError("æŸ¥è¯¢ä¸èƒ½ä¸ºç©º")
        
        request_id = kwargs.get("request_id")

        self.logger.info(f"ğŸ”§ request_id: {request_id}, å¼€å§‹æµå¼å¤„ç†æŸ¥è¯¢æ”¹å†™è¯·æ±‚, æŸ¥è¯¢: '{query}'")

        # æ„å»ºè¿è¡Œæ—¶é…ç½®
        run_config = {k: v for k, v in kwargs.items() if k in self.init_config or k == "request_id"}

        inputs = {
            "messages": [],
            "current_query": query,
            "conversation_history": conversation_history or [],
            "max_rewrites": max_rewrites or self.default_max_rewrites,
            "preserve_system": preserve_system,
            "domain_context": domain_context,
            "rewritten_queries": [],
            "optimization_notes": "",
            "validation_errors": [],
            "statistics": {},
            "final_output": None
        }

        with timer(self.logger, f"request_id: {request_id}, å®Œæ•´æµå¼æŸ¥è¯¢æ”¹å†™æµç¨‹"):
            # ä¼ é€’è¿è¡Œæ—¶é…ç½®
            config = {"configurable": run_config} if run_config else {}
            
            async for event in self.graph.astream_events(inputs, config=config):
                event_type = event.get("event", "")
                
                 # å¤„ç†LLMæµå¼è¾“å‡º
                if event_type == "on_chat_model_stream":
                    chunk = event.get("data", {}).get("chunk", None)
                    if chunk and hasattr(chunk, "chat_completion_chunk") and chunk.chat_completion_chunk:
                        chat_completion_chunk = chunk.chat_completion_chunk.to_dict()
                        choices = chat_completion_chunk.get("choices", [])
                        if choices:
                            delta = choices[0].get("delta", {})
                            content = delta.get("content", "")
                            reasoning_content = delta.get("reasoning_content", "")
                            
                            # è¾“å‡ºæ€è€ƒå†…å®¹
                            if reasoning_content:
                                yield StreamChunk(
                                    type="thinking",
                                    content=reasoning_content
                                )
                            # è¾“å‡ºç”Ÿæˆå†…å®¹
                            elif content:
                                yield StreamChunk(
                                    type="content", 
                                    content=content
                                )
                
                # å¤„ç†èŠ‚ç‚¹å¼€å§‹äº‹ä»¶
                elif event_type == "on_chain_start":
                    name = event.get("name", "")
                    if name == "initialize":
                        yield StreamChunk(
                            type="processing",
                            content="å¼€å§‹åˆå§‹åŒ–æŸ¥è¯¢æ”¹å†™ä»»åŠ¡..."
                        )
                    elif name == "rewrite":
                        yield StreamChunk(
                            type="processing",
                            content="æ­£åœ¨åˆ†ææŸ¥è¯¢å’Œå¯¹è¯å†å²ï¼Œç”Ÿæˆä¼˜åŒ–ç‰ˆæœ¬..."
                        )
                    elif name == "validate":
                        yield StreamChunk(
                            type="processing", 
                            content="æ­£åœ¨éªŒè¯æ”¹å†™çš„æŸ¥è¯¢è´¨é‡..."
                        )
                    elif name == "finalize":
                        yield StreamChunk(
                            type="processing",
                            content="æ­£åœ¨æ±‡æ€»æœ€ç»ˆç»“æœ..."
                        )
                
                # å¤„ç†å›¾ç»“æŸäº‹ä»¶, è¾“å‡ºæœ€ç»ˆç»“æœ
                elif event_type == "on_chain_end" and event.get("name", "") == "LangGraph":
                    output = event.get("data", {}).get("output", {})
                    final_output = output.get("final_output", {})
                        
                    yield StreamChunk(
                        type="final",
                        content="",
                        metadata=final_output
                    )

            self.logger.success(f"ğŸ‰ request_id: {request_id}, æµå¼æŸ¥è¯¢æ”¹å†™å®Œæˆ")


# ===== ä½¿ç”¨ç¤ºä¾‹ =====
async def main():
    """ä½¿ç”¨ç¤ºä¾‹"""
    
    # æµ‹è¯•æŸ¥è¯¢å’Œå¯¹è¯å†å²
    TEST_QUERY = "å®ƒæ€ä¹ˆå®‰è£…ï¼Ÿ"
    TEST_HISTORY = [
        {"role": "user", "content": "æˆ‘æƒ³äº†è§£Dockerå®¹å™¨æŠ€æœ¯"},
        {"role": "assistant", "content": "Dockeræ˜¯ä¸€ç§å®¹å™¨åŒ–å¹³å°ï¼Œå¯ä»¥å¸®åŠ©æ‚¨æ‰“åŒ…ã€åˆ†å‘å’Œè¿è¡Œåº”ç”¨ç¨‹åºã€‚"},
        {"role": "user", "content": "é‚£Docker Composeå‘¢ï¼Ÿ"},
        {"role": "assistant", "content": "Docker Composeæ˜¯ä¸€ä¸ªç”¨äºå®šä¹‰å’Œè¿è¡Œå¤šå®¹å™¨Dockeråº”ç”¨ç¨‹åºçš„å·¥å…·ã€‚"},
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæŠ€æœ¯åŠ©æ‰‹ï¼Œä¸“é—¨å›ç­”Dockerç›¸å…³é—®é¢˜"}
    ]
    
    # åˆå§‹åŒ–Agent
    agent = QueryRewriteAgent(
        name="test-query-rewrite-agent",
        base_url="https://api.deepseek.com/v1",
        api_key="YOUR_API_KEY",  # æ›¿æ¢ä¸ºå®é™…APIå¯†é’¥
        temperature=0.3,
        default_max_rewrites=4
    )
    
    # æµå¼å¤„ç†ç¤ºä¾‹
    print("=== æµå¼æŸ¥è¯¢æ”¹å†™ ===")
    async for chunk in agent.run_stream(
        TEST_QUERY, 
        conversation_history=TEST_HISTORY,
        domain_context="Dockerå®¹å™¨æŠ€æœ¯",
        max_rewrites=3,
        preserve_system=False,
        request_id="test-rewrite-001"
    ):
        if chunk.type == "thinking":
            print(f"ğŸ¤” {chunk.content}")
        elif chunk.type == "content":
            print(f"{chunk.content}", end="", flush=True)
        elif chunk.type == "processing":
            print(f"ğŸ”„ {chunk.content}")
        elif chunk.type == "final":
            result = chunk.metadata
            status = "æˆåŠŸ" if result["success"] else f"æœ‰{len(result['validation_errors'])}ä¸ªè­¦å‘Š"
            print(f"\nâœ… æŸ¥è¯¢æ”¹å†™å®Œæˆ: {status}, ç½®ä¿¡åº¦: {result['confidence']:.2f}")
            print(f"ğŸ“Š å®é™…ç”Ÿæˆæ”¹å†™æ•°: {result['rewrite_count']}/{result['max_rewrites_set']}")
            print(f"ğŸ“‹ æ”¹å†™çš„æŸ¥è¯¢:")
            for i, query_item in enumerate(result["rewritten_queries"], 1):
                print(f"  {i}. [{query_item['rewritten_strategy']}] {query_item['rewritten_query']}")
            print(f"ğŸ“ ä¼˜åŒ–è¯´æ˜: {result['optimization_notes']}")
    
    print("\n" + "="*50 + "\n")

    quit()
    
    # åŒæ­¥å¤„ç†ç¤ºä¾‹
    print("=== åŒæ­¥æŸ¥è¯¢æ”¹å†™ ===")
    result = await agent.run(
        TEST_QUERY,
        conversation_history=TEST_HISTORY,
        domain_context="Dockerå®¹å™¨æŠ€æœ¯",
        max_rewrites=4,
        preserve_system=True,
        request_id="test-rewrite-002"
    )
    
    # æ‰“å°ç»“æœ
    print(f"ğŸ“Š æŸ¥è¯¢æ”¹å†™ç»“æœ:")
    print(f"  çŠ¶æ€: {'âœ… æˆåŠŸ' if result['success'] else 'âš ï¸ æœ‰è­¦å‘Š'}")
    print(f"  ç½®ä¿¡åº¦: {result['confidence']:.2f}")
    print(f"  åŸå§‹æŸ¥è¯¢: '{result['original_query']}'")
    print(f"  æ”¹å†™æ•°é‡: {result['rewrite_count']}/{result['max_rewrites_set']}")
    print(f"  å†å²æ¶ˆæ¯æ•°: {result['history_messages_count']}")
    
    if result['validation_errors']:
        print(f"  éªŒè¯è­¦å‘Š: {len(result['validation_errors'])} ä¸ª")
        for error in result['validation_errors'][:2]:
            print(f"    - {error}")
    
    print(f"\nğŸ“‹ æ”¹å†™çš„æŸ¥è¯¢ (æŒ‰ä¼˜åŒ–æ•ˆæœæ’åº):")
    for i, query_item in enumerate(result['rewritten_queries'], 1):
        print(f"  {i}. [{query_item['rewritten_strategy']}] {query_item['rewritten_query']}")
    
    print(f"\nğŸ“ ä¼˜åŒ–è¯´æ˜:")
    print(result['optimization_notes'])
    
    print(f"\nğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯:")
    stats = result['statistics']
    print(f"  - å¹³å‡æŸ¥è¯¢é•¿åº¦: {stats['average_query_length']} å­—ç¬¦")
    print(f"  - é•¿åº¦å˜åŒ–ç‡: {stats['length_change_ratio']:+.2%}")
    print(f"  - ç‹¬ç‰¹ç­–ç•¥æ•°: {stats['unique_strategies']}")
    print(f"  - å¤šæ ·æ€§å¾—åˆ†: {stats['diversity_score']:.2f}")


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())