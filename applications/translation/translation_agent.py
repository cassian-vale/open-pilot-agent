import json
import sys
import asyncio
from pathlib import Path
from typing import AsyncGenerator, TypedDict, Annotated, List, Union, Optional, Dict, Any

from pydantic import BaseModel, Field
from langgraph.graph import StateGraph, END
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.runnables import RunnableConfig
from langchain_core.messages.utils import convert_to_openai_messages

dir_name = Path(__file__).resolve().parent.parent.parent
sys.path.append(str(dir_name))

from applications.translation.translation_prompt import (
    TRANSLATION_SYSTEM_MESSAGE,
    TRANSLATION_PROMPT,
    STYLE_GUIDELINES,
    DIRECTION_DESCRIPTIONS
)
from base_agent import BaseAgent
from llm_api.llm_client_chat_model import LLMClientChatModel
from utils.time_count import timer
from utils.stream_chunk import StreamChunk  # å¼•å…¥æ ‡å‡† StreamChunk


# ===== è¾“å‡ºç»“æ„å®šä¹‰ (å¯¹åº” final_output["output"]) =====
class TranslationOutputContent(BaseModel):
    success: bool = Field(description="ç¿»è¯‘æ˜¯å¦æˆåŠŸ")
    original_text: str = Field(description="åŸæ–‡")
    translated_text: str = Field(description="è¯‘æ–‡")
    translation_direction: str = Field(description="ç¿»è¯‘æ–¹å‘")
    translation_style: str = Field(description="ç¿»è¯‘é£æ ¼")
    quality_score: float = Field(description="ç¿»è¯‘è´¨é‡è¯„åˆ†", ge=0, le=10)
    character_count: Dict[str, int] = Field(description="å­—ç¬¦ç»Ÿè®¡")
    validation_errors: List[str] = Field(default_factory=list, description="éªŒè¯é”™è¯¯ä¿¡æ¯")


# ===== çŠ¶æ€å®šä¹‰ =====
class TranslationState(TypedDict):
    messages: Annotated[List[Union[HumanMessage, AIMessage]], "æ¶ˆæ¯å†å²"]
    original_text: str
    translation_direction: str
    translation_style: str
    translated_text: str
    quality_score: float
    character_count: Dict[str, int]
    validation_errors: List[str]
    # æ–°å¢æ ‡å‡†è¾“å‡ºå­—æ®µ
    final_output: Optional[dict]
    reasoning_content: str
    metadata: Dict[str, Any]


# ===== ç¿»è¯‘Agentä¸»ç±» =====
class TranslationAgent(BaseAgent):
    def __init__(
            self,
            name: str = "translation-agent",
            # openai client init config
            base_url: str = "https://api.deepseek.com/v1",
            api_key: Optional[str] = None,
            timeout: float = 60.0,
            max_retries: int = 3,
            # openai client run config
            model: str = "deepseek-chat",
            max_tokens: Optional[int] = None,
            temperature: float = 0.3,
            top_p: float = 1.0,
            stream: bool = False,
            enable_thinking: bool = False,
    ):
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        super().__init__(
            name=name,
            model=model,
            base_url=base_url,
            api_key=api_key,
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
            timeout=timeout,
            max_retries=max_retries,
            stream=stream,
            enable_thinking=enable_thinking,
        )

        # éªŒè¯æ”¯æŒçš„ç¿»è¯‘é£æ ¼
        self.supported_styles = list(STYLE_GUIDELINES.keys())
        self.supported_directions = list(DIRECTION_DESCRIPTIONS.keys())

        # æ„å»ºå·¥ä½œæµå›¾
        self.graph = self._build_graph()

    def _build_translation_prompt(self, text: str, direction: str, style: str) -> str:
        """æ„å»ºç¿»è¯‘æç¤ºè¯"""
        style_guidelines = STYLE_GUIDELINES.get(style, STYLE_GUIDELINES["æ™®é€š"])
        direction_desc = DIRECTION_DESCRIPTIONS.get(direction, direction)
        
        return TRANSLATION_PROMPT.format(
            text=text,
            direction=direction_desc,
            style=style,
            style_guidelines=style_guidelines
        )

    def _validate_inputs(self, text: str, direction: str, style: str) -> List[str]:
        """éªŒè¯è¾“å…¥å‚æ•°"""
        errors = []
        
        if not text.strip():
            errors.append("å¾…ç¿»è¯‘æ–‡æœ¬ä¸èƒ½ä¸ºç©º")
        
        if direction not in self.supported_directions:
            errors.append(f"ä¸æ”¯æŒçš„ç¿»è¯‘æ–¹å‘: {direction}, æ”¯æŒçš„ç¿»è¯‘æ–¹å‘: {', '.join(self.supported_directions)}")
        
        if style not in self.supported_styles:
            errors.append(f"ä¸æ”¯æŒçš„ç¿»è¯‘é£æ ¼: {style}, æ”¯æŒçš„é£æ ¼: {', '.join(self.supported_styles)}")
        
        return errors

    def _calculate_quality_score(self, original_text: str, translated_text: str, direction: str) -> float:
        """è®¡ç®—ç¿»è¯‘è´¨é‡è¯„åˆ†ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        score = 8.0  # åŸºç¡€åˆ†
        
        if not translated_text.strip():
            return 0.0
        
        original_len = len(original_text)
        translated_len = len(translated_text)
        
        if direction == "ä¸­è¯‘è‹±":
            if translated_len < original_len * 0.3:
                score -= 2.0
            elif translated_len > original_len * 3:
                score -= 1.0
        else:  # è‹±è¯‘ä¸­
            if translated_len > original_len * 2:
                score -= 2.0
            elif translated_len < original_len * 0.3:
                score -= 1.0
        
        problematic_phrases = ["ç¿»è¯‘", "interpret", "sorry", "æ— æ³•ç¿»è¯‘"]
        if any(phrase in translated_text.lower() for phrase in problematic_phrases):
            score -= 1.0
        
        return max(0.0, min(10.0, score))

    def _get_character_count(self, original_text: str, translated_text: str) -> Dict[str, int]:
        """è·å–å­—ç¬¦ç»Ÿè®¡"""
        return {
            "original_chars": len(original_text),
            "translated_chars": len(translated_text),
            "original_words": len(original_text.split()),
            "translated_words": len(translated_text.split())
        }

    def _build_graph(self) -> StateGraph:
        """æ„å»º LangGraph å·¥ä½œæµ"""
        graph = StateGraph(TranslationState)

        async def initialize_node(state: TranslationState, config: RunnableConfig) -> TranslationState:
            """åˆå§‹åŒ–èŠ‚ç‚¹ï¼šéªŒè¯è¾“å…¥å‚æ•°"""
            run_config = config.get("configurable", {})
            request_id = run_config.get("request_id")

            with timer(self.logger, f"request_id: {request_id}, åˆå§‹åŒ–ç¿»è¯‘ä»»åŠ¡"):
                self.logger.info(f"request_id: {request_id}, å¼€å§‹ç¿»è¯‘ä»»åŠ¡, æ–‡æœ¬é•¿åº¦: {len(state['original_text'])}, æ–¹å‘: {state['translation_direction']}, é£æ ¼: {state['translation_style']}")
                
                validation_errors = self._validate_inputs(
                    state["original_text"],
                    state["translation_direction"],
                    state["translation_style"]
                )
                
                if validation_errors:
                    self.logger.warning(f"request_id: {request_id}, è¾“å…¥éªŒè¯å¤±è´¥: {validation_errors}")
                    return {
                        **state,
                        "validation_errors": validation_errors,
                        "success": False
                    }
                
                return state

        async def translate_node(state: TranslationState, config: RunnableConfig) -> TranslationState:
            """ç¿»è¯‘èŠ‚ç‚¹ï¼šæ‰§è¡Œç¿»è¯‘"""
            run_config = config.get("configurable", {})
            request_id = run_config.get("request_id")

            with timer(self.logger, f"request_id: {request_id}, æ‰§è¡Œç¿»è¯‘"):
                if state.get("validation_errors"):
                    return state

                prompt_text = self._build_translation_prompt(
                    state["original_text"],
                    state["translation_direction"],
                    state["translation_style"]
                )

                self.logger.info(f"request_id: {request_id}, è°ƒç”¨LLMè¿›è¡Œç¿»è¯‘")

                messages = [
                    SystemMessage(content=TRANSLATION_SYSTEM_MESSAGE), 
                    HumanMessage(content=prompt_text)
                ]

                llm_client = self.get_llm_client(run_config)
                chat_model = LLMClientChatModel(llm_client=llm_client)

                try:

                    response = await chat_model.ainvoke(messages, config=config)
                    
                    chat_completion = response.chat_completion.to_dict()
                    choices = chat_completion.get("choices", [])
                    
                    # åˆå§‹åŒ– metadata
                    output_metadata = {
                        "usage": chat_completion.get("usage", {}),
                        # "messages": []
                    }

                    if len(choices) > 0:
                        content = choices[0].get("message", {}).get("content", "")
                        reasoning_content = choices[0].get("message", {}).get("reasoning_content", "")

                        translated_text = content.strip()
                        
                        new_messages = state["messages"] + messages + [AIMessage(content=content)]
                        # output_metadata["messages"] = convert_to_openai_messages(new_messages)
                        
                        self.logger.debug(f"request_id: {request_id}, ç¿»è¯‘å®Œæˆ, è¯‘æ–‡é•¿åº¦: {len(translated_text)}")
                        
                        return {
                            **state,
                            "messages": new_messages,
                            "translated_text": translated_text,
                            "reasoning_content": reasoning_content,
                            "metadata": output_metadata
                        }
                    else:
                        raise ValueError("LLM apiè¾“å‡ºé”™è¯¯, choicesä¸ºç©º")
                except asyncio.CancelledError:
                    self.logger.warning(f"â›” request_id: {request_id}, ä»»åŠ¡è¢«ä¸­æ–­ï¼Œå·²åœæ­¢ LLM è¯·æ±‚")
                    raise

        async def evaluate_node(state: TranslationState, config: RunnableConfig) -> TranslationState:
            """è¯„ä¼°èŠ‚ç‚¹ï¼šè¯„ä¼°ç¿»è¯‘è´¨é‡"""
            run_config = config.get("configurable", {})
            request_id = run_config.get("request_id")

            with timer(self.logger, f"request_id: {request_id}, è¯„ä¼°ç¿»è¯‘è´¨é‡"):
                if state.get("validation_errors") or not state.get("translated_text"):
                    return state

                quality_score = self._calculate_quality_score(
                    state["original_text"],
                    state["translated_text"],
                    state["translation_direction"]
                )
                
                character_count = self._get_character_count(
                    state["original_text"],
                    state["translated_text"]
                )
                
                self.logger.debug(f"request_id: {request_id}, è´¨é‡è¯„ä¼°å®Œæˆ, è¯„åˆ†: {quality_score:.2f}")
                
                return {
                    **state,
                    "quality_score": quality_score,
                    "character_count": character_count
                }

        def finalize_node(state: TranslationState, config: RunnableConfig) -> TranslationState:
            """æœ€ç»ˆå¤„ç†èŠ‚ç‚¹ï¼šæ±‡æ€»ç»“æœ"""
            run_config = config.get("configurable", {})
            request_id = run_config.get("request_id")

            with timer(self.logger, f"request_id: {request_id}, ç»“æœæ±‡æ€»"):
                success = not state.get("validation_errors") and bool(state.get("translated_text"))
                
                # æ„å»º output å­—å…¸ï¼ˆä¸šåŠ¡æ•°æ®ï¼‰
                output_data = {
                    # "success": success,
                    "original_text": state["original_text"],
                    "translated_text": state.get("translated_text", ""),
                    "translation_direction": state["translation_direction"],
                    "translation_style": state["translation_style"],
                    # "quality_score": state.get("quality_score", 0.0),
                    # "character_count": state.get("character_count", {}),
                    # "validation_errors": state.get("validation_errors", [])
                }
                
                # è®¡ç®—ç½®ä¿¡åº¦ (ä½¿ç”¨è´¨é‡è¯„åˆ†å½’ä¸€åŒ–)
                confidence = state.get("quality_score", 0.0) / 10.0 if success else 0.0

                # æ„å»ºåŒ…å«å››ä¸ªå›ºå®šå…ƒç´ çš„ final_output
                final_output_structure = {
                    "output": output_data,
                    "content": state.get("translated_text", ""),
                    "reasoning_content": state.get("reasoning_content", ""),
                    "metadata": state.get("metadata", {}),
                    "confidence": confidence
                }

                status_msg = "æˆåŠŸ" if success else "å¤±è´¥"
                quality_msg = f", è´¨é‡è¯„åˆ†: {state.get('quality_score', 0):.2f}" if success else ""
                self.logger.success(f"request_id: {request_id}, ç¿»è¯‘å®Œæˆ, çŠ¶æ€: {status_msg}{quality_msg}")

                return {
                    **state,
                    "final_output": final_output_structure
                }

        # æ·»åŠ èŠ‚ç‚¹
        graph.add_node("initialize", initialize_node)
        graph.add_node("translate", translate_node)
        graph.add_node("evaluate", evaluate_node)
        graph.add_node("finalize", finalize_node)

        # è®¾ç½®å·¥ä½œæµ
        graph.set_entry_point("initialize")
        graph.add_edge("initialize", "translate")
        graph.add_edge("translate", "evaluate")
        graph.add_edge("evaluate", "finalize")
        graph.add_edge("finalize", END)

        return graph.compile()

    async def run(self, text: str, translation_direction: str, translation_style: str = "æ™®é€š", **kwargs) -> Dict[str, Any]:
        """
        æ‰§è¡Œç¿»è¯‘æµç¨‹

        :param text: å¾…ç¿»è¯‘çš„æ–‡æœ¬
        :param translation_direction: ç¿»è¯‘æ–¹å‘
        :param translation_style: ç¿»è¯‘é£æ ¼
        :return: ç»“æ„åŒ–è¾“å‡ºå­—å…¸ {output, reasoning_content, metadata, confidence}
        """
        request_id = kwargs.get("request_id")
        self.logger.info(f"ğŸ”§ request_id: {request_id}, å¼€å§‹å¤„ç†ç¿»è¯‘è¯·æ±‚, text_length: {len(text)}")

        run_config = {k: v for k, v in kwargs.items() if k in self.init_config or k == "request_id"}

        inputs = {
            "messages": [],
            "original_text": text,
            "translation_direction": translation_direction,
            "translation_style": translation_style,
            "translated_text": "",
            "quality_score": 0.0,
            "character_count": {},
            "validation_errors": [],
            "final_output": {},
            "reasoning_content": "",
            "metadata": {}
        }

        with timer(self.logger, f"request_id: {request_id}, å®Œæ•´ç¿»è¯‘æµç¨‹"):
            config = {"configurable": run_config} if run_config else {}
            final_state = await self.graph.ainvoke(inputs, config=config)
            output = final_state.get("final_output", {})
            self.logger.success(f"ğŸ‰ request_id: {request_id}, ç¿»è¯‘å®Œæˆ")

        return output
    
    async def run_stream(self, text: str, translation_direction: str, translation_style: str = "æ™®é€š", **kwargs) -> AsyncGenerator[StreamChunk, None]:
        """
        æµå¼æ‰§è¡Œç¿»è¯‘æµç¨‹

        :return: StreamChunk æµå¼è¾“å‡ºç”Ÿæˆå™¨
        """
        request_id = kwargs.get("request_id")
        self.logger.info(f"ğŸ”§ request_id: {request_id}, å¼€å§‹æµå¼å¤„ç†ç¿»è¯‘è¯·æ±‚")

        run_config = {k: v for k, v in kwargs.items() if k in self.init_config or k == "request_id"}

        inputs = {
            "messages": [],
            "original_text": text,
            "translation_direction": translation_direction,
            "translation_style": translation_style,
            "translated_text": "",
            "quality_score": 0.0,
            "character_count": {},
            "validation_errors": [],
            "final_output": {},
            "reasoning_content": "",
            "metadata": {}
        }

        with timer(self.logger, f"request_id: {request_id}, å®Œæ•´æµå¼ç¿»è¯‘æµç¨‹"):
            config = {"configurable": run_config} if run_config else {}
            
            async for event in self.graph.astream_events(inputs, config=config):
                event_type = event.get("event", "")
                
                # å¤„ç†LLMæµå¼è¾“å‡º
                if event_type == "on_chat_model_stream":
                    chunk = event.get("data", {}).get("chunk", None)
                    if chunk and hasattr(chunk, "chat_completion_chunk") and chunk.chat_completion_chunk:
                        chat_completion_chunk = chunk.chat_completion_chunk.to_dict()
                        choices = chat_completion_chunk.get("choices", [])
                        if choices:
                            delta = choices[0].get("delta", {})
                            content = delta.get("content", "")
                            reasoning_content = delta.get("reasoning_content", "")
                            
                            if reasoning_content:
                                yield StreamChunk(
                                    type="thinking",
                                    content=reasoning_content
                                )
                            elif content:
                                yield StreamChunk(
                                    type="content",
                                    content=content
                                )
                
                # å¤„ç†èŠ‚ç‚¹å¼€å§‹äº‹ä»¶
                elif event_type == "on_chain_start":
                    name = event.get("name", "")
                    if name == "initialize":
                        yield StreamChunk(
                            type="processing",
                            content="åˆå§‹åŒ–ï¼šéªŒè¯è¾“å…¥å‚æ•°..."
                        )
                    elif name == "translate":
                        yield StreamChunk(
                            type="processing",
                            content=f"æ­£åœ¨æ‰§è¡Œ{translation_direction}ç¿»è¯‘ï¼ˆ{translation_style}é£æ ¼ï¼‰..."
                        )
                    elif name == "evaluate":
                        yield StreamChunk(
                            type="processing", 
                            content="æ­£åœ¨è¯„ä¼°ç¿»è¯‘è´¨é‡..."
                        )
                    elif name == "finalize":
                        yield StreamChunk(
                            type="processing",
                            content="æ±‡æ€»ï¼šç”Ÿæˆæœ€ç»ˆç»“æœ..."
                        )
                
                # å¤„ç†å›¾ç»“æŸäº‹ä»¶
                elif event_type == "on_chain_end":
                    name = event.get("name", "")
                    if name == "LangGraph":
                        output = event.get("data", {}).get("output", {})
                        final_output_data = output.get("final_output", {})

                        if final_output_data:
                            yield StreamChunk(
                                type="final",
                                content="",
                                metadata=final_output_data
                            )

            self.logger.success(f"ğŸ‰ request_id: {request_id}, æµå¼ç¿»è¯‘å®Œæˆ")


# ===== ä½¿ç”¨ç¤ºä¾‹ =====
async def main():
    """ä½¿ç”¨ç¤ºä¾‹"""
    
    # åˆå§‹åŒ–Agent
    agent = TranslationAgent(
        name="test-translation-agent",
        base_url="https://api.deepseek.com/v1",
        api_key="YOUR_API_KEY",  # æ›¿æ¢ä¸ºå®é™…APIå¯†é’¥
        temperature=0.3
    )
    
    test_text = "ä»Šå¤©å¤©æ°”çœŸå¥½, æˆ‘ä»¬ä¸€èµ·å»å…¬å›­æ•£æ­¥å§ï¼"
    direction = "ä¸­è¯‘è‹±"
    style = "æ™®é€š"
    
    print(f"\n=== æµ‹è¯•ç¿»è¯‘: {test_text} ===")
    
    # 1. éæµå¼
    print("\n--- éæµå¼ ---")
    result = await agent.run(
        text=test_text,
        translation_direction=direction,
        translation_style=style,
        request_id="test-trans-001"
    )
    print(f"âœ… ç»“æœ: {result['output']['translated_text']}")
    print(f"ğŸ“Š è¯„åˆ†: {result['output']['quality_score']}")
    
    # 2. æµå¼
    print("\n--- æµå¼ ---")
    async for chunk in agent.run_stream(
        text=test_text,
        translation_direction=direction,
        translation_style=style,
        request_id="test-trans-002"
    ):
        if chunk.type == "thinking":
            print(f"ğŸ¤” {chunk.content}")
        elif chunk.type == "translation":
            print(chunk.content, end="", flush=True)
        elif chunk.type == "final":
            final_data = chunk.metadata.get("output", {})
            print(f"\nâœ… æµå¼å®Œæˆ: {final_data.get('translated_text')}")

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())