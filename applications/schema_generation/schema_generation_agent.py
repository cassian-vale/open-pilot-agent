import asyncio
import json
import re
import sys
from pathlib import Path
from typing import AsyncGenerator, TypedDict, Annotated, List, Union, Optional, Dict, Any

from pydantic import BaseModel, Field
from langgraph.graph import StateGraph, END
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.runnables import RunnableConfig
from langchain_core.messages.utils import convert_to_openai_messages

from utils.schema_parse import SchemaParser # å¼•å…¥æ­¤è¡Œ

dir_name = Path(__file__).resolve().parent.parent.parent
sys.path.append(str(dir_name))

from base_agent import BaseAgent
from applications.schema_generation.schema_generation_prompt import (
    SCHEMA_GENERATION_SYSTEM_MESSAGE,
    SCHEMA_GENERATION_PROMPT
)
from llm_api.llm_client_chat_model import LLMClientChatModel
from utils.schema_validate import SchemaValidator
from utils.time_count import timer
from utils.stream_chunk import StreamChunk


# ===== è¾“å‡ºç»“æ„å®šä¹‰ =====
class SchemaGenerationOutput(BaseModel):
    success: bool = Field(description="Schemaç”Ÿæˆæ˜¯å¦æˆåŠŸ")
    generated_schema: Dict[str, Any] = Field(description="ç”Ÿæˆçš„Schemaå®šä¹‰")
    validation_errors: List[str] = Field(default_factory=list, description="SchemaéªŒè¯é”™è¯¯ä¿¡æ¯")
    confidence: float = Field(description="æ•´ä½“ç½®ä¿¡åº¦", ge=0, le=1)
    schema_description: str = Field(description="Schemaçš„è¯¦ç»†è¯´æ˜")


# ===== çŠ¶æ€å®šä¹‰ =====
class SchemaGenerationState(TypedDict):
    messages: Annotated[List[Union[HumanMessage, AIMessage]], "æ¶ˆæ¯å†å²"]
    user_requirements: str
    domain_context: Optional[str]
    generated_schema: Dict[str, Any]
    validation_errors: List[str]
    schema_description: str
    final_output: Optional[dict] # ä¿®æ”¹ä¸ºOptional[dict]


# ===== Schemaç”ŸæˆAgentä¸»ç±» =====
class SchemaGenerationAgent(BaseAgent):
    def __init__(
            self,
            name: str = "schema-generation-agent",
            # openai client init config
            base_url: str = "https://api.deepseek.com/v1",
            api_key: Optional[str] = None,
            timeout: float = 60.0,
            max_retries: int = 3,
            # openai client run config
            model: str = "deepseek-chat",
            max_tokens: Optional[int] = None,
            temperature: float = 0.1,
            top_p: float = 1.0,
            stream: bool = False,
            enable_thinking: bool = False,
    ):
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        super().__init__(
            name=name,
            model=model,
            base_url=base_url,
            api_key=api_key,
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
            timeout=timeout,
            max_retries=max_retries,
            stream=stream,
            enable_thinking=enable_thinking,
        )

        # åˆå§‹åŒ–ç»„ä»¶
        self.validator = SchemaValidator()

        self.schema_parser = SchemaParser()

        # æ„å»ºå·¥ä½œæµå›¾
        self.graph = self._build_graph()

    def _build_graph(self) -> StateGraph:
        """æ„å»º LangGraph å·¥ä½œæµ"""
        graph = StateGraph(SchemaGenerationState)

        def initialize_node(state: SchemaGenerationState, config: RunnableConfig) -> SchemaGenerationState:
            """åˆå§‹åŒ–èŠ‚ç‚¹ï¼šå‡†å¤‡Schemaç”Ÿæˆä»»åŠ¡"""
            run_config = config.get("configurable", {})
            request_id = run_config.get("request_id")

            with timer(self.logger, f"request_id: {request_id}, åˆå§‹åŒ–Schemaç”Ÿæˆä»»åŠ¡"):
                self.logger.info(f"request_id: {request_id}, å¼€å§‹Schemaç”Ÿæˆ, ç”¨æˆ·éœ€æ±‚é•¿åº¦: {len(state['user_requirements'])}")
                
                # è®¾ç½®é»˜è®¤é¢†åŸŸä¸Šä¸‹æ–‡
                if not state.get("domain_context"):
                    state["domain_context"] = "é€šç”¨æ•°æ®æ¨¡å‹"
                
                return state

        async def generate_node(state: SchemaGenerationState, config: RunnableConfig) -> SchemaGenerationState:
            """ç”ŸæˆèŠ‚ç‚¹ï¼šæ‰§è¡ŒSchemaç”Ÿæˆ"""
            run_config = config.get("configurable", {})
            request_id = run_config.get("request_id")

            with timer(self.logger, f"request_id: {request_id}, æ‰§è¡ŒSchemaç”Ÿæˆ"):
                # æ„å»ºæç¤ºè¯
                prompt_text = self._build_generation_prompt(
                    state["user_requirements"], 
                    state["domain_context"]
                )

                self.logger.info(f"request_id: {request_id}, è°ƒç”¨LLMè¿›è¡ŒSchemaç”Ÿæˆ, éœ€æ±‚é•¿åº¦: {len(state['user_requirements'])}")

                system_message_content = SCHEMA_GENERATION_SYSTEM_MESSAGE
                messages = [SystemMessage(content=system_message_content), HumanMessage(content=prompt_text)]

                # è°ƒç”¨LLM
                llm_client = self.get_llm_client(run_config)
                chat_model = LLMClientChatModel(llm_client=llm_client)
                try:
                    response = await chat_model.ainvoke(messages, config=config)

                    chat_completion = response.chat_completion.to_dict()
                    choices = chat_completion.get("choices", [])
                    
                    # åˆå§‹åŒ–æœ€ç»ˆè¾“å‡ºç»“æ„ï¼ŒåŒ…å«metadata
                    output_metadata = {
                        "usage": chat_completion.get("usage", {}),
                        # "messages": []
                    }
                    
                    if len(choices) > 0:
                        content = choices[0].get("message", {}).get("content", "")
                        reasoning_content = choices[0].get("message", {}).get("reasoning_content", "")

                        self.logger.debug(f"request_id: {request_id}, LLMå“åº”é•¿åº¦: {len(content)}")

                        # æå–JSONç»“æœ
                        generated_schema = self.schema_parser.parse_response_to_json(content)
                        
                        # ç”ŸæˆSchemaæè¿°
                        schema_description = self.validator.generate_schema_description(generated_schema)
                        
                        # æ›´æ–°æ¶ˆæ¯å†å²
                        new_messages = state["messages"] + messages + [AIMessage(content=content)]
                        # output_metadata["messages"] = convert_to_openai_messages(new_messages)
                        
                        # æ„é€  output å­—å…¸
                        output_data = {
                            "generated_schema": generated_schema,
                            "schema_description": schema_description
                        }

                        # æ„å»ºåŒ…å«å››ä¸ªå›ºå®šå…ƒç´ çš„ final_output
                        final_output_structure = {
                            "output": output_data,
                            "content": content,
                            "reasoning_content": reasoning_content,
                            "metadata": output_metadata,
                            "confidence": 0.0 # æš‚æ—¶è®¾ä¸º0ï¼Œåœ¨finalize_nodeä¸­æ›´æ–°
                        }
                        
                        return {
                            **state,
                            "messages": new_messages,
                            "generated_schema": generated_schema,
                            "schema_description": schema_description,
                            "final_output": final_output_structure # æ›´æ–°final_output
                        }
                    else:
                        raise ValueError("LLM apiè¾“å‡ºé”™è¯¯, choicesä¸ºç©º")
                except asyncio.CancelledError:
                    self.logger.warning(f"â›” request_id: {request_id}, ä»»åŠ¡è¢«ä¸­æ–­ï¼Œå·²åœæ­¢ LLM è¯·æ±‚")
                    raise

        def validate_node(state: SchemaGenerationState, config: RunnableConfig) -> SchemaGenerationState:
            """éªŒè¯èŠ‚ç‚¹ï¼šéªŒè¯ç”Ÿæˆçš„Schema"""
            run_config = config.get("configurable", {})
            request_id = run_config.get("request_id")

            with timer(self.logger, f"request_id: {request_id}, éªŒè¯ç”Ÿæˆçš„Schema"):
                # éªŒè¯Schemaæ ¼å¼
                validation_errors = self.validator.validate_schema(state["generated_schema"], strict=False)

                if not validation_errors:
                    self.logger.info(f"request_id: {request_id}, SchemaéªŒè¯é€šè¿‡")
                else:
                    error_count = len(validation_errors)
                    self.logger.warning(f"request_id: {request_id}, å‘ç° {error_count} ä¸ªSchemaéªŒè¯é”™è¯¯")
                    for error in validation_errors[:3]:  # åªè®°å½•å‰3ä¸ªé”™è¯¯
                        self.logger.debug(f"SchemaéªŒè¯é”™è¯¯: {error}")

                state["validation_errors"] = validation_errors
                return state

        def finalize_node(state: SchemaGenerationState, config: RunnableConfig) -> SchemaGenerationState:
            """æœ€ç»ˆå¤„ç†èŠ‚ç‚¹ï¼šæ±‡æ€»ç»“æœ"""
            run_config = config.get("configurable", {})
            request_id = run_config.get("request_id")

            with timer(self.logger, f"request_id: {request_id}, ç»“æœæ±‡æ€»"):
                success = len(state["validation_errors"]) == 0
                
                # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆåŸºäºéªŒè¯é”™è¯¯æ•°é‡ï¼‰
                confidence = max(0.0, 1.0 - len(state["validation_errors"]) * 0.1)
                
                # æ›´æ–° final_output å­—å…¸
                final_output = state.get("final_output", {})
                
                # æ›´æ–° output éƒ¨åˆ†
                # if "output" in final_output:
                #     final_output["output"].update({
                #         "success": success,
                #         "validation_errors": state["validation_errors"],
                #         "requirements_length": len(state["user_requirements"]),
                #         "schema_field_count": len(state["generated_schema"])
                #     })
                
                # æ›´æ–° confidence
                final_output["confidence"] = confidence

                status_msg = "æˆåŠŸ" if success else f"æœ‰{len(state['validation_errors'])}ä¸ªè­¦å‘Š"
                self.logger.success(f"request_id: {request_id}, Schemaç”Ÿæˆå®Œæˆ, çŠ¶æ€: {status_msg}, ç½®ä¿¡åº¦: {confidence:.2f}")

                return {
                    **state,
                    "final_output": final_output
                }

        # æ·»åŠ èŠ‚ç‚¹
        graph.add_node("initialize", initialize_node)
        graph.add_node("generate", generate_node)
        graph.add_node("validate", validate_node)
        graph.add_node("finalize", finalize_node)

        # è®¾ç½®å·¥ä½œæµ
        graph.set_entry_point("initialize")
        graph.add_edge("initialize", "generate")
        graph.add_edge("generate", "validate")
        graph.add_edge("validate", "finalize")
        graph.add_edge("finalize", END)

        return graph.compile()

    def _build_generation_prompt(self, requirements: str, domain_context: str) -> str:
        """æ„å»ºSchemaç”Ÿæˆæç¤ºè¯"""
        return SCHEMA_GENERATION_PROMPT.format(
            user_requirements=requirements,
            domain_context=domain_context
        )

    async def run(self, user_requirements: str, domain_context: Optional[str] = None, **kwargs) -> Dict[str, Any]:
        """
        æ‰§è¡ŒSchemaç”Ÿæˆæµç¨‹

        :param user_requirements: ç”¨æˆ·éœ€æ±‚æè¿°
        :param domain_context: é¢†åŸŸä¸Šä¸‹æ–‡ä¿¡æ¯
        :return: ç»“æ„åŒ–è¾“å‡ºå­—å…¸
        """
        if not user_requirements.strip():
            raise ValueError("ç”¨æˆ·éœ€æ±‚ä¸èƒ½ä¸ºç©º")
        
        request_id = kwargs.get("request_id")

        self.logger.info(f"ğŸ”§ request_id: {request_id}, å¼€å§‹å¤„ç†Schemaç”Ÿæˆè¯·æ±‚, requirements_length: {len(user_requirements)}")

        # æ„å»ºè¿è¡Œæ—¶é…ç½®
        run_config = {k: v for k, v in kwargs.items() if k in self.init_config or k == "request_id"}

        inputs = {
            "messages": [],
            "user_requirements": user_requirements,
            "domain_context": domain_context,
            "generated_schema": {},
            "validation_errors": [],
            "schema_description": "",
            "final_output": None
        }

        with timer(self.logger, f"request_id: {request_id}, å®Œæ•´Schemaç”Ÿæˆæµç¨‹"):
            # ä¼ é€’è¿è¡Œæ—¶é…ç½®
            config = {"configurable": run_config} if run_config else {}
            final_state = await self.graph.ainvoke(inputs, config=config)
            output = final_state.get("final_output", {})
            self.logger.success(f"ğŸ‰ request_id: {request_id}, Schemaç”Ÿæˆå®Œæˆ")

        return output
    
    async def run_stream(self, user_requirements: str, domain_context: Optional[str] = None, **kwargs) -> AsyncGenerator[StreamChunk, None]:
        """
        æµå¼æ‰§è¡ŒSchemaç”Ÿæˆæµç¨‹

        :param user_requirements: ç”¨æˆ·éœ€æ±‚æè¿°
        :param domain_context: é¢†åŸŸä¸Šä¸‹æ–‡ä¿¡æ¯
        :return: æµå¼è¾“å‡ºç”Ÿæˆå™¨
        """
        if not user_requirements.strip():
            raise ValueError("ç”¨æˆ·éœ€æ±‚ä¸èƒ½ä¸ºç©º")
        
        request_id = kwargs.get("request_id")

        self.logger.info(f"ğŸ”§ request_id: {request_id}, å¼€å§‹æµå¼å¤„ç†Schemaç”Ÿæˆè¯·æ±‚, requirements_length: {len(user_requirements)}")

        # æ„å»ºè¿è¡Œæ—¶é…ç½®
        run_config = {k: v for k, v in kwargs.items() if k in self.init_config or k == "request_id"}

        inputs = {
            "messages": [],
            "user_requirements": user_requirements,
            "domain_context": domain_context,
            "generated_schema": {},
            "validation_errors": [],
            "schema_description": "",
            "final_output": None
        }

        with timer(self.logger, f"request_id: {request_id}, å®Œæ•´æµå¼Schemaç”Ÿæˆæµç¨‹"):
            # ä¼ é€’è¿è¡Œæ—¶é…ç½®
            config = {"configurable": run_config} if run_config else {}
            
            async for event in self.graph.astream_events(inputs, config=config):
                event_type = event.get("event", "")
                
                 # å¤„ç†LLMæµå¼è¾“å‡º
                if event_type == "on_chat_model_stream":
                    chunk = event.get("data", {}).get("chunk", None)
                    if chunk and hasattr(chunk, "chat_completion_chunk") and chunk.chat_completion_chunk:
                        chat_completion_chunk = chunk.chat_completion_chunk.to_dict()
                        choices = chat_completion_chunk.get("choices", [])
                        if choices:
                            delta = choices[0].get("delta", {})
                            content = delta.get("content", "")
                            reasoning_content = delta.get("reasoning_content", "")
                            
                            # è¾“å‡ºæ€è€ƒå†…å®¹
                            if reasoning_content:
                                yield StreamChunk(
                                    type="thinking",
                                    content=reasoning_content
                                )
                            # è¾“å‡ºç”Ÿæˆå†…å®¹
                            elif content:
                                yield StreamChunk(
                                    type="content", 
                                    content=content
                                )
                
                # å¤„ç†èŠ‚ç‚¹å¼€å§‹äº‹ä»¶
                elif event_type == "on_chain_start":
                    name = event.get("name", "")
                    if name == "initialize":
                        yield StreamChunk(
                            type="processing",
                            content="å¼€å§‹åˆå§‹åŒ–Schemaç”Ÿæˆä»»åŠ¡..."
                        )
                    elif name == "generate":
                        yield StreamChunk(
                            type="processing",
                            content="æ­£åœ¨åˆ†æéœ€æ±‚å¹¶ç”ŸæˆSchema..."
                        )
                    elif name == "validate":
                        yield StreamChunk(
                            type="processing", 
                            content="æ­£åœ¨éªŒè¯ç”Ÿæˆçš„Schema..."
                        )
                    elif name == "finalize":
                        yield StreamChunk(
                            type="processing",
                            content="æ­£åœ¨æ±‡æ€»æœ€ç»ˆç»“æœ..."
                        )
                
                
                # å¤„ç†å›¾ç»“æŸäº‹ä»¶, è¾“å‡ºæœ€ç»ˆç»“æœ
                elif event_type == "on_chain_end" and event.get("name", "") == "LangGraph":
                    output = event.get("data", {}).get("output", {})
                    final_output_data = output.get("final_output", {}) # ç¡®ä¿è·å–çš„æ˜¯æ•´ä¸ªfinal_output 
                    
                    yield StreamChunk(
                        type="final",
                        content="",
                        metadata=final_output_data
                    )

            self.logger.success(f"ğŸ‰ request_id: {request_id}, æµå¼Schemaç”Ÿæˆå®Œæˆ")


# ===== ä½¿ç”¨ç¤ºä¾‹ =====
async def main():
    """ä½¿ç”¨ç¤ºä¾‹"""
    
    # æµ‹è¯•ç”¨æˆ·éœ€æ±‚
    TEST_REQUIREMENTS = """
    æˆ‘éœ€è¦ä¸€ä¸ªå•†å“è¯„è®ºçš„æ•°æ®Schema, åŒ…å«ä»¥ä¸‹ä¿¡æ¯ï¼š
    - è¯„è®ºIDï¼ˆå”¯ä¸€æ ‡è¯†ï¼‰
    - ç”¨æˆ·ä¿¡æ¯ï¼šç”¨æˆ·IDã€ç”¨æˆ·å
    - å•†å“ä¿¡æ¯ï¼šå•†å“IDã€å•†å“åç§°ã€å•†å“åˆ†ç±»
    - è¯„è®ºå†…å®¹ï¼šè¯„åˆ†ï¼ˆ1-5åˆ†ï¼‰ã€è¯„è®ºæ ‡é¢˜ã€è¯¦ç»†å†…å®¹ã€è¯„è®ºæ—¶é—´
    - æœ‰ç”¨æ€§ç»Ÿè®¡ï¼šç‚¹èµæ•°ã€ç‚¹è¸©æ•°
    - æ ‡ç­¾ï¼šç”¨æˆ·è‡ªå®šä¹‰çš„æ ‡ç­¾åˆ—è¡¨
    - å›¾ç‰‡ä¿¡æ¯ï¼šå›¾ç‰‡URLåˆ—è¡¨
    - æ˜¯å¦åŒ¿åè¯„è®º
    """
    
    # åˆå§‹åŒ–Agent
    agent = SchemaGenerationAgent(
        name="test-schema-agent",
        base_url="https://api.deepseek.com/v1",
        api_key="YOUR_API_KEY",  # æ›¿æ¢ä¸ºå®é™…APIå¯†é’¥
        temperature=0.1
    )
    
    # æµå¼å¤„ç†ç¤ºä¾‹
    print("=== æµå¼Schemaç”Ÿæˆ ===")
    async for chunk in agent.run_stream(
        TEST_REQUIREMENTS, 
        domain_context="ç”µå•†è¯„è®ºç³»ç»Ÿ",
        request_id="test-schema-001"
    ):
        if chunk.type == "thinking":
            print(f"ğŸ¤” {chunk.content}")
        elif chunk.type == "content":
            print(f"{chunk.content}", end="", flush=True)
        elif chunk.type == "processing": # æ·»åŠ å¤„ç†processingç±»å‹
            print(f"ğŸ”„ {chunk.content}")
        elif chunk.type == "final":
            result = json.loads(chunk.content)
            status = "æˆåŠŸ" if result["success"] else f"æœ‰{len(result['validation_errors'])}ä¸ªè­¦å‘Š"
            print(f"\nâœ… Schemaç”Ÿæˆå®Œæˆ: {status}, ç½®ä¿¡åº¦: {result['confidence']:.2f}")
            print(f"ğŸ“Š ç”Ÿæˆå­—æ®µæ•°: {result['schema_field_count']}")
            print(f"ğŸ“‹ ç”Ÿæˆçš„Schema:")
            print(json.dumps(result['generated_schema'], ensure_ascii=False, indent=2))
    
    print("\n" + "="*50 + "\n")
    
    # åŒæ­¥å¤„ç†ç¤ºä¾‹
    print("=== åŒæ­¥Schemaç”Ÿæˆ ===")
    result = await agent.run(
        TEST_REQUIREMENTS,
        domain_context="ç”µå•†è¯„è®ºç³»ç»Ÿ", 
        request_id="test-schema-002"
    )
    
    # æ‰“å°ç»“æœ
    print(f"ğŸ“Š Schemaç”Ÿæˆç»“æœ:")
    # ä»resultä¸­æå–éœ€è¦çš„å­—æ®µ
    output_data = result.get("output", {})
    success = output_data.get("success", False)
    confidence = result.get("confidence", 0.0) # confidenceç°åœ¨ç›´æ¥åœ¨final_outputçš„é¡¶å±‚
    validation_errors = output_data.get("validation_errors", [])
    generated_schema = output_data.get("generated_schema", {})
    schema_description = output_data.get("schema_description", "")
    requirements_length = output_data.get("requirements_length", 0)
    schema_field_count = output_data.get("schema_field_count", 0)


    print(f"  çŠ¶æ€: {'âœ… æˆåŠŸ' if success else 'âš ï¸ æœ‰è­¦å‘Š'}")
    print(f"  ç½®ä¿¡åº¦: {confidence:.2f}")
    print(f"  éœ€æ±‚é•¿åº¦: {requirements_length}")
    print(f"  ç”Ÿæˆå­—æ®µæ•°: {schema_field_count}")
    
    if validation_errors:
        print(f"  éªŒè¯è­¦å‘Š: {len(validation_errors)} ä¸ª")
        for error in validation_errors[:3]:
            print(f"    - {error}")
    
    print(f"\nğŸ“‹ ç”Ÿæˆçš„Schema:")
    print(json.dumps(generated_schema, ensure_ascii=False, indent=2))
    
    print(f"\nğŸ“ Schemaè¯´æ˜:")
    print(schema_description)


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())