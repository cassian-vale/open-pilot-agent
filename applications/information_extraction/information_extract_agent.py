import asyncio
import json
import re
import sys
from pathlib import Path
from typing import AsyncGenerator, TypedDict, Annotated, List, Union, Optional, Dict, Any

from pydantic import BaseModel, Field
from langgraph.graph import StateGraph, END
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.runnables import RunnableConfig
from langchain_core.messages.utils import convert_to_openai_messages

dir_name = Path(__file__).resolve().parent.parent.parent
sys.path.append(str(dir_name))

from base_agent import BaseAgent
from llm_api.llm_client_chat_model import LLMClientChatModel
from applications.information_extraction.information_extract_prompt import ie_prompt, ie_system_message
from utils.schema_validate import SchemaValidator
from utils.time_count import timer
from utils.stream_chunk import StreamChunk


# ===== è¾“å‡ºç»“æ„å®šä¹‰ =====
class ExtractionOutput(BaseModel):
    success: bool = Field(description="æŠ½å–æ˜¯å¦æˆåŠŸ")
    extraction_result: Dict[str, Any] = Field(description="æŠ½å–çš„ç»“æ„åŒ–ç»“æœ")
    validation_errors: List[str] = Field(default_factory=list, description="éªŒè¯é”™è¯¯ä¿¡æ¯")
    confidence: float = Field(description="æ•´ä½“ç½®ä¿¡åº¦", ge=0, le=1)


# ===== çŠ¶æ€å®šä¹‰ =====
class ExtractionState(TypedDict):
    messages: Annotated[List[Union[HumanMessage, AIMessage]], "æ¶ˆæ¯å†å²"]
    original_text: str
    extraction_schema: Dict[str, Any]
    extraction_result: Dict[str, Any]
    validation_errors: List[str]
    final_output: Optional[dict]


# ===== ä¿¡æ¯æŠ½å–Agentä¸»ç±» =====
class InformationExtractionAgent(BaseAgent):
    def __init__(
            self,
            name: str = "information-extraction-agent",
            # openai client init config
            base_url: str = "https://api.deepseek.com/v1",
            api_key: Optional[str] = None,
            timeout: float = 60.0,
            max_retries: int = 3,
            # openai client run config
            model: str = "deepseek-chat",
            max_tokens: Optional[int] = None,
            temperature: float = 0.1,
            top_p: float = 1.0,
            stream: bool = False,
            enable_thinking: bool = False,
    ):
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        super().__init__(
            name=name,
            model=model,
            base_url=base_url,
            api_key=api_key,
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
            timeout=timeout,
            max_retries=max_retries,
            stream=stream,
            enable_thinking=enable_thinking,
        )

        # åˆå§‹åŒ–ç»„ä»¶
        self.validator = SchemaValidator()

        # æ„å»ºå·¥ä½œæµå›¾
        self.graph = self._build_graph()

    def _build_graph(self) -> StateGraph:
        """æ„å»º LangGraph å·¥ä½œæµ"""
        graph = StateGraph(ExtractionState)

        def initialize_node(state: ExtractionState, config: RunnableConfig) -> ExtractionState:
            """åˆå§‹åŒ–èŠ‚ç‚¹ï¼šå‡†å¤‡æŠ½å–ä»»åŠ¡"""
            run_config = config.get("configurable", {})
            request_id = run_config.get("request_id")

            with timer(self.logger, f"request_id: {request_id}, åˆå§‹åŒ–ä¿¡æ¯æŠ½å–ä»»åŠ¡"):
                schema_fields = list(state["extraction_schema"].keys())
                self.logger.info(f"request_id: {request_id}, å¼€å§‹ç»“æ„åŒ–ä¿¡æ¯æŠ½å–, SchemaåŒ…å« {len(schema_fields)} ä¸ªå­—æ®µ: {schema_fields}")
                
                return state

        async def extract_node(state: ExtractionState, config: RunnableConfig) -> ExtractionState:
            """æŠ½å–èŠ‚ç‚¹ï¼šæ‰§è¡Œä¿¡æ¯æŠ½å–"""
            run_config = config.get("configurable", {})
            request_id = run_config.get("request_id")

            with timer(self.logger, f"request_id: {request_id}, æ‰§è¡Œä¿¡æ¯æŠ½å–"):
                # æ„å»ºæç¤ºè¯
                prompt_text = self._build_extraction_prompt(
                    state["original_text"], 
                    state["extraction_schema"]
                )

                self.logger.info(f"request_id: {request_id}, è°ƒç”¨LLMè¿›è¡Œä¿¡æ¯æŠ½å–, æ–‡æœ¬é•¿åº¦: {len(state['original_text'])}")

                messages = [SystemMessage(content=ie_system_message), HumanMessage(content=prompt_text)]

                # è°ƒç”¨LLM
                llm_client = self.get_llm_client(run_config)
                chat_model = LLMClientChatModel(llm_client=llm_client)

                try:
                    response = await chat_model.ainvoke(messages, config=config)

                    chat_completion = response.chat_completion.to_dict()
                    choices = chat_completion.get("choices", [])
                    
                    # åˆå§‹åŒ–æœ€ç»ˆè¾“å‡ºç»“æ„
                    final_output = {
                        "metadata": {
                            "usage": chat_completion.get("usage", {}),
                            # "messages": []
                        }
                    }

                    if len(choices) > 0:
                        content = choices[0].get("message", {}).get("content", "")
                        reasoning_content = choices[0].get("message", {}).get("reasoning_content", "")
                        
                        # æ›´æ–°æ¶ˆæ¯å†å²
                        new_messages = state["messages"] + messages + [AIMessage(content=content)]
                        # final_output["metadata"]["messages"] = convert_to_openai_messages(new_messages)

                        final_output["content"] = content
                        final_output["reasoning_content"] = reasoning_content

                        self.logger.debug(f"request_id: {request_id}, LLMå“åº”é•¿åº¦: {len(content)}")

                        # æå–JSONç»“æœ
                        extraction_result = self._parse_extraction_response(content)

                        final_output["output"] = extraction_result   
                        
                        self.logger.info(f"request_id: {request_id}, LLM Parse Output: {json.dumps(extraction_result, ensure_ascii=False)}")
                        
                        return {
                            **state,
                            "messages": new_messages,
                            "extraction_result": extraction_result,
                            "final_output": final_output
                        }
                    else:
                        raise ValueError("LLM apiè¾“å‡ºé”™è¯¯, choicesä¸ºç©º")
                    
                except asyncio.CancelledError:
                    self.logger.warning(f"â›” request_id: {request_id}, ä»»åŠ¡è¢«ä¸­æ–­ï¼Œå·²åœæ­¢ LLM è¯·æ±‚")
                    raise

        def validate_node(state: ExtractionState, config: RunnableConfig) -> ExtractionState:
            """éªŒè¯èŠ‚ç‚¹ï¼šéªŒè¯æŠ½å–ç»“æœ"""
            run_config = config.get("configurable", {})
            request_id = run_config.get("request_id")

            with timer(self.logger, f"request_id: {request_id}, éªŒè¯æŠ½å–ç»“æœ"):
                validation_result = self.validator.validate_data(
                    state["extraction_result"], 
                    state["extraction_schema"]
                )

                if validation_result["valid"]:
                    self.logger.info(f"request_id: {request_id}, éªŒè¯é€šè¿‡, ä½¿ç”¨æ¸…ç†åçš„æ•°æ®")
                    # ä½¿ç”¨éªŒè¯åçš„æ•°æ®ï¼ˆç»è¿‡Pydanticæ¸…ç†å’Œè½¬æ¢ï¼‰
                    state["extraction_result"] = validation_result["data"]
                else:
                    error_count = len(validation_result["errors"])
                    self.logger.warning(f"request_id: {request_id}, å‘ç° {error_count} ä¸ªéªŒè¯é”™è¯¯")
                    for error in validation_result["errors"][:3]:  # åªè®°å½•å‰3ä¸ªé”™è¯¯
                        self.logger.debug(f"éªŒè¯é”™è¯¯: {error}")
                    state["validation_errors"].extend(validation_result["errors"])

                return state

        def finalize_node(state: ExtractionState, config: RunnableConfig) -> ExtractionState:
            """æœ€ç»ˆå¤„ç†èŠ‚ç‚¹ï¼šæ±‡æ€»ç»“æœ"""
            run_config = config.get("configurable", {})
            request_id = run_config.get("request_id")

            with timer(self.logger, f"request_id: {request_id}, ç»“æœæ±‡æ€»"):
                success = len(state["validation_errors"]) == 0
                
                # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆåŸºäºéªŒè¯é”™è¯¯æ•°é‡ï¼‰
                confidence = max(0.0, 1.0 - len(state["validation_errors"]) * 0.1)
                
                # æ„å»ºæœ€ç»ˆè¾“å‡ºï¼Œä¿ç•™metadataä¿¡æ¯
                final_output = state.get("final_output", {})
                final_output.update({
                    # "success": success,
                    # "extraction_result": state["extraction_result"],
                    # "validation_errors": state["validation_errors"],
                    "confidence": confidence,
                    # "original_text_length": len(state["original_text"]),
                    # "schema_fields": list(state["extraction_schema"].keys())
                })

                status_msg = "æˆåŠŸ" if success else f"æœ‰{len(state['validation_errors'])}ä¸ªé”™è¯¯"
                self.logger.success(f"request_id: {request_id}, ä¿¡æ¯æŠ½å–å®Œæˆ, çŠ¶æ€: {status_msg}, ç½®ä¿¡åº¦: {confidence:.2f}")

                return {
                    **state,
                    "final_output": final_output
                }

        # æ·»åŠ èŠ‚ç‚¹
        graph.add_node("initialize", initialize_node)
        graph.add_node("extract", extract_node)
        graph.add_node("validate", validate_node)
        graph.add_node("finalize", finalize_node)

        # è®¾ç½®å·¥ä½œæµ
        graph.set_entry_point("initialize")
        graph.add_edge("initialize", "extract")
        graph.add_edge("extract", "validate")
        graph.add_edge("validate", "finalize")
        graph.add_edge("finalize", END)

        return graph.compile()

    def _build_extraction_prompt(self, text: str, schema: Dict[str, Any]) -> str:
        """æ„å»ºä¿¡æ¯æŠ½å–æç¤ºè¯"""
        # ç”ŸæˆSchemaæ–‡æ¡£æè¿°
        schema_doc = self.validator.generate_schema_description(schema)
        
        # ç”Ÿæˆæ™ºèƒ½ç¤ºä¾‹
        example = self.validator.generate_example_data(schema)
        
        prompt = ie_prompt.format(
            text=text, 
            schema_doc=schema_doc, 
            example=json.dumps(example, ensure_ascii=False, indent=2)
        )
        
        return prompt

    def _parse_extraction_response(self, content: str) -> Dict[str, Any]:
        """è§£æLLMçš„æŠ½å–å“åº”"""
        try:
            # å°è¯•ç›´æ¥è§£æ
            extraction_result = json.loads(content)
            return extraction_result
        except json.JSONDecodeError:
            # å¦‚æœç›´æ¥è§£æå¤±è´¥, å°è¯•æå–JSONå¯¹è±¡
            json_match = re.search(r'\{.*\}', content, re.DOTALL)
            if json_match:
                try:
                    extraction_result = json.loads(json_match.group())
                    self.logger.warning("ä»å“åº”æ–‡æœ¬ä¸­æå–JSONæˆåŠŸ")
                    return extraction_result
                except json.JSONDecodeError as e:
                    self.logger.error(f"JSONæå–åè§£æå¤±è´¥: {e}")
            else:
                self.logger.error("æ— æ³•ä»å“åº”ä¸­æå–æœ‰æ•ˆçš„JSON")
            
            # è¿”å›ç©ºç»“æœ
            return {}

    async def run(self, text: str, schema: Dict[str, Any], **kwargs) -> Dict[str, Any]:
        """
        æ‰§è¡Œä¿¡æ¯æŠ½å–æµç¨‹

        :param text: è¾“å…¥æ–‡æœ¬
        :param schema: æŠ½å–schemaå®šä¹‰
        :return: ç»“æ„åŒ–è¾“å‡ºå­—å…¸
        """
        if not text.strip():
            raise ValueError("æ–‡æœ¬ä¸èƒ½ä¸ºç©º")
        
        if not schema:
            raise ValueError("Schemaä¸èƒ½ä¸ºç©º")
        
        request_id = kwargs.get("request_id")

        self.logger.info(f"ğŸ” request_id: {request_id}, å¼€å§‹å¤„ç†ä¿¡æ¯æŠ½å–è¯·æ±‚, text_length: {len(text)}, schema_fields: {len(schema)}")

        # æ„å»ºè¿è¡Œæ—¶é…ç½®
        run_config = {k: v for k, v in kwargs.items() if k in self.init_config or k == "request_id"}

        inputs = {
            "messages": [],
            "original_text": text,
            "extraction_schema": schema,
            "extraction_result": {},
            "validation_errors": [],
            "final_output": None
        }

        with timer(self.logger, f"request_id: {request_id}, å®Œæ•´ä¿¡æ¯æŠ½å–æµç¨‹"):
            # ä¼ é€’è¿è¡Œæ—¶é…ç½®
            config = {"configurable": run_config} if run_config else {}
            final_state = await self.graph.ainvoke(inputs, config=config)
            output = final_state.get("final_output", {})
            self.logger.success(f"ğŸ‰ request_id: {request_id}, ä¿¡æ¯æŠ½å–å®Œæˆ")

        return output
    
    async def run_stream(self, text: str, schema: Dict[str, Any], **kwargs) -> AsyncGenerator[StreamChunk, None]:
        """
        æµå¼æ‰§è¡Œä¿¡æ¯æŠ½å–æµç¨‹

        :param text: è¾“å…¥æ–‡æœ¬
        :param schema: æŠ½å–schemaå®šä¹‰
        :return: æµå¼è¾“å‡ºç”Ÿæˆå™¨
        """
        if not text.strip():
            raise ValueError("æ–‡æœ¬ä¸èƒ½ä¸ºç©º")
        
        if not schema:
            raise ValueError("Schemaä¸èƒ½ä¸ºç©º")
        
        request_id = kwargs.get("request_id")

        self.logger.info(f"ğŸ” request_id: {request_id}, å¼€å§‹æµå¼å¤„ç†ä¿¡æ¯æŠ½å–è¯·æ±‚, text_length: {len(text)}, schema_fields: {len(schema)}")

        # æ„å»ºè¿è¡Œæ—¶é…ç½®
        run_config = {k: v for k, v in kwargs.items() if k in self.init_config or k == "request_id"}

        inputs = {
            "messages": [],
            "original_text": text,
            "extraction_schema": schema,
            "extraction_result": {},
            "validation_errors": [],
            "final_output": None
        }

        with timer(self.logger, f"request_id: {request_id}, å®Œæ•´æµå¼ä¿¡æ¯æŠ½å–æµç¨‹"):
            # ä¼ é€’è¿è¡Œæ—¶é…ç½®
            config = {"configurable": run_config} if run_config else {}
            
            async for event in self.graph.astream_events(inputs, config=config):
                event_type = event.get("event", "")
                
                 # å¤„ç†LLMæµå¼è¾“å‡º
                if event_type == "on_chat_model_stream":
                    chunk = event.get("data", {}).get("chunk", None)
                    if chunk and hasattr(chunk, "chat_completion_chunk") and chunk.chat_completion_chunk:
                        chat_completion_chunk = chunk.chat_completion_chunk.to_dict()
                        choices = chat_completion_chunk.get("choices", [])
                        if choices:
                            delta = choices[0].get("delta", {})
                            content = delta.get("content", "")
                            reasoning_content = delta.get("reasoning_content", "")
                            
                            # è¾“å‡ºæ€è€ƒå†…å®¹
                            if reasoning_content:
                                yield StreamChunk(
                                    type="thinking",
                                    content=reasoning_content
                                )
                            # è¾“å‡ºç»“æœå†…å®¹
                            elif content:
                                yield StreamChunk(
                                    type="content", 
                                    content=content
                                )
                
                # å¤„ç†èŠ‚ç‚¹å¼€å§‹äº‹ä»¶
                elif event_type == "on_chain_start":
                    name = event.get("name", "")
                    if name == "initialize":
                        yield StreamChunk(
                            type="processing",
                            content="å¼€å§‹åˆå§‹åŒ–ä¿¡æ¯æŠ½å–ä»»åŠ¡..."
                        )
                    elif name == "extract":
                        yield StreamChunk(
                            type="processing",
                            content="æ­£åœ¨è°ƒç”¨LLMè¿›è¡Œä¿¡æ¯æŠ½å–..."
                        )
                    elif name == "validate":
                        yield StreamChunk(
                            type="processing", 
                            content="æ­£åœ¨éªŒè¯æŠ½å–ç»“æœ..."
                        )
                    elif name == "finalize":
                        yield StreamChunk(
                            type="processing",
                            content="æ­£åœ¨æ±‡æ€»æœ€ç»ˆç»“æœ..."
                        )
                
                # å¤„ç†å›¾ç»“æŸäº‹ä»¶, è¾“å‡ºæœ€ç»ˆç»“æœ
                elif event_type == "on_chain_end" and event.get("name", "") == "LangGraph":
                    output = event.get("data", {}).get("output", {})
                    final_output = output.get("final_output", {})
                        
                    yield StreamChunk(
                        type="final",
                        content="",
                        metadata=final_output
                    )

            self.logger.success(f"ğŸ‰ request_id: {request_id}, æµå¼ä¿¡æ¯æŠ½å–å®Œæˆ")


# ===== ä½¿ç”¨ç¤ºä¾‹ =====
async def main():
    """ä½¿ç”¨ç¤ºä¾‹"""
    from data.information_extraction.schema_sample import PRODUCT_REVIEW_SCHEMA
    from data.information_extraction.text_sample import TEST_PRODUCT_REVIEW
    
    # åˆå§‹åŒ–Agent
    agent = InformationExtractionAgent(
        name="test-ie-agent",
        base_url="https://api.deepseek.com/v1",
        api_key="YOUR_API_KEY",  # æ›¿æ¢ä¸ºå®é™…APIå¯†é’¥
        temperature=0.1
    )
    
    # æµå¼å¤„ç†ç¤ºä¾‹
    async for chunk in agent.run_stream(
        TEST_PRODUCT_REVIEW, 
        PRODUCT_REVIEW_SCHEMA, 
        request_id="test-ie-001"
    ):
        if chunk.type == "thinking":
            print(f"ğŸ¤” {chunk.content}")
        if chunk.type == "content":
            print(f"{chunk.content}", end="", flush=True)
        elif chunk.type == "final":
            result = chunk.metadata
            status = "æˆåŠŸ" if result["success"] else f"æœ‰{len(result['validation_errors'])}ä¸ªé”™è¯¯"
            print(f"âœ… ä¿¡æ¯æŠ½å–å®Œæˆ: {status}, ç½®ä¿¡åº¦: {result['confidence']:.2f}")
    
    # åŒæ­¥å¤„ç†ç¤ºä¾‹
    # result = await agent.run(
    #     TEST_PRODUCT_REVIEW, 
    #     PRODUCT_REVIEW_SCHEMA, 
    #     request_id="test-ie-002"
    # )
    
    # # æ‰“å°ç»“æœ
    # print(f"\nğŸ“Š ä¿¡æ¯æŠ½å–ç»“æœ:")
    # print(f"  çŠ¶æ€: {'âœ… æˆåŠŸ' if result['success'] else 'âŒ æœ‰é”™è¯¯'}")
    # print(f"  ç½®ä¿¡åº¦: {result['confidence']:.2f}")
    # print(f"  åŸæ–‡é•¿åº¦: {result['original_text_length']}")
    # print(f"  Schemaå­—æ®µæ•°: {len(result['schema_fields'])}")
    # print(f"  Tokenä½¿ç”¨æƒ…å†µ: {result.get('metadata', {}).get('usage', {})}")
    
    # if result['validation_errors']:
    #     print(f"  éªŒè¯é”™è¯¯: {len(result['validation_errors'])} ä¸ª")
    #     for error in result['validation_errors'][:3]:
    #         print(f"    - {error}")
    
    # print(f"\nğŸ“‹ æŠ½å–ç»“æœ:")
    # print(json.dumps(result['extraction_result'], ensure_ascii=False, indent=2))


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())