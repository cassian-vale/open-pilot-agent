# coding: utf-8
import asyncio
import json
import sys
from pathlib import Path
from pydantic import BaseModel, Field
from typing import AsyncGenerator, TypedDict, Annotated, List, Union, Optional, Tuple, Dict, Any, Iterator

from langgraph.graph import StateGraph, END
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.runnables import RunnableConfig
from langchain_core.messages.utils import convert_to_openai_messages

dir_name = Path(__file__).resolve().parent.parent.parent
sys.path.append(str(dir_name))

from base_agent import BaseAgent
from preprocess.chunk import TextChunker
from utils.time_count import timer
from utils.schema_parse import SchemaParser
from utils.stream_chunk import StreamChunk
from llm_api.llm_client_chat_model import LLMClientChatModel


# ===== è¾“å‡ºç»“æ„å®šä¹‰ =====
class QAOutput(BaseModel):
    answer: str = Field(
        description="åŸºäºç›¸å…³å¥å­ç”Ÿæˆçš„è‡ªç„¶è¯­è¨€ç­”æ¡ˆï¼Œç®€æ´å‡†ç¡®å®Œæ•´"
    )
    sentence_indices: List[Tuple[int, int]] = Field(
        description="ä¸é—®é¢˜è¯­ä¹‰ç›¸å…³å¥å­åœ¨åŸæ–‡ä¸­çš„ç´¢å¼•èŒƒå›´åˆ—è¡¨ï¼Œæ ¼å¼å¦‚ [[0, 91], [91, 173]]"
    )


# ===== çŠ¶æ€å®šä¹‰ =====
class AgentState(TypedDict):
    messages: Annotated[List[Union[HumanMessage, AIMessage]], "æ¶ˆæ¯å†å²"]
    doc_text: str
    query: str
    structured_doc: str
    final_output: Optional[dict]


# ===== Agent ä¸»ç±»ï¼ˆç»§æ‰¿åŸºç±»ï¼‰=====
class DocQAAgent(BaseAgent):
    def __init__(
            self,
            name: str,
            # openai client init config
            base_url: str = "https://api.deepseek.com/v1",
            api_key: Optional[str] = None,
            timeout: float = 60.0,
            max_retries: int = 3,
            # openai client run config
            model: str = "deepseek-chat",
            max_tokens: Optional[int] = None,
            temperature: float = 0.0,
            top_p: float = 1.0,
            stream: bool = False,
            enable_thinking: bool = False,
            # chunk config
            chunk_size: int = 512,
            overlap: int = 100,
            return_sentences: bool = True
    ):
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        super().__init__(
            name=name,
            model=model,
            base_url=base_url,
            api_key=api_key,
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
            timeout=timeout,
            max_retries=max_retries,
            stream=stream,
            enable_thinking=enable_thinking,  # ä¼ é€’æ€è€ƒæ¨¡å¼é…ç½®
        )

        # ä¿å­˜è‡ªå®šä¹‰é…ç½®
        self.init_config.update(
            {
                "chunk_size": chunk_size,
                "overlap": overlap,
                "return_sentences": return_sentences,
            }
        )

        self.chunker = TextChunker()

        # åˆå§‹åŒ–è¾“å‡ºè§£æå™¨
        self.output_parser = SchemaParser(QAOutput)

        # æ„å»ºå·¥ä½œæµå›¾
        self.graph = self._build_graph()

    def _build_graph(self) -> StateGraph:
        """æ„å»º LangGraph å·¥ä½œæµ"""
        graph = StateGraph(AgentState)

        def preprocess_node(state: AgentState, config: RunnableConfig) -> AgentState:
            run_config = config.get("configurable", {})
            request_id = run_config.get("request_id")

            with timer(self.logger, f"request_id: {request_id}, æ–‡æ¡£é¢„å¤„ç†"):
                # ä»configä¸­è·å–chunk_sizeï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨åˆå§‹åŒ–å€¼
                
                chunk_size = run_config.get("chunk_size", self.init_config.get("chunk_size"))
                overlap_size = run_config.get("overlap", self.init_config.get("overlap"))
                return_sentences = run_config.get("return_sentences", self.init_config.get("return_sentences"))

                chunks = self.chunker.chunk(
                    state["doc_text"],
                    chunk_size=chunk_size,
                    overlap=overlap_size,
                    return_sentences=return_sentences
                )

                structured_doc = self.chunker.add_start_end(chunks)
                self.logger.info(f"request_id: {request_id}, ğŸ“„ æ–‡æ¡£é¢„å¤„ç†å®Œæˆ")
                return {**state, "structured_doc": structured_doc}


        async def llm_qa_node(state: AgentState, config: RunnableConfig) -> AgentState:
            run_config = config.get("configurable", {})
            request_id = run_config.get("request_id")

            with timer(self.logger, f"request_id: {request_id}, LLMé—®ç­”"):
                # ä»configä¸­è·å–è¿è¡Œæ—¶å‚æ•°
                
                # æ„å»ºæç¤ºè¯
                prompt_text = self._get_prompt(
                    text=state["structured_doc"],
                    query=state["query"]
                )

                self.logger.info(f"request_id: {request_id}, LLM Input: {json.dumps({"input_text": prompt_text}, ensure_ascii=False)}")

                messages = [HumanMessage(content=prompt_text)]

                # è°ƒç”¨LLM      
                llm_client = self.get_llm_client(run_config)
                chat_model = LLMClientChatModel(llm_client=llm_client)

                try:
                    response = await chat_model.ainvoke(messages, config=config)
                    # ç»“æœè§£æ  
                    chat_completion = response.chat_completion.to_dict()

                    self.logger.info(f"request_id: {request_id}, LLM Response: {json.dumps(chat_completion, ensure_ascii=False)}")

                    choices = chat_completion.get("choices", [])
                    
                    final_output = {
                        "metadata": {
                            "usage": chat_completion.get("usage", {}),
                            # "messages": []
                        }
                    }

                    if len(choices) > 0:
                        content = choices[0].get("message", {}).get("content", "")
                        messages += [AIMessage(content=content)]
                        # final_output["metadata"]["messages"] = convert_to_openai_messages(messages)
                        final_output["content"] = content
                        reasoning_content = choices[0].get("message", {}).get("reasoning_content", "")
                        final_output["reasoning_content"] = reasoning_content
                        output_json = self.output_parser.parse_response_to_json(content)
                        final_output["output"] = output_json     

                        self.logger.info(f"request_id: {request_id}, LLM Parse Output: {json.dumps(output_json, ensure_ascii=False)}")          

                        return {
                            **state,
                            "messages": messages,
                            "final_output": final_output
                        }
                    else:
                        raise ValueError("LLM apiè¾“å‡ºé”™è¯¯ï¼Œchoicesä¸ºç©º")
                except asyncio.CancelledError:
                    self.logger.warning(f"â›” request_id: {request_id}, ä»»åŠ¡è¢«ä¸­æ–­ï¼Œå·²åœæ­¢ LLM è¯·æ±‚")
                    raise
                

        graph.add_node("preprocess", preprocess_node)
        graph.add_node("llm_qa", llm_qa_node)
        graph.set_entry_point("preprocess")
        graph.add_edge("preprocess", "llm_qa")
        graph.add_edge("llm_qa", END)

        return graph.compile()

    def _get_prompt(self, text: str, query: str) -> str:
        """ç”Ÿæˆ LLM æç¤ºè¯"""
        return f"""{text}

ä»¥ä¸Šæ˜¯ä¸€ç¯‡æ–‡ç« çš„å¥å­ç»“æ„åŒ–ç»“æœï¼ˆå·²ç»å¯¹åº”äº†å„ä¸ªå¥å­åœ¨æ–‡ç« ä¸­çš„ç´¢å¼•ï¼‰ï¼Œæˆ‘æœ‰ä¸€ä¸ªé—®é¢˜æ˜¯ï¼š{query}ã€‚

ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½é—®ç­”åŠ©ç†ï¼Œè¯·å®Œæˆä»¥ä¸‹ä¸¤ä¸ªä»»åŠ¡ï¼š
1. æ£€ç´¢å‡ºæ–‡ç« ä¸­æ‰€æœ‰ä¸è¿™ä¸ªé—®é¢˜ç›´æ¥ç›¸å…³çš„å¥å­ï¼Œè¾“å‡ºè¿™äº›å¥å­çš„ç´¢å¼•èŒƒå›´[start, end];è¦æ±‚ï¼šè¾“å‡ºçš„æ¯ä¸€ä¸ªå¥å­éƒ½å¿…é¡»èƒ½å¤Ÿæ”¯æŒå›ç­”é—®é¢˜ï¼›
2. åŸºäºè¿™äº›å¥å­ï¼Œç”Ÿæˆä¸€ä¸ªç®€æ´ã€å‡†ç¡®ã€å®Œæ•´çš„è‡ªç„¶è¯­è¨€ç­”æ¡ˆã€‚

æ³¨æ„ï¼šæœ€ç»ˆè¾“å‡ºçš„ç­”æ¡ˆçš„è¯­è¨€éœ€è¦ä¸¥æ ¼éµå¾ªç”¨æˆ·é—®é¢˜çš„è¯­è¨€ï¼Œé™¤éç”¨æˆ·é—®é¢˜é‡Œæ˜ç¡®æåˆ°ä½¿ç”¨æŸç§è¯­è¨€å›ç­”ã€‚

{self.output_parser.schema_generation_prompt}
"""

    async def run(self, doc_text: str, query: str, **kwargs) -> Dict[str, Any]:
        """
        æ‰§è¡Œæ–‡æ¡£é—®ç­”æµç¨‹

        :param doc_text: è¾“å…¥æ–‡æ¡£æ–‡æœ¬
        :param query: ç”¨æˆ·é—®é¢˜
        :return: ç»“æ„åŒ–è¾“å‡ºå­—å…¸
        """
        if not doc_text.strip():
            raise ValueError("æ–‡æ¡£æ–‡æœ¬ä¸èƒ½ä¸ºç©º")
        if not query.strip():
            raise ValueError("é—®é¢˜ä¸èƒ½ä¸ºç©º")
        
        request_id = kwargs.get("request_id")

        self.logger.info(f"ğŸ” request_id: {request_id}, å¼€å§‹å¤„ç†é—®ç­”è¯·æ±‚, query: {query}, doc_text_len: {len(doc_text)}")

        # æ„å»ºè¿è¡Œæ—¶é…ç½®
        run_config = {k: v for k, v in kwargs.items() if k in self.init_config or k == "request_id"}

        inputs = {
            "messages": [],
            "doc_text": doc_text,
            "query": query,
            "structured_doc": "",
            "final_output": None
        }

        with timer(self.logger, f"request_id: {request_id}, å®Œæ•´é—®ç­”æµç¨‹"):
            # ä¼ é€’è¿è¡Œæ—¶é…ç½®
            config = {"configurable": run_config} if run_config else {}
            final_state = await self.graph.ainvoke(inputs, config=config)
            output = final_state.get("final_output", {})
            self.logger.success(f"ğŸ‰ request_id: {request_id}, é—®ç­”å®Œæˆ")

        return output


    async def run_stream(self, doc_text: str, query: str, **kwargs) -> AsyncGenerator[StreamChunk, None]:
        """
        æµå¼æ‰§è¡Œæ–‡æ¡£é—®ç­”æµç¨‹

        :param doc_text: è¾“å…¥æ–‡æ¡£æ–‡æœ¬
        :param query: ç”¨æˆ·é—®é¢˜
        :return: æµå¼è¾“å‡ºè¿­ä»£å™¨
        """
        if not doc_text.strip():
            raise ValueError("æ–‡æ¡£æ–‡æœ¬ä¸èƒ½ä¸ºç©º")
        if not query.strip():
            raise ValueError("é—®é¢˜ä¸èƒ½ä¸ºç©º")
        
        request_id = kwargs.get("request_id")

        self.logger.info(f"ğŸ” request_id: {request_id}, å¼€å§‹å¤„ç†é—®ç­”æµå¼è¯·æ±‚, query: {query}, doc_text_len: {len(doc_text)}")

        # æ„å»ºè¿è¡Œæ—¶é…ç½®
        run_config = {k: v for k, v in kwargs.items() if k in self.init_config or k == "request_id"}

        inputs = {
            "messages": [],
            "doc_text": doc_text,
            "query": query,
            "structured_doc": "",
            "final_output": None
        }

        with timer(self.logger, f"request_id: {request_id}, å®Œæ•´æµå¼é—®ç­”æµç¨‹"):
            # ä¼ é€’è¿è¡Œæ—¶é…ç½®
            config = {"configurable": run_config} if run_config else {}
            async for event in self.graph.astream_events(inputs, config=config):
                event_type = event.get("event", "")
                if event_type == "on_chat_model_stream":
                    chunk = event.get("data", {}).get("chunk", None)
                    if chunk and hasattr(chunk, "chat_completion_chunk") and chunk.chat_completion_chunk:
                        chat_completion_chunk = chunk.chat_completion_chunk.to_dict()
                        choices = chat_completion_chunk.get("choices", [])
                        if choices:
                            delta = choices[0].get("delta", {})
                            content = delta.get("content", "")
                            reasoning_content = delta.get("reasoning_content", "")
                            if content:
                                yield StreamChunk(
                                    type="content",
                                    content=content
                                )
                            elif reasoning_content:
                                yield StreamChunk(
                                    type="thinking",
                                    content=reasoning_content
                                )                                                         
                elif event_type == "on_chain_end" and event.get("name", "") == "LangGraph":
                    output = event.get("data", {}).get("output", {})
                    yield StreamChunk(
                        type="final",
                        content="",
                        metadata=output.get("final_output", {})
                    )
            self.logger.success(f"ğŸ‰ request_id: {request_id}, æµå¼é—®ç­”å®Œæˆ")
