import asyncio
import json
import re
import sys
from pathlib import Path
from typing import AsyncGenerator, TypedDict, Annotated, List, Union, Optional, Dict, Any

from pydantic import BaseModel, Field
from langgraph.graph import StateGraph, END
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.runnables import RunnableConfig
from langchain_core.messages.utils import convert_to_openai_messages

dir_name = Path(__file__).resolve().parent.parent.parent
sys.path.append(str(dir_name))

from applications.keyword_generation.keyword_prompt import (
    KEYWORD_GENERATION_PROMPT,
    KEYWORD_GENERATION_SYSTEM_MESSAGE
)
from base_agent import BaseAgent
from llm_api.llm_client_chat_model import LLMClientChatModel
from utils.time_count import timer
from utils.stream_chunk import StreamChunk


# ===== è¾“å‡ºç»“æ„å®šä¹‰ =====
class KeywordGenerationOutput(BaseModel):
    success: bool = Field(description="å…³é”®è¯ç”Ÿæˆæ˜¯å¦æˆåŠŸ")
    keywords: List[str] = Field(description="ç”Ÿæˆçš„å…³é”®è¯åˆ—è¡¨ï¼ŒæŒ‰é‡è¦æ€§æ’åº")
    validation_errors: List[str] = Field(default_factory=list, description="éªŒè¯é”™è¯¯ä¿¡æ¯")
    confidence: float = Field(description="æ•´ä½“ç½®ä¿¡åº¦", ge=0, le=1)
    keyword_analysis: str = Field(description="å…³é”®è¯åˆ†æè¯´æ˜")
    statistics: Dict[str, Any] = Field(description="ç»Ÿè®¡ä¿¡æ¯")


# ===== çŠ¶æ€å®šä¹‰ =====
class KeywordGenerationState(TypedDict):
    messages: Annotated[List[Union[HumanMessage, AIMessage]], "æ¶ˆæ¯å†å²"]
    content: str
    domain_context: Optional[str]
    max_keywords: int
    generated_keywords: List[str]
    validation_errors: List[str]
    keyword_analysis: str
    statistics: Dict[str, Any]
    final_output: Optional[dict]


# ===== å…³é”®è¯ç”ŸæˆAgentä¸»ç±» =====
class KeywordGenerationAgent(BaseAgent):
    def __init__(
            self,
            name: str = "keyword-generation-agent",
            # openai client init config
            base_url: str = "https://api.deepseek.com/v1",
            api_key: Optional[str] = None,
            timeout: float = 60.0,
            max_retries: int = 3,
            # openai client run config
            model: str = "deepseek-chat",
            max_tokens: Optional[int] = None,
            temperature: float = 0.1,
            top_p: float = 1.0,
            stream: bool = False,
            enable_thinking: bool = False,
            default_max_keywords: int = 10
    ):
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        super().__init__(
            name=name,
            model=model,
            base_url=base_url,
            api_key=api_key,
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
            timeout=timeout,
            max_retries=max_retries,
            stream=stream,
            enable_thinking=enable_thinking,
        )

        self.default_max_keywords = default_max_keywords
        # æ„å»ºå·¥ä½œæµå›¾
        self.graph = self._build_graph()

    def _build_graph(self) -> StateGraph:
        """æ„å»º LangGraph å·¥ä½œæµ"""
        graph = StateGraph(KeywordGenerationState)

        def initialize_node(state: KeywordGenerationState, config: RunnableConfig) -> KeywordGenerationState:
            """åˆå§‹åŒ–èŠ‚ç‚¹ï¼šå‡†å¤‡å…³é”®è¯ç”Ÿæˆä»»åŠ¡"""
            run_config = config.get("configurable", {})
            request_id = run_config.get("request_id")

            with timer(self.logger, f"request_id: {request_id}, åˆå§‹åŒ–å…³é”®è¯ç”Ÿæˆä»»åŠ¡"):
                self.logger.info(f"request_id: {request_id}, å¼€å§‹å…³é”®è¯ç”Ÿæˆ, å†…å®¹é•¿åº¦: {len(state['content'])}")
                
                # è®¾ç½®é»˜è®¤é¢†åŸŸä¸Šä¸‹æ–‡å’Œæœ€å¤§å…³é”®è¯æ•°
                if not state.get("domain_context"):
                    state["domain_context"] = "é€šç”¨é¢†åŸŸ"
                
                if not state.get("max_keywords") or state["max_keywords"] <= 0:
                    state["max_keywords"] = self.default_max_keywords
                
                self.logger.debug(f"request_id: {request_id}, æœ€å¤§å…³é”®è¯æ•°: {state['max_keywords']}, é¢†åŸŸ: {state['domain_context']}")
                
                return state

        async def generate_node(state: KeywordGenerationState, config: RunnableConfig) -> KeywordGenerationState:
            """ç”ŸæˆèŠ‚ç‚¹ï¼šæ‰§è¡Œå…³é”®è¯ç”Ÿæˆ"""
            run_config = config.get("configurable", {})
            request_id = run_config.get("request_id")

            with timer(self.logger, f"request_id: {request_id}, æ‰§è¡Œå…³é”®è¯ç”Ÿæˆ"):
                # æ„å»ºæç¤ºè¯
                prompt_text = self._build_generation_prompt(
                    state["content"], 
                    state["domain_context"],
                    state["max_keywords"]
                )

                self.logger.info(f"request_id: {request_id}, è°ƒç”¨LLMè¿›è¡Œå…³é”®è¯ç”Ÿæˆ, å†…å®¹é•¿åº¦: {len(state['content'])}")

                messages = [SystemMessage(content=KEYWORD_GENERATION_SYSTEM_MESSAGE), HumanMessage(content=prompt_text)]

                # è°ƒç”¨LLM
                llm_client = self.get_llm_client(run_config)
                chat_model = LLMClientChatModel(llm_client=llm_client)

                try:
                    response = await chat_model.ainvoke(messages, config=config)

                    chat_completion = response.chat_completion.to_dict()
                    choices = chat_completion.get("choices", [])
                    
                    # åˆå§‹åŒ–æœ€ç»ˆè¾“å‡ºç»“æ„
                    final_output = {
                        "metadata": {
                            "usage": chat_completion.get("usage", {}),
                            # "messages": []
                        }
                    }

                    if len(choices) > 0:
                        content = choices[0].get("message", {}).get("content", "")
                        reasoning_content = choices[0].get("message", {}).get("reasoning_content", "")

                        self.logger.debug(f"request_id: {request_id}, LLMå“åº”é•¿åº¦: {len(content)}")

                        # æå–å…³é”®è¯ç»“æœ
                        generated_keywords = self._parse_generation_response(content)
                        
                        # ç”Ÿæˆå…³é”®è¯åˆ†æ
                        keyword_analysis = self._generate_keyword_analysis(generated_keywords, state["content"])
                        
                        # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
                        statistics = self._generate_statistics(generated_keywords, state["content"])
                        
                        # æ›´æ–°æ¶ˆæ¯å†å²
                        new_messages = state["messages"] + messages + [AIMessage(content=content)]
                        # final_output["metadata"]["messages"] = convert_to_openai_messages(new_messages)
                        
                        final_output["content"] = content
                        final_output["reasoning_content"] = reasoning_content

                        self.logger.info(f"request_id: {request_id}, LLM Parse Output: {json.dumps(generated_keywords, ensure_ascii=False)}")
                        
                        return {
                            **state,
                            "messages": new_messages,
                            "generated_keywords": generated_keywords,
                            "keyword_analysis": keyword_analysis,
                            "statistics": statistics,
                            "final_output": final_output
                        }
                    else:
                        raise ValueError("LLM apiè¾“å‡ºé”™è¯¯, choicesä¸ºç©º")
                
                except asyncio.CancelledError:
                    self.logger.warning(f"â›” request_id: {request_id}, ä»»åŠ¡è¢«ä¸­æ–­ï¼Œå·²åœæ­¢ LLM è¯·æ±‚")
                    raise

        def validate_node(state: KeywordGenerationState, config: RunnableConfig) -> KeywordGenerationState:
            """éªŒè¯èŠ‚ç‚¹ï¼šéªŒè¯ç”Ÿæˆçš„å…³é”®è¯"""
            run_config = config.get("configurable", {})
            request_id = run_config.get("request_id")

            with timer(self.logger, f"request_id: {request_id}, éªŒè¯ç”Ÿæˆçš„å…³é”®è¯"):
                validation_errors = []
                keywords = state["generated_keywords"]
                
                # éªŒè¯å…³é”®è¯æ ¼å¼
                if not isinstance(keywords, list):
                    validation_errors.append("å…³é”®è¯æ ¼å¼é”™è¯¯ï¼šåº”è¯¥æ˜¯ä¸€ä¸ªåˆ—è¡¨")
                
                if len(keywords) == 0:
                    validation_errors.append("æœªç”Ÿæˆä»»ä½•å…³é”®è¯")
                
                if len(keywords) > state["max_keywords"] * 1.5:  # å…è®¸ä¸€å®šçš„çµæ´»æ€§
                    validation_errors.append(f"ç”Ÿæˆçš„å…³é”®è¯æ•°é‡({len(keywords)})è¶…è¿‡é™åˆ¶({state['max_keywords']})")
                
                # éªŒè¯æ¯ä¸ªå…³é”®è¯
                for i, keyword in enumerate(keywords):
                    if not isinstance(keyword, str):
                        validation_errors.append(f"ç¬¬{i+1}ä¸ªå…³é”®è¯ä¸æ˜¯å­—ç¬¦ä¸²ç±»å‹")
                        continue
                    
                    keyword = keyword.strip()
                    if not keyword:
                        validation_errors.append(f"ç¬¬{i+1}ä¸ªå…³é”®è¯ä¸ºç©º")
                        continue
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯çŸ­è¯­ï¼ˆåŒ…å«ç©ºæ ¼ä½†å…è®¸ä¸“ä¸šå¤åˆè¯ï¼‰
                    if ' ' in keyword and not self._is_professional_term(keyword):
                        validation_errors.append(f"å…³é”®è¯'{keyword}'å¯èƒ½æ˜¯çŸ­è¯­è€Œéå•ä¸ªè¯è¯­")
                
                if not validation_errors:
                    self.logger.info(f"request_id: {request_id}, å…³é”®è¯éªŒè¯é€šè¿‡, ç”Ÿæˆ{len(keywords)}ä¸ªå…³é”®è¯")
                else:
                    error_count = len(validation_errors)
                    self.logger.warning(f"request_id: {request_id}, å‘ç° {error_count} ä¸ªå…³é”®è¯éªŒè¯é—®é¢˜")
                    for error in validation_errors[:3]:
                        self.logger.debug(f"å…³é”®è¯éªŒè¯é—®é¢˜: {error}")

                state["validation_errors"] = validation_errors
                return state

        def finalize_node(state: KeywordGenerationState, config: RunnableConfig) -> KeywordGenerationState:
            """æœ€ç»ˆå¤„ç†èŠ‚ç‚¹ï¼šæ±‡æ€»ç»“æœ"""
            run_config = config.get("configurable", {})
            request_id = run_config.get("request_id")

            with timer(self.logger, f"request_id: {request_id}, ç»“æœæ±‡æ€»"):
                success = len(state["validation_errors"]) == 0
                
                # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆåŸºäºéªŒè¯é”™è¯¯æ•°é‡å’Œå…³é”®è¯è´¨é‡ï¼‰
                base_confidence = max(0.0, 1.0 - len(state["validation_errors"]) * 0.2)
                
                # æ ¹æ®å…³é”®è¯æ•°é‡å’Œè´¨é‡è°ƒæ•´ç½®ä¿¡åº¦
                keyword_count = len(state["generated_keywords"])
                expected_count = state["max_keywords"]
                count_ratio = min(keyword_count / expected_count, 1.0) if expected_count > 0 else 1.0
                confidence = base_confidence * 0.7 + count_ratio * 0.3
                
                # æ„å»ºæœ€ç»ˆè¾“å‡ºï¼Œä¿ç•™metadataä¿¡æ¯
                final_output = state.get("final_output", {})
                final_output.update({
                    # "success": success,
                    "output": state["generated_keywords"][:expected_count],
                    # "validation_errors": state["validation_errors"],
                    "confidence": confidence,
                    # "keyword_analysis": state["keyword_analysis"],
                    # "statistics": state["statistics"],
                    # "content_length": len(state["content"]),
                    # "keyword_count": keyword_count,
                    # "max_keywords_set": state["max_keywords"]
                })

                status_msg = "æˆåŠŸ" if success else f"æœ‰{len(state['validation_errors'])}ä¸ªè­¦å‘Š"
                self.logger.success(f"request_id: {request_id}, å…³é”®è¯ç”Ÿæˆå®Œæˆ, çŠ¶æ€: {status_msg}, ç½®ä¿¡åº¦: {confidence:.2f}, ç”Ÿæˆ{keyword_count}ä¸ªå…³é”®è¯")

                return {
                    **state,
                    "final_output": final_output
                }

        # æ·»åŠ èŠ‚ç‚¹
        graph.add_node("initialize", initialize_node)
        graph.add_node("generate", generate_node)
        graph.add_node("validate", validate_node)
        graph.add_node("finalize", finalize_node)

        # è®¾ç½®å·¥ä½œæµ
        graph.set_entry_point("initialize")
        graph.add_edge("initialize", "generate")
        graph.add_edge("generate", "validate")
        graph.add_edge("validate", "finalize")
        graph.add_edge("finalize", END)

        return graph.compile()

    def _build_generation_prompt(self, content: str, domain_context: str, max_keywords: int) -> str:
        """æ„å»ºå…³é”®è¯ç”Ÿæˆæç¤ºè¯"""
        return KEYWORD_GENERATION_PROMPT.format(
            content=content,
            domain_context=domain_context,
            max_keywords=max_keywords
        )

    def _parse_generation_response(self, content: str) -> List[str]:
        """è§£æLLMçš„ç”Ÿæˆå“åº”"""
        try:
            # å°è¯•ç›´æ¥è§£æJSONæ•°ç»„
            keywords = json.loads(content)
            if isinstance(keywords, list):
                # æ¸…ç†æ¯ä¸ªå…³é”®è¯
                cleaned_keywords = []
                for keyword in keywords:
                    if isinstance(keyword, str):
                        cleaned_keyword = keyword.strip()
                        if cleaned_keyword:
                            cleaned_keywords.append(cleaned_keyword)
                return cleaned_keywords
        except json.JSONDecodeError:
            # å¦‚æœç›´æ¥è§£æå¤±è´¥, å°è¯•æå–JSONæ•°ç»„
            json_match = re.search(r'\[.*\]', content, re.DOTALL)
            if json_match:
                try:
                    keywords = json.loads(json_match.group())
                    if isinstance(keywords, list):
                        self.logger.warning("ä»å“åº”æ–‡æœ¬ä¸­æå–JSONæ•°ç»„æˆåŠŸ")
                        return [str(k).strip() for k in keywords if str(k).strip()]
                except json.JSONDecodeError as e:
                    self.logger.error(f"JSONæå–åè§£æå¤±è´¥: {e}")
            else:
                self.logger.error("æ— æ³•ä»å“åº”ä¸­æå–æœ‰æ•ˆçš„JSONæ•°ç»„")
        
        # å¦‚æœJSONè§£æå¤±è´¥ï¼Œå°è¯•æŒ‰è¡Œåˆ†å‰²
        lines = content.strip().split('\n')
        keywords = []
        for line in lines:
            line = line.strip()
            # ç§»é™¤ç¼–å·å’Œç‰¹æ®Šå­—ç¬¦
            line = re.sub(r'^\d+[\.\)\-\*]\s*', '', line)
            line = re.sub(r'^[\-\*]\s*', '', line)
            if line and len(line) < 50:  # é¿å…è¿‡é•¿çš„"å…³é”®è¯"
                keywords.append(line)
        
        self.logger.warning(f"ä½¿ç”¨å¤‡é€‰è§£ææ–¹æ³•ï¼Œæå–åˆ° {len(keywords)} ä¸ªå…³é”®è¯")
        return keywords[:self.default_max_keywords]

    def _generate_keyword_analysis(self, keywords: List[str], original_content: str) -> str:
        """ç”Ÿæˆå…³é”®è¯åˆ†æè¯´æ˜"""
        if not keywords:
            return "æœªç”Ÿæˆæœ‰æ•ˆå…³é”®è¯"
        
        analysis_parts = []
        
        # åŸºæœ¬ç»Ÿè®¡
        analysis_parts.append(f"å…±ç”Ÿæˆ {len(keywords)} ä¸ªå…³é”®è¯ï¼ŒæŒ‰é‡è¦æ€§æ’åºã€‚")
        
        # å…³é”®è¯ç±»å‹åˆ†æ
        single_word_count = sum(1 for k in keywords if ' ' not in k)
        compound_word_count = len(keywords) - single_word_count
        
        if compound_word_count > 0:
            analysis_parts.append(f"åŒ…å« {compound_word_count} ä¸ªä¸“ä¸šå¤åˆè¯ã€‚")
        
        # é‡è¦æ€§åˆ†å¸ƒè¯´æ˜
        if len(keywords) >= 3:
            top_keywords = keywords[:3]
            analysis_parts.append(f"æœ€é‡è¦çš„å‰3ä¸ªå…³é”®è¯æ˜¯ï¼š{', '.join(top_keywords)}")
        
        return " ".join(analysis_parts)

    def _generate_statistics(self, keywords: List[str], original_content: str) -> Dict[str, Any]:
        """ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
        content_words = len(original_content.split())
        keyword_chars = sum(len(k) for k in keywords)
        
        return {
            "total_keywords": len(keywords),
            "average_keyword_length": round(keyword_chars / len(keywords), 2) if keywords else 0,
            "content_word_count": content_words,
            "keyword_to_content_ratio": round(len(keywords) / content_words, 4) if content_words > 0 else 0,
            "single_word_keywords": sum(1 for k in keywords if ' ' not in k),
            "compound_word_keywords": sum(1 for k in keywords if ' ' in k),
        }

    def _is_professional_term(self, term: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºä¸“ä¸šå¤åˆè¯ï¼ˆç®€å•çš„å¯å‘å¼åˆ¤æ–­ï¼‰"""
        professional_indicators = [
            # å¸¸è§çš„ä¸“ä¸šå¤åˆè¯æ¨¡å¼
            r'.*[A-Z].*',  # åŒ…å«å¤§å†™å­—æ¯ï¼ˆå¦‚JavaScriptï¼‰
            r'.*[0-9].*',  # åŒ…å«æ•°å­—ï¼ˆå¦‚C++ï¼‰
            r'.*[+\-*/].*',  # åŒ…å«è¿ç®—ç¬¦å·
            r'^[A-Z].*',  # é¦–å­—æ¯å¤§å†™ï¼ˆå¯èƒ½ä¸ºä¸“æœ‰åè¯ï¼‰
        ]
        
        for pattern in professional_indicators:
            if re.match(pattern, term):
                return True
        return False

    async def run(self, content: str, domain_context: Optional[str] = None, max_keywords: Optional[int] = None, **kwargs) -> Dict[str, Any]:
        """
        æ‰§è¡Œå…³é”®è¯ç”Ÿæˆæµç¨‹

        :param content: éœ€è¦æå–å…³é”®è¯çš„å†…å®¹
        :param domain_context: é¢†åŸŸä¸Šä¸‹æ–‡ä¿¡æ¯
        :param max_keywords: æœ€å¤§å…³é”®è¯æ•°é‡
        :return: ç»“æ„åŒ–è¾“å‡ºå­—å…¸
        """
        if not content.strip():
            raise ValueError("å†…å®¹ä¸èƒ½ä¸ºç©º")
        
        request_id = kwargs.get("request_id")

        self.logger.info(f"ğŸ”§ request_id: {request_id}, å¼€å§‹å¤„ç†å…³é”®è¯ç”Ÿæˆè¯·æ±‚, å†…å®¹é•¿åº¦: {len(content)}")

        # æ„å»ºè¿è¡Œæ—¶é…ç½®
        run_config = {k: v for k, v in kwargs.items() if k in self.init_config or k == "request_id"}

        inputs = {
            "messages": [],
            "content": content,
            "domain_context": domain_context,
            "max_keywords": max_keywords or self.default_max_keywords,
            "generated_keywords": [],
            "validation_errors": [],
            "keyword_analysis": "",
            "statistics": {},
            "final_output": None
        }

        with timer(self.logger, f"request_id: {request_id}, å®Œæ•´å…³é”®è¯ç”Ÿæˆæµç¨‹"):
            # ä¼ é€’è¿è¡Œæ—¶é…ç½®
            config = {"configurable": run_config} if run_config else {}
            final_state = await self.graph.ainvoke(inputs, config=config)
            output = final_state.get("final_output", {})
            self.logger.success(f"ğŸ‰ request_id: {request_id}, å…³é”®è¯ç”Ÿæˆå®Œæˆ")

        return output
    
    async def run_stream(self, content: str, domain_context: Optional[str] = None, max_keywords: Optional[int] = None, **kwargs) -> AsyncGenerator[StreamChunk, None]:
        """
        æµå¼æ‰§è¡Œå…³é”®è¯ç”Ÿæˆæµç¨‹

        :param content: éœ€è¦æå–å…³é”®è¯çš„å†…å®¹
        :param domain_context: é¢†åŸŸä¸Šä¸‹æ–‡ä¿¡æ¯
        :param max_keywords: æœ€å¤§å…³é”®è¯æ•°é‡
        :return: æµå¼è¾“å‡ºç”Ÿæˆå™¨
        """
        if not content.strip():
            raise ValueError("å†…å®¹ä¸èƒ½ä¸ºç©º")
        
        request_id = kwargs.get("request_id")

        self.logger.info(f"ğŸ”§ request_id: {request_id}, å¼€å§‹æµå¼å¤„ç†å…³é”®è¯ç”Ÿæˆè¯·æ±‚, å†…å®¹é•¿åº¦: {len(content)}")

        # æ„å»ºè¿è¡Œæ—¶é…ç½®
        run_config = {k: v for k, v in kwargs.items() if k in self.init_config or k == "request_id"}

        inputs = {
            "messages": [],
            "content": content,
            "domain_context": domain_context,
            "max_keywords": max_keywords or self.default_max_keywords,
            "generated_keywords": [],
            "validation_errors": [],
            "keyword_analysis": "",
            "statistics": {},
            "final_output": None
        }

        with timer(self.logger, f"request_id: {request_id}, å®Œæ•´æµå¼å…³é”®è¯ç”Ÿæˆæµç¨‹"):
            # ä¼ é€’è¿è¡Œæ—¶é…ç½®
            config = {"configurable": run_config} if run_config else {}
            
            async for event in self.graph.astream_events(inputs, config=config):
                event_type = event.get("event", "")
                
                 # å¤„ç†LLMæµå¼è¾“å‡º
                if event_type == "on_chat_model_stream":
                    chunk = event.get("data", {}).get("chunk", None)
                    if chunk and hasattr(chunk, "chat_completion_chunk") and chunk.chat_completion_chunk:
                        chat_completion_chunk = chunk.chat_completion_chunk.to_dict()
                        choices = chat_completion_chunk.get("choices", [])
                        if choices:
                            delta = choices[0].get("delta", {})
                            content = delta.get("content", "")
                            reasoning_content = delta.get("reasoning_content", "")
                            
                            # è¾“å‡ºæ€è€ƒå†…å®¹
                            if reasoning_content:
                                yield StreamChunk(
                                    type="thinking",
                                    content=reasoning_content
                                )
                            # è¾“å‡ºç”Ÿæˆå†…å®¹
                            elif content:
                                yield StreamChunk(
                                    type="content", 
                                    content=content
                                )
                
                # å¤„ç†èŠ‚ç‚¹å¼€å§‹äº‹ä»¶
                elif event_type == "on_chain_start":
                    name = event.get("name", "")
                    if name == "initialize":
                        yield StreamChunk(
                            type="processing",
                            content="å¼€å§‹åˆå§‹åŒ–å…³é”®è¯ç”Ÿæˆä»»åŠ¡..."
                        )
                    elif name == "generate":
                        yield StreamChunk(
                            type="processing",
                            content="æ­£åœ¨åˆ†æå†…å®¹å¹¶æå–å…³é”®è¯..."
                        )
                    elif name == "validate":
                        yield StreamChunk(
                            type="processing", 
                            content="æ­£åœ¨éªŒè¯ç”Ÿæˆçš„å…³é”®è¯..."
                        )
                    elif name == "finalize":
                        yield StreamChunk(
                            type="processing",
                            content="æ­£åœ¨æ±‡æ€»æœ€ç»ˆç»“æœ..."
                        )
                
                # å¤„ç†å›¾ç»“æŸäº‹ä»¶, è¾“å‡ºæœ€ç»ˆç»“æœ
                elif event_type == "on_chain_end" and event.get("name", "") == "LangGraph":
                    output = event.get("data", {}).get("output", {})
                    final_output = output.get("final_output", {})
                        
                    yield StreamChunk(
                        type="final",
                        content="",
                        metadata=final_output
                    )

            self.logger.success(f"ğŸ‰ request_id: {request_id}, æµå¼å…³é”®è¯ç”Ÿæˆå®Œæˆ")


# ===== ä½¿ç”¨ç¤ºä¾‹ =====
async def main():
    """ä½¿ç”¨ç¤ºä¾‹"""
    
    # æµ‹è¯•å†…å®¹
    TEST_CONTENT = """
    äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä¼å›¾äº†è§£æ™ºèƒ½çš„å®è´¨ï¼Œå¹¶ç”Ÿäº§å‡ºä¸€ç§æ–°çš„èƒ½ä»¥äººç±»æ™ºèƒ½ç›¸ä¼¼çš„æ–¹å¼åšå‡ºååº”çš„æ™ºèƒ½æœºå™¨ï¼Œè¯¥é¢†åŸŸçš„ç ”ç©¶åŒ…æ‹¬æœºå™¨äººã€è¯­è¨€è¯†åˆ«ã€å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œä¸“å®¶ç³»ç»Ÿç­‰ã€‚
    è‡ªä»äººå·¥æ™ºèƒ½è¯ç”Ÿä»¥æ¥ï¼Œç†è®ºå’ŒæŠ€æœ¯æ—¥ç›Šæˆç†Ÿï¼Œåº”ç”¨é¢†åŸŸä¹Ÿä¸æ–­æ‰©å¤§ï¼Œå¯ä»¥è®¾æƒ³ï¼Œæœªæ¥äººå·¥æ™ºèƒ½å¸¦æ¥çš„ç§‘æŠ€äº§å“ï¼Œå°†ä¼šæ˜¯äººç±»æ™ºæ…§çš„å®¹å™¨ã€‚
    äººå·¥æ™ºèƒ½å¯ä»¥å¯¹äººçš„æ„è¯†ã€æ€ç»´çš„ä¿¡æ¯è¿‡ç¨‹çš„æ¨¡æ‹Ÿã€‚äººå·¥æ™ºèƒ½ä¸æ˜¯äººçš„æ™ºèƒ½ï¼Œä½†èƒ½åƒäººé‚£æ ·æ€è€ƒï¼Œä¹Ÿå¯èƒ½è¶…è¿‡äººçš„æ™ºèƒ½ã€‚
    """
    
    # åˆå§‹åŒ–Agent
    agent = KeywordGenerationAgent(
        name="test-keyword-agent",
        base_url="https://api.deepseek.com/v1",
        api_key="YOUR_API_KEY",  # æ›¿æ¢ä¸ºå®é™…APIå¯†é’¥
        temperature=0.1,
        default_max_keywords=8
    )
    
    # æµå¼å¤„ç†ç¤ºä¾‹
    print("=== æµå¼å…³é”®è¯ç”Ÿæˆ ===")
    async for chunk in agent.run_stream(
        TEST_CONTENT, 
        domain_context="äººå·¥æ™ºèƒ½æŠ€æœ¯",
        max_keywords=6,
        request_id="test-keyword-001"
    ):
        if chunk.type == "thinking":
            print(f"ğŸ¤” {chunk.content}")
        elif chunk.type == "content":
            print(f"{chunk.content}", end="", flush=True)
        elif chunk.type == "processing":
            print(f"ğŸ”„ {chunk.content}")
        elif chunk.type == "final":
            result = chunk.metadata
            status = "æˆåŠŸ" if result["success"] else f"æœ‰{len(result['validation_errors'])}ä¸ªè­¦å‘Š"
            print(f"\nâœ… å…³é”®è¯ç”Ÿæˆå®Œæˆ: {status}, ç½®ä¿¡åº¦: {result['confidence']:.2f}")
            print(f"ğŸ“Š å®é™…ç”Ÿæˆå…³é”®è¯æ•°: {result['keyword_count']}/{result['max_keywords_set']}")
            print(f"ğŸ“‹ å…³é”®è¯åˆ—è¡¨: {', '.join(result['keywords'])}")
            print(f"ğŸ“ åˆ†æ: {result['keyword_analysis']}")
    
    print("\n" + "="*50 + "\n")

    quit()


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())