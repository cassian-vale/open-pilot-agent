# coding: utf-8
import json
import re
import sys
import asyncio
from pathlib import Path
from pydantic import BaseModel, Field
from typing import AsyncGenerator, TypedDict, Annotated, List, Union, Optional, Tuple, Dict, Any, Iterator

from langgraph.graph import StateGraph, END
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.runnables import RunnableConfig
from langchain_core.messages.utils import convert_to_openai_messages

# å‡è®¾ç›¸å¯¹è·¯å¾„é…ç½®ä¿æŒä¸å˜
dir_name = Path(__file__).resolve().parent.parent.parent
sys.path.append(str(dir_name))

from base_agent import BaseAgent
from preprocess.long_text_preprocessor import LongTextPreprocessor
from applications.text_correction.text_correction_prompt import (
    ctc_system_prompt,
    ctc_user_prompt
)
from utils.time_count import timer
from utils.schema_parse import SchemaParser
from utils.stream_chunk import StreamChunk
from llm_api.llm_client_chat_model import LLMClientChatModel


# ===== è¾“å‡ºç»“æ„å®šä¹‰ =====
class CorrectionItem(BaseModel):
    error_type: str = Field(description="é”™è¯¯ç±»å‹ï¼šé”™åˆ«å­—/å½¢è¿‘å­—é”™è¯¯/éŸ³è¿‘å­—é”™è¯¯/æ‹¼éŸ³ä¸²é”™è¯¯ç­‰")
    original_text: str = Field(description="åŸé”™è¯¯æ–‡æœ¬")
    corrected_text: str = Field(description="ä¿®æ­£åçš„æ–‡æœ¬")
    reason: str = Field(description="é”™è¯¯åŸå› è¯´æ˜")
    confidence: int = Field(description="ç½®ä¿¡åº¦, 0-5åˆ†", ge=0, le=5)
    sentence_start_idx: int = Field(description="é”™è¯¯åœ¨åŸæ–‡ä¸­çš„èµ·å§‹ä½ç½®")
    sentence_end_idx: int = Field(description="é”™è¯¯åœ¨åŸæ–‡ä¸­çš„ç»“æŸä½ç½®")


class CorrectionOutput(BaseModel):
    corrections: List[CorrectionItem] = Field(description="çº é”™ç»“æœåˆ—è¡¨, æ— é”™è¯¯åˆ™ä¸ºç©ºåˆ—è¡¨")


# ===== æœ€ç»ˆå“åº”ç»“æ„ =====
class TextCorrectionResponse(BaseModel):
    output: Dict[str, Any] = Field(description="ä¸šåŠ¡ç»“æœï¼ŒåŒ…å«corrections, corrected_textç­‰")
    content: str = Field(default="", description="æ¨¡å‹æœ€ç»ˆè¾“å‡º")
    reasoning_content: str = Field(default="", description="æ€è€ƒè¿‡ç¨‹")
    metadata: Dict[str, Any] = Field(default=None, description="å…ƒæ•°æ®")
    confidence: float = Field(default=1.0, description="æ•´ä½“ç½®ä¿¡åº¦", ge=0, le=1)


# ===== çŠ¶æ€å®šä¹‰ =====
class CorrectionState(TypedDict):
    messages: Annotated[List[Union[HumanMessage, AIMessage]], "æ¶ˆæ¯å†å²"]
    original_text: str
    processed_chunks: List[Dict[str, Any]]
    corrections: List[Dict[str, Any]]
    current_chunk_index: int
    final_output: Optional[TextCorrectionResponse]
    # [æ–°å¢] ç”¨äºå­˜å‚¨ç´¯åŠ çš„ token usage
    usage: Dict[str, int]


# ===== çº é”™Agentä¸»ç±» =====
class TextCorrectionAgent(BaseAgent):
    def __init__(
            self,
            name: str = "text-correction-agent",
            # openai client init config
            base_url: str = "https://api.deepseek.com/v1",
            api_key: Optional[str] = None,
            timeout: float = 60.0,
            max_retries: int = 3,
            # openai client run config
            model: str = "deepseek-chat",
            max_tokens: Optional[int] = None,
            temperature: float = 0.0,
            top_p: float = 1.0,
            stream: bool = False,
            enable_thinking: bool = False,
            # chunk config
            max_chunk_length: int = 512  # é™åˆ¶æ¯ä¸ªçº é”™å—çš„æœ€å¤§é•¿åº¦
    ):
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        super().__init__(
            name=name,
            model=model,
            base_url=base_url,
            api_key=api_key,
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
            timeout=timeout,
            max_retries=max_retries,
            stream=stream,
            enable_thinking=enable_thinking,
        )

        # ä¿å­˜è‡ªå®šä¹‰é…ç½®
        self.init_config.update({
            "max_chunk_length": max_chunk_length,
        })

        # åˆå§‹åŒ–ç»„ä»¶
        self.preprocessor = LongTextPreprocessor()
        self.output_parser = SchemaParser(CorrectionOutput)

        # æ„å»ºå·¥ä½œæµå›¾
        self.graph = self._build_graph()

    def _build_graph(self) -> StateGraph:
        """æ„å»º LangGraph å·¥ä½œæµ"""
        graph = StateGraph(CorrectionState)

        def preprocess_node(state: CorrectionState, config: RunnableConfig) -> CorrectionState:
            """é¢„å¤„ç†èŠ‚ç‚¹ï¼šæ–‡æœ¬åˆ†å—"""
            run_config = config.get("configurable", {})
            request_id = run_config.get("request_id")

            with timer(self.logger, f"request_id: {request_id}, æ–‡æœ¬é¢„å¤„ç†åˆ†å—"):
                # ä»configä¸­è·å–max_chunk_length, å¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨åˆå§‹åŒ–å€¼
                max_chunk_length = run_config.get("max_chunk_length", self.init_config.get("max_chunk_length", 512))

                chunks = self.preprocessor.prepare_correction_chunks(
                    state["original_text"],
                    max_chunk_length=max_chunk_length
                )

                self.logger.info(f"request_id: {request_id}, æ–‡æœ¬åˆ†å—å®Œæˆ, å…±{len(chunks)}ä¸ªå—, æœ€å¤§å—é•¿åº¦{max_chunk_length}")
                for i, chunk in enumerate(chunks):
                    self.logger.debug(f"å— {i+1}: ä½ç½®[{chunk['text_start']}-{chunk['text_end']}], é•¿åº¦{len(chunk['text'])}")
                
                return {
                    **state,
                    "processed_chunks": chunks,
                    "corrections": [],
                    "current_chunk_index": 0,
                    # [æ–°å¢] åˆå§‹åŒ– usage è®¡æ•°å™¨
                    "usage": {
                        "prompt_tokens": 0, 
                        "completion_tokens": 0, 
                        "total_tokens": 0
                    }
                }

        async def correct_chunk_node(state: CorrectionState, config: RunnableConfig) -> CorrectionState:
            """çº é”™èŠ‚ç‚¹ï¼šå¤„ç†å•ä¸ªæ–‡æœ¬å—"""
            run_config = config.get("configurable", {})
            request_id = run_config.get("request_id")

            current_index = state["current_chunk_index"]
            chunks = state["processed_chunks"]

            if current_index >= len(chunks):
                return state

            current_chunk = chunks[current_index]

            with timer(self.logger, f"request_id: {request_id}, å¤„ç†æ–‡æœ¬å— {current_index + 1}/{len(chunks)}"):
                # æ£€æŸ¥å—é•¿åº¦æ˜¯å¦è¶…è¿‡é™åˆ¶
                max_chunk_length = run_config.get("max_chunk_length", self.init_config.get("max_chunk_length", 512))
                if len(current_chunk["text"]) > max_chunk_length:
                    self.logger.warning(f"request_id: {request_id}, å— {current_index + 1} é•¿åº¦ {len(current_chunk['text'])} è¶…è¿‡é™åˆ¶ {max_chunk_length}")

                # æ„å»ºæç¤ºè¯
                prompt_text = self._get_correction_prompt(current_chunk)

                self.logger.info(f"request_id: {request_id}, å¤„ç†å— {current_index + 1}, é•¿åº¦: {len(current_chunk['text'])}, ä½ç½®: {current_chunk['text_start']}-{current_chunk['text_end']}")

                messages = [HumanMessage(content=prompt_text)]

                # è°ƒç”¨LLM
                llm_client = self.get_llm_client(run_config)
                chat_model = LLMClientChatModel(llm_client=llm_client)
                try:
                    response = await chat_model.ainvoke(messages, config=config)
                    
                    # ç»“æœè§£æ
                    chat_completion = response.chat_completion.to_dict()
                    choices = chat_completion.get("choices", [])

                    # [æ–°å¢] è·å–å¹¶ç´¯åŠ  usage
                    current_usage = chat_completion.get("usage", {}) or {}
                    prev_usage = state.get("usage", {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0})
                    
                    new_usage = {
                        "prompt_tokens": prev_usage.get("prompt_tokens", 0) + current_usage.get("prompt_tokens", 0),
                        "completion_tokens": prev_usage.get("completion_tokens", 0) + current_usage.get("completion_tokens", 0),
                        "total_tokens": prev_usage.get("total_tokens", 0) + current_usage.get("total_tokens", 0)
                    }
                    
                    if len(choices) > 0:
                        content = choices[0].get("message", {}).get("content", "")
                        reasoning_content = choices[0].get("message", {}).get("reasoning_content", "")
                        
                        # è¿™é‡Œçš„ final_output ä»…ä½œä¸­é—´å­˜å‚¨ï¼Œæœ€ç»ˆä¼šç”± finalize_node è¦†ç›–
                        temp_final_output = state.get("final_output", {}) or {}
                        temp_final_output["reasoning_content"] = reasoning_content
                        
                        self.logger.debug(f"request_id: {request_id}, LLM Response Length: {len(content)}")
                        
                        # è§£æçº é”™ç»“æœ
                        try:
                            correction_data = self.output_parser.parse_response_to_json(content)
                            corrections = correction_data.get("corrections", [])
                            
                            # éªŒè¯å¹¶ä¿®æ­£ä½ç½®ä¿¡æ¯
                            valid_corrections = []
                            for correction in corrections:
                                start_idx = correction.get("sentence_start_idx", 0)
                                end_idx = correction.get("sentence_end_idx", 0)
                                original_text = correction.get("original_text", "")
                                corrected_text = correction.get("corrected_text", "")

                                if not corrected_text:
                                    self.logger.warning(f"request_id: {request_id}, LLMæ²¡æœ‰è¾“å‡ºä¿®æ­£æ–‡æœ¬")
                                    continue
                                
                                # å¦‚æœä½ç½®ä¿¡æ¯çœ‹èµ·æ¥ä¸åˆç†, å°è¯•åŸºäºæ–‡æœ¬åŒ¹é…ä¿®æ­£
                                if start_idx < current_chunk["text_start"] or end_idx > current_chunk["text_end"]:
                                    self.logger.warning(f"request_id: {request_id}, ä½ç½®ä¿¡æ¯å¼‚å¸¸")
                                    continue
                                
                                chunk_text = current_chunk["text"]
                                corrected_sentence = chunk_text[start_idx-current_chunk["text_start"]: end_idx-current_chunk["text_start"]]
                                correction["corrected_sentence"] = corrected_sentence
                                if original_text in corrected_sentence:
                                    corrected_start = corrected_sentence.find(original_text)
                                    if corrected_start != -1:
                                        corrected_end = corrected_start + len(original_text)
                                        correction["corrected_sentence"] = corrected_sentence[:corrected_start] + corrected_text + corrected_sentence[corrected_end:] 
                                        self.logger.info(f"request_id: {request_id}, å¥å­ä¿®æ­£ä¸º: {correction['corrected_sentence']}")

                                        # ä¸ºæ¯ä¸ªçº æ­£æ·»åŠ å—ä¿¡æ¯
                                        correction["chunk_index"] = current_index
                                        correction["chunk_text"] = current_chunk["text"]
                                        valid_corrections.append(correction)
                            
                            self.logger.info(f"request_id: {request_id}, å— {current_index + 1} å‘ç° {len(valid_corrections)} ä¸ªé”™è¯¯")
                            
                        except Exception as e:
                            import traceback
                            self.logger.error(f"request_id: {request_id}, è§£æçº é”™ç»“æœå¤±è´¥: {traceback.format_exc()}")
                            valid_corrections = []
                        
                        # æ›´æ–°æ¶ˆæ¯å†å²
                        new_messages = state["messages"] + [HumanMessage(content=prompt_text)] + [AIMessage(content=content)]
                        
                        temp_final_output["content"] = content
                        
                        return {
                            **state,
                            "messages": new_messages,
                            "corrections": state["corrections"] + valid_corrections,
                            "current_chunk_index": current_index + 1,
                            "final_output": temp_final_output,
                            "usage": new_usage # [æ–°å¢] æ›´æ–°ç´¯åŠ åçš„ usage
                        }
                    else:
                        raise ValueError("LLM apiè¾“å‡ºé”™è¯¯, choicesä¸ºç©º")
                except asyncio.CancelledError:
                    self.logger.warning(f"â›” request_id: {request_id}, ä»»åŠ¡è¢«ä¸­æ–­ï¼Œå·²åœæ­¢ LLM è¯·æ±‚")
                    raise

        def finalize_node(state: CorrectionState, config: RunnableConfig) -> CorrectionState:
            """æœ€ç»ˆå¤„ç†èŠ‚ç‚¹ï¼šæ±‡æ€»ç»“æœ"""
            run_config = config.get("configurable", {})
            request_id = run_config.get("request_id")
            final_output = state.get("final_output", {}) or {}

            with timer(self.logger, f"request_id: {request_id}, ç»“æœæ±‡æ€»"):
                # åº”ç”¨æ‰€æœ‰çº æ­£åˆ°åŸæ–‡æœ¬
                corrected_text = self._apply_corrections(
                    state["original_text"], 
                    state["corrections"]
                )
                
                # è®¡ç®—æ•´ä½“ç½®ä¿¡åº¦
                overall_confidence = 1.0
                if state["corrections"]:
                    avg_confidence = sum(c.get("confidence", 0) for c in state["corrections"]) / len(state["corrections"])
                    overall_confidence = avg_confidence / 5.0  # å½’ä¸€åŒ–åˆ°0-1
                
                # æ„å»ºä¸šåŠ¡è¾“å‡º
                business_output = {
                    "original_text": state["original_text"],
                    "total_errors": len(state["corrections"]),
                    "corrections": state["corrections"],
                    "corrected_text": corrected_text
                }

                # [ä¿®æ”¹] ç»„è£…æœ€ç»ˆ metadataï¼ŒåŒ…å«ç´¯åŠ çš„ token usage
                metadata = {
                    "usage": state.get("usage", {}),
                    "chunk_count": len(state["processed_chunks"])
                }

                final_output["output"] = business_output
                final_output["confidence"] = overall_confidence
                final_output["metadata"] = metadata

                self.logger.success(f"request_id: {request_id}, çº é”™å®Œæˆ, å…±å¤„ç† {len(state['processed_chunks'])} ä¸ªå—, å‘ç° {len(state['corrections'])} ä¸ªé”™è¯¯")
                self.logger.info(f"request_id: {request_id}, Total Token Usage: {state.get('usage')}")

                return {
                    **state,
                    "final_output": final_output
                }

        def should_continue(state: CorrectionState) -> str:
            """åˆ¤æ–­æ˜¯å¦ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªç‰‡æ®µ"""
            if state["current_chunk_index"] >= len(state["processed_chunks"]):
                return "end"
            return "continue"

        # æ·»åŠ èŠ‚ç‚¹
        graph.add_node("preprocess", preprocess_node)
        graph.add_node("correct_chunk", correct_chunk_node)
        graph.add_node("finalize", finalize_node)

        # è®¾ç½®å·¥ä½œæµ
        graph.set_entry_point("preprocess")
        graph.add_edge("preprocess", "correct_chunk")
        
        # æ¡ä»¶è¾¹ï¼šå¾ªç¯å¤„ç†æ‰€æœ‰å—
        graph.add_conditional_edges(
            "correct_chunk",
            should_continue,
            {
                "continue": "correct_chunk",
                "end": "finalize"
            }
        )
        
        graph.add_edge("finalize", END)

        return graph.compile()

    def _get_correction_prompt(self, chunk: Dict[str, Any]) -> str:
        """ç”Ÿæˆçº é”™æç¤ºè¯"""
        return f"{ctc_system_prompt}\n\n{self.output_parser.schema_generation_prompt}\n\n{ctc_user_prompt.format(text=chunk)}"

    def _apply_corrections(self, original_text: str, corrections: List[Dict]) -> str:
        """åº”ç”¨æ‰€æœ‰çº æ­£åˆ°åŸæ–‡æœ¬"""
        if not corrections:
            return original_text

        # æŒ‰å¥å­èµ·å§‹ä½ç½®æ­£åºæ’åº
        corrections_sorted = sorted(corrections, key=lambda x: x["sentence_start_idx"])
        
        result = []
        current_pos = 0
        
        for correction in corrections_sorted:
            start = correction["sentence_start_idx"]
            end = correction["sentence_end_idx"]
            corrected = correction["corrected_sentence"]
            
            # æ·»åŠ å½“å‰ä¿®æ­£ç‚¹ä¹‹å‰çš„æ–‡æœ¬
            if current_pos < start:
                result.append(original_text[current_pos:start])
            
            # æ·»åŠ ä¿®æ­£åçš„æ–‡æœ¬
            result.append(corrected)
            current_pos = end
        
        # æ·»åŠ å‰©ä½™æ–‡æœ¬
        if current_pos < len(original_text):
            result.append(original_text[current_pos:])
        
        return "".join(result)

    async def run(self, text: str, **kwargs) -> TextCorrectionResponse:
        """
        æ‰§è¡Œæ–‡æœ¬çº é”™æµç¨‹
        """
        if not text.strip():
            raise ValueError("æ–‡æœ¬ä¸èƒ½ä¸ºç©º")
        
        request_id = kwargs.get("request_id")

        self.logger.info(f"ğŸ” request_id: {request_id}, å¼€å§‹å¤„ç†çº é”™è¯·æ±‚, text_length: {len(text)}")

        # æ„å»ºè¿è¡Œæ—¶é…ç½®
        run_config = {k: v for k, v in kwargs.items() if k in self.init_config or k == "request_id"}

        inputs = {
            "messages": [],
            "original_text": text,
            "processed_chunks": [],
            "corrections": [],
            "current_chunk_index": 0,
            "final_output": None,
            "usage": {} # åˆå§‹åŒ–ä¸ºç©ºï¼Œpreprocess èŠ‚ç‚¹ä¼šå¡«å……åˆå§‹å€¼
        }

        with timer(self.logger, f"request_id: {request_id}, å®Œæ•´çº é”™æµç¨‹"):
            config = {"configurable": run_config} if run_config else {}
            final_state = await self.graph.ainvoke(inputs, config=config)
            output = final_state.get("final_output")
            self.logger.success(f"ğŸ‰ request_id: {request_id}, çº é”™å®Œæˆ")

        return output
    
    async def run_stream(self, text: str, **kwargs) -> AsyncGenerator[StreamChunk, None]:
        """
        æµå¼æ‰§è¡Œæ–‡æœ¬çº é”™æµç¨‹
        """
        if not text.strip():
            raise ValueError("æ–‡æœ¬ä¸èƒ½ä¸ºç©º")
        
        request_id = kwargs.get("request_id")

        self.logger.info(f"ğŸ” request_id: {request_id}, å¼€å§‹æµå¼å¤„ç†çº é”™è¯·æ±‚, text_length: {len(text)}")

        run_config = {k: v for k, v in kwargs.items() if k in self.init_config or k == "request_id"}

        inputs = {
            "messages": [],
            "original_text": text,
            "processed_chunks": [],
            "corrections": [],
            "current_chunk_index": 0,
            "final_output": None,
            "usage": {}
        }

        with timer(self.logger, f"request_id: {request_id}, å®Œæ•´æµå¼çº é”™æµç¨‹"):
            config = {"configurable": run_config} if run_config else {}
            
            async for event in self.graph.astream_events(inputs, config=config):
                event_type = event.get("event", "")
                
                # å¤„ç†LLMæµå¼è¾“å‡º
                if event_type == "on_chat_model_stream":
                    chunk = event.get("data", {}).get("chunk", None)
                    if chunk and hasattr(chunk, "chat_completion_chunk") and chunk.chat_completion_chunk:
                        chat_completion_chunk = chunk.chat_completion_chunk.to_dict()
                        choices = chat_completion_chunk.get("choices", [])
                        if choices:
                            delta = choices[0].get("delta", {})
                            content = delta.get("content", "")
                            reasoning_content = delta.get("reasoning_content", "")
                            
                            if reasoning_content:
                                yield StreamChunk(
                                    type="thinking",
                                    content=reasoning_content
                                )
                            elif content:
                                yield StreamChunk(
                                    type="content", 
                                    content=content
                                )
                
                # å¤„ç†èŠ‚ç‚¹äº‹ä»¶
                elif event_type == "on_chain_start":
                    name = event.get("name", "")
                    if name == "preprocess":
                        yield StreamChunk(
                            type="processing",
                            content="å¼€å§‹æ–‡æœ¬é¢„å¤„ç†å’Œåˆ†å—..."
                        )
                    elif name == "correct_chunk":
                        tags = event.get("tags", [])
                        if tags and "graph:step:" in tags[0]:
                            current_index = event.get("data", {}).get("input", {}).get("current_chunk_index", 0)
                            chunks = event.get("data", {}).get("input", {}).get("processed_chunks", [])
                            if current_index < len(chunks):
                                yield StreamChunk(
                                    type="processing",
                                    content=f"æ­£åœ¨å¤„ç†ç¬¬ {current_index + 1}/{len(chunks)} ä¸ªæ–‡æœ¬å—..."
                                )
                    elif name == "finalize":
                        yield StreamChunk(
                            type="processing", 
                            content="æ­£åœ¨æ±‡æ€»æœ€ç»ˆçº é”™ç»“æœ..."
                        )
                
                elif event_type == "on_chain_end":
                    name = event.get("name", "")
                    if name == "preprocess":
                        output = event.get("data", {}).get("output", {})
                        chunk_count = len(output.get("processed_chunks", []))
                        yield StreamChunk(
                            type="processing",
                            content=f"æ–‡æœ¬é¢„å¤„ç†å®Œæˆ, å…±åˆ†æˆ {chunk_count} ä¸ªå—"
                        )
                    elif name == "correct_chunk":
                        tags = event.get("tags", [])
                        if tags and "graph:step:" in tags[0]:
                            output = event.get("data", {}).get("output", {})
                            current_index = output.get("current_chunk_index", 0)
                            corrections = output.get("corrections", [])
                            
                            current_corrections = [
                                c for c in corrections 
                                if c.get("chunk_index", -1) == current_index - 1
                            ]
                            
                            if current_corrections:
                                yield StreamChunk(
                                    type="processing",
                                    content=f"ç¬¬ {current_index} ä¸ªæ–‡æœ¬å—å¤„ç†å®Œæˆ, å‘ç° {len(current_corrections)} ä¸ªé”™è¯¯"
                                )     
                
                    elif name == "LangGraph":
                        output = event.get("data", {}).get("output", {})
                        final_output = output.get("final_output")
                        
                        yield StreamChunk(
                            type="final",
                            content="",
                            metadata=final_output
                        )

            self.logger.success(f"ğŸ‰ request_id: {request_id}, æµå¼çº é”™å®Œæˆ")


# ===== ä½¿ç”¨ç¤ºä¾‹ =====
async def main():
    """ä½¿ç”¨ç¤ºä¾‹"""
    agent = TextCorrectionAgent(
        name="test-correction-agent",
        base_url="https://api.deepseek.com/v1",
        api_key="sk-xxxx", # æ›¿æ¢ä½ çš„ key
        max_chunk_length=512
    )
    
    test_text = """é™•å˜»è¢å®¶æ‘å¤ªå’Œå±…ç‘æ–¯ä¸½é…’åº—æœ‰é™å…¬å¸ã€‚"""

    # åŒæ­¥è°ƒç”¨ç¤ºä¾‹
    result = await agent.run(test_text, request_id="test-001")
    print(json.dumps(result, ensure_ascii=False, indent=2))


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())