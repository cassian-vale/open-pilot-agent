# BaseAgent è¿›é˜¶å¼€å‘æŒ‡å—

## 1.è‡ªå®šä¹‰ LLMClient ä¸ LangGraph ç»Ÿä¸€æ‰§è¡Œæœºåˆ¶çš„æ·±åº¦é›†æˆ

### 1.1 æ ¸å¿ƒç»„ä»¶æ¶æ„

#### 1.1.1 LLMClientChatModel ç±»ç»“æ„

```python
class LLMClientChatModel(BaseChatModel):
    """å°† LLMClient åŒ…è£…æˆ LangChain ChatModelï¼Œæ”¯æŒè‡ªå®šä¹‰æ¶ˆæ¯ç±»å‹"""
    llm_client: LLMClient = Field(..., description="LLMå®¢æˆ·ç«¯å®ä¾‹")
    
    # æ ¸å¿ƒæ–¹æ³•
    _generate()      # åŒæ­¥éæµå¼
    _stream()        # åŒæ­¥æµå¼  
    _agenerate()     # å¼‚æ­¥éæµå¼
    _astream()       # å¼‚æ­¥æµå¼
    _generate_with_cache()    # åŒæ­¥ç¼“å­˜å’Œæµå¼å†³ç­–
    _agenerate_with_cache()   # å¼‚æ­¥ç¼“å­˜å’Œæµå¼å†³ç­–
```

#### 1.1.2 è‡ªå®šä¹‰æ¶ˆæ¯ç±»å‹

```python
# æ”¯æŒå®Œæ•´çš„ OpenAI åŸç”Ÿçš„ ChatCompletion / ChatCompletionChunk å¯¹è±¡ä¼ é€’
ChatMessage(content="", chat_completion=response)
ChatMessageChunk(content="", chat_completion_chunk=chunk)
merge_chunks_to_completion  # æ”¯æŒå°†ChatCompletionChunkæµåˆå¹¶ä¸ºChatCompletionå“åº”çš„å‡½æ•°
```

### 1.2. LangGraph ç»Ÿä¸€æ‰§è¡Œæœºåˆ¶
 

#### 1.2.1 åŒæ­¥å›¾çš„æ‰§è¡Œæµç¨‹ (graph.invokeæ ¸å¿ƒè°ƒç”¨é“¾)

```mermaid
graph TD
    A[ç”¨æˆ·ä»£ç ] --> B[graph.invoke<br/>inputs, config]
    B --> C[LangGraph._execute_graph<br/>åŒæ­¥å›¾æ‰§è¡Œå¼•æ“]
    C --> D[Node.invoke<br/>å¯¹æ¯ä¸ªèŠ‚ç‚¹è°ƒç”¨ invoke]
    D --> E[LLMClientChatModel.invoke]
    E --> F[LLMClientChatModel._generate_with_cache<br/>åŒæ­¥ç¼“å­˜å’Œæµå¼å†³ç­–]
    
    F --> G{_should_stream æ£€æµ‹}
    G -->|éœ€è¦æµå¼| H[_stream + merge_chunks_to_completion<br/>run_manager=None è·³è¿‡äº‹ä»¶]
    G -->|éæµå¼| I[_generate<br/>ç›´æ¥è¿”å›å®Œæ•´ç»“æœ]
    
    H --> J[è¿”å›å®Œæ•´ ChatCompletion]
    I --> J
    
    J --> K[ç”¨æˆ·è·å¾—æœ€ç»ˆçŠ¶æ€]
```


#### 1.2.2 å¼‚æ­¥å›¾çš„æ‰§è¡Œæµç¨‹ (graph.ainvokeæ ¸å¿ƒè°ƒç”¨é“¾)

```mermaid
graph TD
    A[ç”¨æˆ·ä»£ç  async] --> B[await graph.ainvoke<br/>inputs, config]
    B --> C[LangGraph._aexecute_graph<br/>å¼‚æ­¥å›¾æ‰§è¡Œå¼•æ“]
    C --> D[Node.ainvoke<br/>å¯¹æ¯ä¸ªèŠ‚ç‚¹è°ƒç”¨ ainvoke]
    D --> E[LLMClientChatModel.ainvoke]
    E --> F[LLMClientChatModel._agenerate_with_cache<br/>å¼‚æ­¥ç¼“å­˜å’Œæµå¼å†³ç­–]
    
    F --> G{_should_stream æ£€æµ‹}
    G -->|éœ€è¦æµå¼| H[_astream + merge_chunks_to_completion<br/>run_manager=None è·³è¿‡äº‹ä»¶]
    G -->|éæµå¼| I[_agenerate<br/>ç›´æ¥è¿”å›å®Œæ•´ç»“æœ]
    
    H --> J[è¿”å›å®Œæ•´ ChatCompletion]
    I --> J
    
    J --> K[ç”¨æˆ·è·å¾—æœ€ç»ˆçŠ¶æ€]
```


#### 1.2.3 åŒæ­¥å›¾/å¼‚æ­¥å›¾çš„äº‹ä»¶æµæ‰§è¡Œæµç¨‹ (graph.astream_eventsæ ¸å¿ƒè°ƒç”¨é“¾ä¸äº‹ä»¶æœºåˆ¶)

```mermaid
graph TD
    A[ç”¨æˆ·ä»£ç  async] --> B[async for event in<br/>graph.astream_events]
    B --> C[LangGraph._astream_events_internal<br/>äº‹ä»¶æµå¼•æ“]
    
    C --> D{åˆ¤æ–­æ‰§è¡Œå¼•æ“}
    D -->|å¼‚æ­¥å›¾| E[EventCaptureRunManager<br/>è°ƒç”¨ graph.ainvoke]
    D -->|åŒæ­¥å›¾| F[EventCaptureRunManager<br/>è°ƒç”¨ graph.invoke]
    
    E --> G[LLMClientChatModel.ainvoke]
    F --> H[LLMClientChatModel.invoke]
    
    G --> I[_agenerate_with_cache<br/>run_manager=EventCaptureRunManager]
    H --> J[_generate_with_cache<br/>run_manager=EventCaptureRunManager]
    
    I --> K[å¼ºåˆ¶æµå¼æ¨¡å¼]
    J --> L[å¼ºåˆ¶æµå¼æ¨¡å¼]
    
    K --> M[_astream<br/>å®æ—¶äº§ç”Ÿchunks]
    L --> N[_stream<br/>å®æ—¶äº§ç”Ÿchunks]
    
    M --> O[å®æ—¶è§¦å‘ on_llm_new_token<br/>ç”¨æˆ·ç«‹å³æ”¶åˆ° stream äº‹ä»¶]
    N --> P[å®æ—¶è§¦å‘ on_llm_new_token<br/>ç”¨æˆ·ç«‹å³æ”¶åˆ° stream äº‹ä»¶]
    
    O --> Q[merge_chunks_to_completion<br/>å†…éƒ¨å®Œæ•´ç»“æœ]
    P --> R[merge_chunks_to_completion<br/>å†…éƒ¨å®Œæ•´ç»“æœ]
    
    Q --> S[å›¾ç»§ç»­æ‰§è¡Œ]
    R --> S
```

### 1.3. LangGraph é›†æˆè°ƒç”¨è¯¦è§£

#### 1.3.1. åŒæ­¥å›¾éæµå¼è°ƒç”¨ï¼š`graph.invoke`

```python
class MyAgent(BaseAgent):
    def _build_graph(self):
        graph = StateGraph(AgentState)
        
        def llm_node(state: AgentState):
            # è·å– LLMClientChatModel å®ä¾‹
            llm_client = self.get_llm_client()
            chat_model = LLMClientChatModel(llm_client=llm_client)
            
            # åŒæ­¥è°ƒç”¨
            result = chat_model.invoke(state["messages"])
            return {**state, "response": result}
        
        graph.add_node("llm_node", llm_node)
        return graph.compile()

# ä½¿ç”¨
agent = MyAgent()
result = agent.graph.invoke({"messages": [HumanMessage(content="Hello")]})
```

#### 1.3.2. å¼‚æ­¥å›¾éæµå¼è°ƒç”¨ï¼š`graph.ainvoke`

```python
async def llm_node(state: AgentState):
    llm_client = self.get_llm_client()
    chat_model = LLMClientChatModel(llm_client=llm_client)
    
    # å¼‚æ­¥è°ƒç”¨
    result = await chat_model.ainvoke(state["messages"])
    return {**state, "response": result}

# ä½¿ç”¨
result = await agent.graph.ainvoke({"messages": [HumanMessage(content="Hello")]})
```

#### 1.3.3. åŒæ­¥å›¾/å¼‚æ­¥å›¾æµå¼è°ƒç”¨ï¼š`graph.astream_events`

```python
async def run_stream(self, input_data: str, **kwargs):
    """é«˜çº§æµå¼å¤„ç†å®ç°"""
    config = {"configurable": kwargs}
    
    async for event in self.graph.astream_events(
        {"input": input_data}, 
        config=config, 
        version="v1"
    ):
        event_type = event.get("event", "")
        
        # ç›‘å¬ LLM æµå¼äº‹ä»¶
        if event_type == "on_chat_model_stream":
            chunk = event.get("data", {}).get("chunk")
            if chunk and hasattr(chunk, "chat_completion_chunk"):
                # å¤„ç†æ€è€ƒå†…å®¹å’Œæ™®é€šå†…å®¹
                yield self._process_stream_chunk(chunk)
        
        # ç›‘å¬å…¶ä»–èŠ‚ç‚¹äº‹ä»¶
        elif event_type.startswith("on_"):
            yield self._process_graph_event(event)
```

## 2.è‡ªå®šä¹‰ FinalOutput å®ç°æµç¨‹

### 2.1. åŸºç¡€è¾“å‡ºç»“æ„å®šä¹‰

```python
from pydantic import BaseModel, Field
from typing import Dict, Any, List, Optional

class CustomOutput(BaseModel):
    """è‡ªå®šä¹‰è¾“å‡ºç»“æ„"""
    answer: str = Field(description="ä¸»è¦å›ç­”")
    confidence: float = Field(description="ç½®ä¿¡åº¦", ge=0, le=1)
    supporting_evidence: List[str] = Field(description="æ”¯æŒè¯æ®")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="å…ƒæ•°æ®")
    
    @classmethod
    def from_llm_response(cls, response: ChatCompletion) -> 'CustomOutput':
        """ä» LLM å“åº”æ„å»ºè¾“å‡º"""
        content = response.choices[0].message.content
        # è§£æ LLM å“åº”ï¼Œæå–ç»“æ„åŒ–æ•°æ®
        return cls(
            answer=content,
            confidence=0.9,  # ä»å“åº”ä¸­æå–æˆ–è®¡ç®—
            supporting_evidence=[],
            metadata={"usage": response.usage}
        )
```

### 2.2 æµå¼ FinalOutput å¤„ç†

```python
class StreamFinalOutput(BaseModel):
    """æµå¼æœ€ç»ˆè¾“å‡º"""
    type: str = Field(description="è¾“å‡ºç±»å‹: thinking|content|final")
    content: str = Field(description="å†…å®¹")
    metadata: Dict[str, Any] = Field(default_factory=dict)
    partial_result: Optional[CustomOutput] = Field(default=None)

async def run_stream_with_final_output(self, **kwargs) -> Iterator[StreamFinalOutput]:
    """æ”¯æŒå®Œæ•´ FinalOutput çš„æµå¼å¤„ç†"""
    
    partial_output = CustomOutput(answer="", confidence=0.0, supporting_evidence=[])
    
    async for event in self.graph.astream_events(inputs, config=config, version="v1"):
        event_type = event.get("event", "")
        
        if event_type == "on_chat_model_stream":
            chunk = self._extract_chunk_data(event)
            
            if chunk.type == "thinking":
                yield StreamFinalOutput(
                    type="thinking",
                    content=chunk.content,
                    metadata=chunk.metadata
                )
            elif chunk.type == "content":
                # æ›´æ–°éƒ¨åˆ†ç»“æœ
                partial_output.answer += chunk.content
                yield StreamFinalOutput(
                    type="content",
                    content=chunk.content,
                    partial_result=partial_output
                )
        
        elif event_type == "on_chain_end" and event.get("name") == "final_processing":
            # æœ€ç»ˆè¾“å‡ºå¤„ç†
            final_data = event.get("data", {}).get("output")
            final_output = CustomOutput.from_llm_response(final_data)
            
            yield StreamFinalOutput(
                type="final",
                content="",
                metadata={"usage": final_data.usage},
                partial_result=final_output
            )
```

## 3.è¿è¡Œæ—¶é…ç½®ç³»ç»Ÿè¯¦è§£

### 3.1. é…ç½®ç»§æ‰¿ä¸è¦†ç›–æœºåˆ¶

```python
class AdvancedAgent(BaseAgent):
    def __init__(
        self,
        # åŸºç¡€é…ç½®
        model: str = "deepseek-chat",
        temperature: float = 0.0,
        # è‡ªå®šä¹‰é…ç½®
        chunk_size: int = 512,
        similarity_threshold: float = 0.8,
        max_retrieval: int = 5,
        **kwargs
    ):
        super().__init__(model=model, temperature=temperature, **kwargs)
        
        # ä¿å­˜è‡ªå®šä¹‰é…ç½®åˆ° init_config
        self.init_config.update({
            "chunk_size": chunk_size,
            "similarity_threshold": similarity_threshold,
            "max_retrieval": max_retrieval,
        })
        
        # åˆå§‹åŒ–è‡ªå®šä¹‰ç»„ä»¶
        self.retriever = VectorRetriever(
            chunk_size=chunk_size,
            similarity_threshold=similarity_threshold
        )

    async def run(self, query: str, **runtime_config) -> Dict[str, Any]:
        """æ”¯æŒè¿è¡Œæ—¶é…ç½®è¦†ç›–"""
        
        # åˆå¹¶é…ç½®ï¼šè¿è¡Œæ—¶é…ç½® > åˆå§‹åŒ–é…ç½®
        effective_config = {**self.init_config, **runtime_config}
        
        # åº”ç”¨é…ç½®åˆ°å„ä¸ªç»„ä»¶
        self.retriever.chunk_size = effective_config.get("chunk_size")
        self.retriever.similarity_threshold = effective_config.get("similarity_threshold")
        
        # æ„å»º LangGraph é…ç½®
        graph_config = {"configurable": effective_config}
        
        result = await self.graph.ainvoke(
            {"query": query}, 
            config=graph_config
        )
        return result
```

### 3.2. å¤šå±‚çº§é…ç½®ç®¡ç†

```python
class ConfigurableAgent(BaseAgent):
    def _build_graph(self):
        graph = StateGraph(AgentState)
        
        def retrieval_node(state: AgentState, config: RunnableConfig):
            """æ”¯æŒè¿è¡Œæ—¶é…ç½®çš„èŠ‚ç‚¹"""
            run_config = config.get("configurable", {})
            
            # ä»è¿è¡Œæ—¶é…ç½®è·å–å‚æ•°ï¼Œå›é€€åˆ°åˆå§‹åŒ–é…ç½®
            chunk_size = run_config.get(
                "chunk_size", 
                self.init_config.get("chunk_size", 512)
            )
            max_retrieval = run_config.get(
                "max_retrieval",
                self.init_config.get("max_retrieval", 5)
            )
            
            # ä½¿ç”¨é…ç½®æ‰§è¡Œæ“ä½œ
            results = self.retriever.retrieve(
                state["query"],
                chunk_size=chunk_size,
                max_results=max_retrieval
            )
            
            return {**state, "retrieved_docs": results}
        
        graph.add_node("retrieval", retrieval_node)
        return graph.compile()
```

## 4. æ€è€ƒæ¨¡å¼è¿›é˜¶é…ç½®

### 4.1 æ€è€ƒæ¨¡å¼é€‚ç”¨æ€§åˆ†æ

| ä»»åŠ¡ç±»å‹ | æ¨èç¨‹åº¦ | åŸå›  |
|----------|----------|------|
| **å¤æ‚æ¨ç†** | â­â­â­â­â­ | æ•°å­¦è¯æ˜ã€é€»è¾‘æ¨ç†éœ€è¦å±•ç¤ºæ¨ç†é“¾ |
| **ä»£ç è°ƒè¯•** | â­â­â­â­â­ | å¸®åŠ©ç†è§£é—®é¢˜å®šä½è¿‡ç¨‹ |
| **æ–‡æ¡£é—®ç­”** | â­â­â­â­ | å±•ç¤ºè¯æ®æå–å’Œæ¨ç†è¿‡ç¨‹ |
| **ä¿¡æ¯æŠ½å–** | â­â­â­ | å¤æ‚ Schema æ—¶æœ‰ç”¨ |
| **æ–‡æœ¬ç¿»è¯‘** | â­ | ç®€å•ä»»åŠ¡ï¼Œä¸éœ€è¦æ€è€ƒè¿‡ç¨‹ |
| **æ–‡æœ¬åˆ†ç±»** | â­ | ç»“æœç¡®å®šæ€§é«˜ï¼Œä¸éœ€è¦å±•ç¤ºæ¨ç† |
| **å…³é”®è¯æå–** | â­ | ç®€å•ä»»åŠ¡ï¼Œå¢åŠ ä¸å¿…è¦çš„å¼€é”€ |

### 4.2 æµå¼å¤„ç†ä¸­çš„æ€è€ƒå†…å®¹

```python
async def process_stream_with_thinking(self, **kwargs):
    """å¤„ç†åŒ…å«æ€è€ƒå†…å®¹çš„æµå¼è¾“å‡º"""
    thinking_content = ""
    main_content = ""
    
    async for event in self.graph.astream_events(inputs, config=config, version="v1"):
        if event.get("event") == "on_chat_model_stream":
            chunk = event.get("data", {}).get("chunk")
            if chunk and hasattr(chunk, "chat_completion_chunk"):
                cc_chunk = chunk.chat_completion_chunk
                delta = cc_chunk.choices[0].delta
                
                # åŒºåˆ†æ€è€ƒå†…å®¹å’Œæ­£å¸¸å†…å®¹
                if hasattr(delta, "reasoning_content") and delta.reasoning_content:
                    thinking_content += delta.reasoning_content
                    yield {"type": "thinking", "content": delta.reasoning_content}
                elif delta.content:
                    main_content += delta.content
                    yield {"type": "content", "content": delta.content}
    
    yield {
        "type": "final",
        "thinking": thinking_content,
        "content": main_content
    }
```

### 4.3 æ‰©å±•æ¨¡å‹æ€è€ƒé…ç½®

é…ç½®æ–‡ä»¶ä½äº `llm_api/thinking_config.py`ï¼Œæ”¯æŒä»¥ä¸‹æ¨¡å‹ï¼š

```python
class ThinkingConfig(object):
    def __init__(self):
        self.model_type_thinking_params = {
            "glm": {
                "enable_thinking": {"thinking": {"type": "enabled"}},
                "disable_thinking": {"thinking": {"type": "disabled"}}
            },
            "deepseek": {
                "enable_thinking": {},  # DeepSeek é€šè¿‡æ¨¡å‹åç§°æ§åˆ¶
                "disable_thinking": {}
            },
            "qwen": {
                "enable_thinking": {"enable_thinking": True},
                "disable_thinking": {"enable_thinking": False}
            }
        }
```

> ğŸ“Œ **æ³¨æ„**ï¼šå½“å‰ä»…é€‚é…äº† **OpenAI å…¼å®¹æ ¼å¼** çš„ API æ¥å£ï¼ˆå¦‚ DeepSeekã€GLMã€Qwen ç­‰æä¾›çš„ OpenAI å…¼å®¹ç«¯ç‚¹ï¼‰ã€‚å¦‚éœ€é›†æˆ Anthropicã€Google ç­‰åŸç”Ÿæ ¼å¼çš„ APIï¼Œéœ€è¦è‡ªè¡Œæ‰©å±• `LLMClient`ã€‚

**æ·»åŠ æ–°æ¨¡å‹æ”¯æŒ**:

1. åœ¨ `model_type_thinking_params` ä¸­æ·»åŠ é…ç½®ï¼š
```python
"anthropic": {
    "enable_thinking": {"thinking": {"type": "enabled", "budget_tokens": 10000}},
    "disable_thinking": {}
}
```

2. åœ¨ `get_model_type()` ä¸­æ·»åŠ åŒ¹é…è§„åˆ™ï¼š
```python
elif model_name.startswith("claude"):
    return "anthropic"
```

> âš ï¸ **æ·»åŠ æ–°æ¨¡å‹å‰çš„å¿…è¦æ­¥éª¤**ï¼š
> 1. æŸ¥é˜…ç›®æ ‡æ¨¡å‹ API çš„å®˜æ–¹æ–‡æ¡£
> 2. ç¡®è®¤è¯¥æ¨¡å‹æ˜¯å¦æ”¯æŒæ€è€ƒ/æ¨ç†æ¨¡å¼
> 3. äº†è§£å¯ç”¨æ€è€ƒæ¨¡å¼æ‰€éœ€çš„å…·ä½“å‚æ•°æ ¼å¼
> 4. æµ‹è¯•å‚æ•°ä¼ é€’æ–¹å¼ï¼ˆå¯èƒ½æ˜¯ `extra_body`ã€è¯·æ±‚å¤´æˆ–å…¶ä»–æ–¹å¼ï¼‰
> 5. ç¡®è®¤æ˜¯å¦éœ€è¦ç‰¹å®šæ¨¡å‹ç‰ˆæœ¬ï¼ˆå¦‚ DeepSeek çš„ `deepseek-reasoner`ã€Qwen çš„ `qwen3-235b-a22b`ï¼‰

### 4.4 æ€è€ƒæ¨¡å¼æ€§èƒ½è€ƒé‡

```python
class ThinkingAwareAgent(BaseAgent):
    """æ ¹æ®ä»»åŠ¡å¤æ‚åº¦åŠ¨æ€å†³å®šæ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼"""
    
    def should_enable_thinking(self, input_data: dict) -> bool:
        """æ ¹æ®è¾“å…¥å¤æ‚åº¦åˆ¤æ–­æ˜¯å¦éœ€è¦æ€è€ƒæ¨¡å¼"""
        text = input_data.get("text", "")
        
        # ç®€å•è§„åˆ™ï¼šé•¿æ–‡æœ¬æˆ–åŒ…å«ç‰¹å®šå…³é”®è¯æ—¶å¯ç”¨
        if len(text) > 1000:
            return True
        if any(keyword in text for keyword in ["ä¸ºä»€ä¹ˆ", "å¦‚ä½•", "åˆ†æ", "æ¨ç†"]):
            return True
        return False
    
    async def run(self, **kwargs):
        # åŠ¨æ€å†³å®šæ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼
        enable_thinking = self.should_enable_thinking(kwargs)
        
        # åˆ›å»ºè¿è¡Œæ—¶é…ç½®
        runtime_config = {**kwargs, "enable_thinking": enable_thinking}
        
        # è®°å½•å†³ç­–
        self.logger.info(f"æ€è€ƒæ¨¡å¼: {'å¯ç”¨' if enable_thinking else 'ç¦ç”¨'}")
        
        return await self._execute(runtime_config)
```

---

æœ¬æŒ‡å—å±•ç¤ºäº†å¦‚ä½•æ·±åº¦å®šåˆ¶ BaseAgentï¼Œç‰¹åˆ«æ˜¯ä¸ LangGraph çš„é›†æˆã€æµå¼å¤„ç†ã€è¿è¡Œæ—¶é…ç½®ç³»ç»Ÿå’Œæ€è€ƒæ¨¡å¼çš„é«˜çº§ç”¨æ³•ã€‚è¿™äº›æ¨¡å¼å¯ä»¥å¸®åŠ©æ„å»ºç”Ÿäº§çº§çš„æ™ºèƒ½ä½“åº”ç”¨ã€‚

